{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import Data and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy import stats\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "from datetime import datetime\n",
    "import random\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 98928 entries, 1 to 100000\n",
      "Columns: 132 entries, item_id to user_state_Thuringia\n",
      "dtypes: float64(3), int64(4), uint8(125)\n",
      "memory usage: 17.8 MB\n"
     ]
    }
   ],
   "source": [
    "#os.chdir(\"C:/Users/TonyG/Documents/GitHub/bads/kaggle\") # Tony\n",
    "#os.chdir(\"C:/Users/erin-/Documents/bads/kaggle\") # Erin\n",
    "os.chdir(\"C:/Users/gotschat/Documents\") # Server\n",
    "data = pd.read_pickle('./data/known_cleaned_w_dummies')\n",
    "X = data.drop(\"return\", axis = 1)\n",
    "y = data[\"return\"]\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Preliminary version only: Create test and train sets based on the known dataset via random splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 314)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Boosted Trees "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 with Random Search CV and custom score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_custom_score(estimator, X, y):\n",
    "    zeros = np.zeros(y.shape[0])\n",
    "    hat = 1 * estimator.predict(X)\n",
    "    val = X.loc[:, \"item_price\"]\n",
    "    score = -1 * (np.fmax(np.array(y - hat), zeros) * (0.5 + 0.1 * val) +\n",
    "                 np.fmax(np.array(hat - y), zeros) * (0.5 * val))\n",
    "    return score.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_custom_eval(y_true, y_test, X, relative = False):\n",
    "    '''Custom Evaluation based on cost matrix\n",
    "    relative: True: return TP, FP, FN, TN, False: return the aggregated score, i.e. TP + TN + FP + FN\n",
    "    '''\n",
    "    y_true = 1 * y_true # Convert to integer\n",
    "    y_test = 1 * y_test\n",
    "    \n",
    "    zeros = np.zeros(y_true.shape[0]) # zeros for calculation\n",
    "    val = X.loc[:, \"item_price\"] # item_price of item \n",
    "    \n",
    "    TP, TN = np.array([0, 0]) # TP and TN are always 0\n",
    "    \n",
    "    # Calculate FP\n",
    "    FP = np.fmax(np.array(y_test - y_true), zeros) * (0.5 * val)\n",
    "    \n",
    "    # Calculate FN\n",
    "    FN = np.fmax(np.array(y_true - y_test), zeros) * (0.5 + 0.1 * val)\n",
    "    \n",
    "    # If clause for relative parameter\n",
    "    if(relative == True):\n",
    "        return(np.array([TP, FP.sum(), FN.sum(), TN]))\n",
    "    else:\n",
    "        return(np.sum([FP, FN]))\n",
    "    \n",
    "    #score = 1 * (np.fmax(np.array(y_true - y_test), zeros) * (0.5 + 0.1 * val) +\n",
    "    #             np.fmax(np.array(y_test - y_true), zeros) * (0.5 * val))\n",
    "    #return score.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:12:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=40, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a simple boosted tree classifier and calculate the custom score for this model using the X_test and y_test hold out\n",
    "ref_mod = xgb.XGBClassifier(objective = \"binary:logistic\")\n",
    "ref_mod.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175353.275"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_perf = cost_custom_eval(y_test, 1 * ref_mod.predict(X_test), X_test)\n",
    "base_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_boosting_rnds = True # Implemented a switch whether number of boosting rounds shoul be tuned as well\n",
    "param_grid = {\"max_depth\" : np.arange(5,80, step = 5), # maximum number of nodes for each base learner\n",
    "             \"eta\" : stats.uniform(0.05, 0.70), # Learning rate\n",
    "             \"gamma\" : stats.uniform(0.05, 3), # Minimum node impurity gain required to attempt a split\n",
    "             \"lambda\" : stats.uniform(0, 8), # L2 regularization coefficent\n",
    "             \"colsample_bytree\" : np.arange(0.2, 1, step = 0.1), # proportion of features considered for each base learner\n",
    "             \"subsample\" : np.arange(0.5, 1, step = 0.1), # proportion of training data subsampled for each base learner\n",
    "             \"n_estimators\" : np.where(fixed_boosting_rnds, 20, np.arange(10, 40, step = 5))} # Number of boosting rounds; will be inreased for the winner\n",
    "    \n",
    "gbm = xgb.XGBClassifier(objective = \"binary:logistic\") # initiate estimator\n",
    "metric = cost_custom_score # Scoring metric used during training\n",
    "n = 600 # number of candidates to consider\n",
    "fold = 5 # number of folds for cv\n",
    "n * fold # number of fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 600 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 40 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "randomized_auc = RandomizedSearchCV(estimator = gbm, n_iter = n, cv = fold, scoring = metric,\n",
    "                                    param_distributions = param_grid, verbose = 1, random_state = 321,\n",
    "                                   n_jobs = -1, refit = True)\n",
    "randomized_auc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Parameters:\", randomized_auc.best_params_)\n",
    "print(\"Best Score:\", randomized_auc.best_score_ * (-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If trained on server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy paste saved dictionary if not on server\n",
    "# params =\n",
    "\n",
    "params = randomized_auc.best_params_\n",
    "\n",
    "eval_set = [(X_train, y_train), (X_test, y_test)] # Evaluation set for early stopping\n",
    "randomized_auc = xgb.XGBClassifier(objective = \"binary:logistic\",\n",
    "                                   max_depth = params[\"max_depth\"],\n",
    "                                   learning_rate =  params[\"eta\"],\n",
    "                                   gamma = params[\"gamma\"],\n",
    "                                   reg_lambda = params[\"lambda\"],\n",
    "                                   colsample_bytree = params[\"colsample_bytree\"],\n",
    "                                   subsample = params[\"subsample\"],\n",
    "                                   n_estimators = 200, # Train with higher number of boosting rounds as in the cv part\n",
    "                                   random_state = 321)\n",
    "randomized_auc.fit(X_train, y_train, early_stopping_rounds = 10, eval_metric = \"auc\", eval_set = eval_set,\n",
    "                  verbose = True) # Enable earlier stopping when within 10 rounds no improvement is observed using auc as evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomized_auc.best_iteration, randomized_auc.best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display evaluation performance during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "results = randomized_auc.evals_result()\n",
    "epochs = len(results['validation_0']['auc'])\n",
    "x_axis = range(0, epochs)\n",
    "# plot roc\n",
    "fig, ax = pyplot.subplots()\n",
    "ax.plot(x_axis, results['validation_0']['auc'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['auc'], label='Test')\n",
    "ax.legend()\n",
    "pyplot.ylabel('ROC')\n",
    "pyplot.title('XGBoost Classification Error')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display custom score using the tuned model and the hold out test set and compare with base performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = randomized_auc.predict(X_test)\n",
    "print(\"Tuned Custom Evaluation Score:\", cost_custom_eval(y_test, 1 * preds, X_test).round(5))\n",
    "print(\"Base Custom Evaluation Score:\", base_perf.round(5))\n",
    "print(\"Tuned AUC score:\", metrics.roc_auc_score(y_test, randomized_auc.predict(X_test)).round(5))\n",
    "print(\"Base AUC Score:\", metrics.roc_auc_score(y_test, ref_mod.predict(X_test)).round(5))\n",
    "print(\"Monkey Custom Evaluation Score:\", cost_custom_eval(y_test, 1* random.choices(np.array([True, False]), k = y_test.shape[0]), X_test).round(5))\n",
    "print(\"Monkey AUC Score:\", metrics.roc_auc_score(y_test, random.choices(np.array([True, False]), k = y_test.shape[0])).round(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "item_size_unsized                    0.075183\n",
       "time_to_delivery                     0.054843\n",
       "item_color_pallid                    0.034539\n",
       "item_color_?                         0.019875\n",
       "item_price                           0.017996\n",
       "item_size_Other                      0.012893\n",
       "item_color_dark denim                0.012059\n",
       "item_color_floral                    0.011925\n",
       "item_id                              0.011783\n",
       "item_color_coral                     0.011368\n",
       "brand_id                             0.011221\n",
       "item_color_silver                    0.011153\n",
       "item_color_navy                      0.010941\n",
       "item_size_6                          0.010378\n",
       "customer_age                         0.009335\n",
       "user_id                              0.009224\n",
       "user_title_Mrs                       0.009187\n",
       "item_color_habana                    0.008928\n",
       "item_color_Other                     0.008898\n",
       "item_size_116                        0.008854\n",
       "user_age                             0.008827\n",
       "item_size_l                          0.008777\n",
       "item_size_8                          0.008643\n",
       "item_size_xxl                        0.008479\n",
       "user_state_Bremen                    0.008424\n",
       "item_size_xl                         0.008225\n",
       "user_state_North Rhine-Westphalia    0.008189\n",
       "user_state_Lower Saxony              0.008181\n",
       "user_state_Bavaria                   0.008036\n",
       "item_size_128                        0.007987\n",
       "item_color_black                     0.007977\n",
       "user_state_Baden-Wuerttemberg        0.007938\n",
       "item_color_berry                     0.007905\n",
       "user_state_Rhineland-Palatinate      0.007884\n",
       "item_size_s                          0.007832\n",
       "item_size_46                         0.007787\n",
       "user_state_Schleswig-Holstein        0.007696\n",
       "user_title_Mr                        0.007660\n",
       "dtype: float32"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant = pd.Series(randomized_auc.feature_importances_,\n",
    "                  index = X_train.columns).sort_values(ascending = False).quantile(q = 0.7) # get 70% quantile\n",
    "importance_trained = pd.Series(randomized_auc.feature_importances_, index = X_train.columns).sort_values(ascending = False)\n",
    "importance_trained[importance_trained >= quant] # Display only the 30% highest importance values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "item_color_aubergine        0.005363\n",
       "user_state_Saxony-Anhalt    0.005343\n",
       "item_color_ancient          0.005314\n",
       "item_size_35                0.005301\n",
       "item_color_orange           0.005283\n",
       "item_color_yellow           0.005264\n",
       "item_color_azure            0.005154\n",
       "item_color_striped          0.005153\n",
       "item_size_50                0.005136\n",
       "item_size_xs                0.005118\n",
       "item_size_40+               0.005011\n",
       "item_size_23                0.004774\n",
       "item_size_3                 0.004752\n",
       "item_size_152               0.004724\n",
       "item_size_32                0.004651\n",
       "item_size_34                0.004514\n",
       "item_size_41+               0.004494\n",
       "item_size_42+               0.004445\n",
       "item_size_25                0.004379\n",
       "item_size_9+                0.004364\n",
       "item_size_4+                0.004252\n",
       "item_size_13                0.004189\n",
       "item_size_27                0.004165\n",
       "item_color_gold             0.004017\n",
       "item_size_24                0.003996\n",
       "item_size_26                0.003846\n",
       "item_color_mahagoni         0.003762\n",
       "item_size_19                0.003486\n",
       "user_title_not reported     0.003420\n",
       "item_size_47                0.003038\n",
       "item_size_xxxl              0.003003\n",
       "item_color_blau             0.002990\n",
       "item_size_33                0.002915\n",
       "item_color_aqua             0.002698\n",
       "item_size_164               0.002695\n",
       "item_size_1                 0.002477\n",
       "item_color_dark oliv        0.002051\n",
       "item_size_3332              0.001694\n",
       "dtype: float32"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant = pd.Series(randomized_auc.feature_importances_,\n",
    "                  index = X_train.columns).sort_values(ascending = False).quantile(q = 0.3) # get 70% quantile\n",
    "importance_trained = pd.Series(randomized_auc.feature_importances_, index = X_train.columns).sort_values(ascending = False)\n",
    "importance_trained[importance_trained < quant] # Display only the 30% lowest importance values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save AUC score and custom score into .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_auc = metrics.roc_auc_score(y_test, randomized_auc.predict(X_test))\n",
    "perf_cust = cost_custom_eval(y_test, 1 * preds, X_test)\n",
    "sys_time = datetime.now().strftime(\"%d-%b-%Y (%H:%M)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "# Initiate .txt and column header, is run once\n",
    "f = open(\"results_RS.txt\", \"w+\")\n",
    "file = open(\"results_RS.txt\", \"a\")\n",
    "file.write(\"\\t\".join([\"Time\", \"\\t\\t AUC\", \"\\t Custom Score\", \"\\n\"]))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"results_RS.txt\", \"a\")\n",
    "file.write(\" \".join([sys_time,\"\\t\", str(perf_auc.round(5)),\"\\t\", str(perf_cust.round(5)), \"\\n\"]))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display best parameters and best score during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'objective': 'binary:logistic', 'use_label_encoder': True, 'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.2, 'gamma': 2.176864565876066, 'gpu_id': -1, 'importance_type': 'gain', 'interaction_constraints': '', 'learning_rate': 0.0715808240526306, 'max_delta_step': 0, 'max_depth': 55, 'min_child_weight': 1, 'missing': nan, 'monotone_constraints': '()', 'n_estimators': 200, 'n_jobs': 40, 'num_parallel_tree': 1, 'random_state': 321, 'reg_alpha': 0, 'reg_lambda': 3.1622167750579484, 'scale_pos_weight': 1, 'subsample': 0.8999999999999999, 'tree_method': 'exact', 'validate_parameters': 1, 'verbosity': None}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", randomized_auc.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save hyperparameters inside a .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# Initiate .txt and column header, is run once\n",
    "f = open(\"hyperparams.txt\", \"w+\")\n",
    "file = open(\"hyperparams.txt\", \"a\")\n",
    "file.write(\"Hyperparameters in Dictionary format \\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"hyperparams.txt\", \"a\")\n",
    "file.write(\"\\t\".join([sys_time, str(randomized_auc.get_params()), \"\\n\"]))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Confusion matrix with probabilites and a custom confusion matrix based on the cost matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37845, 0.16456],\n",
       "       [0.15016, 0.30683]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, randomized_auc.predict(X_test), normalize = \"all\").round(5) # TP FP // FN TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cust_confusion_matrix(y_true, y_pred, X, normalize = True, rnd = False):\n",
    "    '''Custom Confusion Matrix with cost matrix\n",
    "    y_true: 1-d array bool\n",
    "    y_pred: 1-d array bool\n",
    "    X: n-d array of data which includes the item price for the respective observations (float)\n",
    "    normalize: True: Should the custom score be divided by the possible maximum costs?\n",
    "    rnd: True: Should the normalizing value be calculated using a monkey guessing at random which item will be returned?\n",
    "    '''\n",
    "    y_true = 1 * y_true # convert to integer\n",
    "    y_pred = 1 * y_pred # convert to integer\n",
    "    \n",
    "    # Calculate TP, FP, FN and TN\n",
    "    TP, FP, FN, TN = cost_custom_eval(y_true, y_pred, X, True)\n",
    "    \n",
    "    # If clause for random normalization\n",
    "    if(normalize == True and rnd == True):\n",
    "        # custom prediction \n",
    "        y_random_pred = random.choices(np.array([False, True]), k = y_true.shape[0])\n",
    "        FP_rel, FN_rel = cost_custom_eval(y_true, y_random_pred, X, True)[1:3] # retrieve FP rate for this case\n",
    "        results = np.array([[TP, FP/FP_rel],\n",
    "                            [FN/FN_rel, TN]])\n",
    "        return(results)\n",
    "    elif(rnd == False):\n",
    "        # If clause for normalize\n",
    "        if(normalize == True):\n",
    "            y_pred_rel_fp = np.ones(y_true.shape[0]) # Every observation is classified as one\n",
    "            FP_rel = cost_custom_eval(y_true, y_pred_rel_fp, X) # retrieve FP rate for this case\n",
    "\n",
    "            y_pred_rel_fn = np.zeros(y_true.shape[0]) # Every observation is classified as zero\n",
    "            FN_rel = cost_custom_eval(y_true, y_pred_rel_fn, X) # retrieve FN rate for this case\n",
    "            results = np.array([[TP, FP/FP_rel],\n",
    "                                [FN/FN_rel, TN]])\n",
    "            return(results)\n",
    "        else:\n",
    "            results = np.array([[TP, FP],\n",
    "                                [FN, TN]])\n",
    "            return(results)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intepretation of a relative cost matrix based confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.40160672],\n",
       "       [0.2652876 , 0.        ]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cust_confusion_matrix(y_test, preds, X_test, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entry [1,2] describes how our model performs compared to a model which classifies all purchases as returns. The lower this value the lower are the missed profits from not discouraging the customer to place their order. Conversely, the entry [2,1] describes how our model performed compared to a model which flags every purchase as an item which will not be returned. The lower this values the less costs occur from handling the return. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compare our model in this manner with a model that randomly flags whether an item is returned using the parameter *rnd*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.79794665],\n",
       "       [0.53712908, 0.        ]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cust_confusion_matrix(y_test, preds, X_test, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create .csv of predictions for unknown data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_u = pd.read_pickle('./data/unknown_cleaned_w_dummies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = randomized_auc.predict_proba(data_u)[:, 1]\n",
    "predict_unknown = pd.Series(preds, index=data_u[\"item_id\"].index, name='return')\n",
    "predict_unknown.to_csv(\"first_pred.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 With random search CV and combined model instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First subset the training set into three categories $C_j$, $j \\in \\{1,2,3\\}$, with $C_j = \\{X_i : X_{il} \\in I_j\\}$, where $X_i$ are the observations, $X_{il}$ is the item price for the respective observation and $I_j$ are item price intervals, namely \n",
    "\\begin{align*}\n",
    "I_1 &= [0, K_1) \\\\\n",
    "I_2 &= [K_1, K_2] \\\\\n",
    "I_3 &= (K_2, \\infty).\n",
    "\\end{align*}\n",
    "The thresholds $K_1$ and $K_2$ are given by the 0.25 and 0.75 quantiles for the distribution in the training set of item price.\n",
    "Afterwards train three different boosted classifier on those subsets using 5-fold crossvalidation. \n",
    "Finally, create a custom class which incorporates all three models and uses a customized predict attribute to use only the relevant model based on the item price of the test observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split trainingsset in three subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get quantiles \n",
    "K1, K2 = X_train[\"item_price\"].quantile([0.25, 0.75])\n",
    "\n",
    "# Create three subsets of training data\n",
    "C1 = X_train[X_train[\"item_price\"] < K1]\n",
    "C2 = X_train.loc[(X_train[\"item_price\"] >= K1) & (X_train[\"item_price\"] <= K2), :]\n",
    "C3 = X_train[X_train[\"item_price\"] > K2]\n",
    "\n",
    "# Create the according target vectors\n",
    "Y1 = y_train[C1.index]\n",
    "Y2 = y_train[C2.index]\n",
    "Y3 = y_train[C3.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train three different models using 5-fold crossvalidation using only the subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid stays the same for all three models\n",
    "param_grid = {\"max_depth\" : np.arange(5,80, step = 5),\n",
    "             \"eta\" : stats.uniform(0.1, 0.8),\n",
    "             \"gamma\" : stats.uniform(0.05, 3),\n",
    "             \"lambda\" : stats.uniform(0, 5),\n",
    "             \"colsample_bytree\" : np.arange(0.2, 1, step = 0.1),\n",
    "             \"subsample\" : np.arange(0.5, 1, step = 0.1),\n",
    "             \"n_estimators\" : np.arange(10, 80, step = 5)}\n",
    "\n",
    "# Instantiate classifier\n",
    "gbm = xgb.XGBClassifier(objective = \"binary:logistic\")\n",
    "\n",
    "metric = cost_custom_score # Custom scoring function, see 2.1\n",
    "n = 300 # Number of search iterations\n",
    "fold = 5 # 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed: 55.2min\n",
      "[Parallel(n_jobs=-1)]: Done 370 tasks      | elapsed: 136.3min\n",
      "[Parallel(n_jobs=-1)]: Done 720 tasks      | elapsed: 269.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1170 tasks      | elapsed: 422.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1500 out of 1500 | elapsed: 531.7min finished\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:22:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { early_stopping_rounds, num_boost_round } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[04:23:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None,\n",
       "                                           early_stopping_rounds=10, gamma=None,\n",
       "                                           gpu_id=None, importance_type='gain',\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           min_child_weight=None, missing=nan,\n",
       "                                           monotone_constrain...\n",
       "                                        'gamma': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000000A2E3F1AA60>,\n",
       "                                        'lambda': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000000A2E3F1A160>,\n",
       "                                        'max_depth': array([ 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75]),\n",
       "                                        'n_estimators': array([10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75]),\n",
       "                                        'subsample': array([0.5, 0.6, 0.7, 0.8, 0.9])},\n",
       "                   random_state=321,\n",
       "                   scoring=<function cost_custom_score at 0x000000A2E3E6F160>,\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subset C1\n",
    "mod_c1 = RandomizedSearchCV(estimator = gbm, n_iter = n, cv = fold, scoring = metric,\n",
    "                                    param_distributions = param_grid, verbose = 1, random_state = 321,\n",
    "                                   n_jobs = -1)\n",
    "mod_c1.fit(C1, Y1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Parameter dictionary\n",
    "mod_c1_params = mod_c1.best_params_\n",
    "with open('mod_c1_params.txt', 'w') as f:\n",
    "    print(mod_c1_params, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed: 139.7min\n",
      "[Parallel(n_jobs=-1)]: Done 370 tasks      | elapsed: 321.3min\n",
      "[Parallel(n_jobs=-1)]: Done 720 tasks      | elapsed: 549.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1170 tasks      | elapsed: 831.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1500 out of 1500 | elapsed: 1043.4min finished\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:46:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { early_stopping_rounds, num_boost_round } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[21:46:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None,\n",
       "                                           early_stopping_rounds=10, gamma=None,\n",
       "                                           gpu_id=None, importance_type='gain',\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           min_child_weight=None, missing=nan,\n",
       "                                           monotone_constrain...\n",
       "                                        'gamma': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000000A2E3F1AA60>,\n",
       "                                        'lambda': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000000A2E3F1A160>,\n",
       "                                        'max_depth': array([ 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75]),\n",
       "                                        'n_estimators': array([10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75]),\n",
       "                                        'subsample': array([0.5, 0.6, 0.7, 0.8, 0.9])},\n",
       "                   random_state=321,\n",
       "                   scoring=<function cost_custom_score at 0x000000A2E3E6F160>,\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subset C2\n",
    "mod_c2 = RandomizedSearchCV(estimator = gbm, n_iter = n, cv = fold, scoring = metric,\n",
    "                                    param_distributions = param_grid, verbose = 1, random_state = 321,\n",
    "                                   n_jobs = -1)\n",
    "mod_c2.fit(C2, Y2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Parameter dictionary\n",
    "mod_c2_params = mod_c2.best_params_\n",
    "with open('mod_c2_params.txt', 'w') as f:\n",
    "    print(mod_c2_params, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed: 47.4min\n",
      "[Parallel(n_jobs=-1)]: Done 370 tasks      | elapsed: 123.0min\n",
      "[Parallel(n_jobs=-1)]: Done 720 tasks      | elapsed: 208.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1170 tasks      | elapsed: 377.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1500 out of 1500 | elapsed: 452.9min finished\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:19:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { early_stopping_rounds, num_boost_round } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[05:19:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None,\n",
       "                                           early_stopping_rounds=10, gamma=None,\n",
       "                                           gpu_id=None, importance_type='gain',\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           min_child_weight=None, missing=nan,\n",
       "                                           monotone_constrain...\n",
       "                                        'gamma': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000000A2E3F1AA60>,\n",
       "                                        'lambda': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000000A2E3F1A160>,\n",
       "                                        'max_depth': array([ 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75]),\n",
       "                                        'n_estimators': array([10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75]),\n",
       "                                        'subsample': array([0.5, 0.6, 0.7, 0.8, 0.9])},\n",
       "                   random_state=321,\n",
       "                   scoring=<function cost_custom_score at 0x000000A2E3E6F160>,\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subset C3\n",
    "mod_c3 = RandomizedSearchCV(estimator = gbm, n_iter = n, cv = fold, scoring = metric,\n",
    "                                    param_distributions = param_grid, verbose = 1, random_state = 321,\n",
    "                                   n_jobs = -1)\n",
    "mod_c3.fit(C3, Y3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Parameter dictionary\n",
    "mod_c3_params = mod_c3.best_params_\n",
    "with open('mod_c3_params.txt', 'w') as f:\n",
    "    print(mod_c3_params, file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([1086.39582286, 2139.23285995,  892.68328056,  938.22382946,\n",
       "        1911.38577652, 1484.85955844, 1007.4564424 ,  132.0170022 ,\n",
       "         740.68059134,  959.93398142,  184.71005397, 1408.77953815,\n",
       "         474.21984429,  132.18336306,  198.52198334,  665.92604961,\n",
       "        1730.91441336,  777.58360434,  732.35615788,  331.56497569,\n",
       "         166.16255465,  170.91654234,  291.70691071,  710.63079195,\n",
       "         993.3320117 ,  677.83053808, 1633.99207878,  948.49874563,\n",
       "         844.82669735,  840.64846525,  289.54442396, 1873.13045855,\n",
       "        1625.66163955, 1815.95087781,  221.20467286,  763.78889213,\n",
       "         502.93275366,  293.61815529,  366.0282342 ,  383.46646271,\n",
       "         512.95542603,  717.21434555,  408.36148791, 1563.88642373,\n",
       "         287.43598909,  668.26690879,  900.55888138,  410.16602941,\n",
       "         221.1534234 ,  273.03789406, 1174.0790411 ,  324.87531991,\n",
       "         361.56072268,  325.77098417,  370.26111913,  163.13002815,\n",
       "         322.54887471,  618.90066833,  292.81157713,  614.82333331,\n",
       "         933.85121012, 2156.63445344,  252.13211942, 1640.99383597,\n",
       "        1320.34339323, 1007.1863822 ,  540.78688436, 1177.9323597 ,\n",
       "         717.57549405, 1489.91884141,  580.60010624,  522.98770761,\n",
       "        1215.54445724, 1102.4070744 ,  241.04942398,  284.12959824,\n",
       "         158.756007  ,  483.33924479,  327.51226454, 1445.2447288 ,\n",
       "         458.52109447, 1079.72958965,   98.7440927 ,  281.38014474,\n",
       "        1186.08482728,  201.4079679 ,  147.97400184,  232.7031692 ,\n",
       "         499.55229135, 1289.07121406,  930.01070385,  989.10973725,\n",
       "         262.91592603,  569.84112349,  671.27981653,  131.60560575,\n",
       "          64.08724737,  736.42388353,  194.91049762,  916.87362585,\n",
       "         388.90190849,  448.24437661,  557.12425094,  283.23113103,\n",
       "         236.66839604,  729.99487934, 1026.41573925,  805.05351424,\n",
       "         828.68211703,  276.53687105,  485.19443531,  159.99360127,\n",
       "         175.69014907,  636.14871325,  521.18316627,  446.54874048,\n",
       "         851.22867579,   88.71941843,  817.6334547 ,  614.1871192 ,\n",
       "         443.00592155, 1203.30644698,  513.89513278,  919.38805237,\n",
       "          73.02587328,  489.80448418,  733.24801865,  224.96470146,\n",
       "         269.95852256,  696.29235473,  629.58057442, 1659.87265458,\n",
       "         309.19839091,  269.39477868,  414.26258259,  665.7835124 ,\n",
       "        1211.73317933,  514.00704079,  462.23207569,  997.41935639,\n",
       "        1189.95976677,  296.06811934,  909.71211486,  889.17489543,\n",
       "         732.82941475,  984.47366314, 1325.30778403,  752.21952696,\n",
       "         638.45994349,  105.42694182,  595.97294159, 1228.90735312,\n",
       "        1066.75386763,  745.11246829,  776.90735207,  805.85268588,\n",
       "         348.13696828,  972.16118107,  677.26679411,  318.70916924,\n",
       "         377.04766903,  590.44600797,  659.16772766,   26.59566593,\n",
       "        1777.43130465,  318.62128458,  124.81124887, 1760.72037773,\n",
       "         374.46097245,  874.74496975, 1462.53741717,  633.59584956,\n",
       "         709.4232265 ,  201.28785219, 1318.77067556, 2712.84922647,\n",
       "        1532.74977555, 1930.24177356,  316.62916174,   73.59582324,\n",
       "         774.75207214,  910.80917339,   55.4559176 , 1242.5495182 ,\n",
       "         867.5143919 ,  170.94436917, 1464.8506496 , 1961.33538041,\n",
       "        1484.34986696,  707.80746737,  141.39605389,  308.07010198,\n",
       "         900.3412714 ,  567.33870826,  906.41813641, 1551.76993065,\n",
       "        1344.99998798,  169.10299225, 1005.12639389,  900.85476708,\n",
       "         650.13701282,  629.80659294,  523.73703079,  158.35562053,\n",
       "        1756.39660487,  597.90840964,  743.08331008, 1454.64059639,\n",
       "         771.66249037,  899.22058983,  299.62615314, 2563.2924973 ,\n",
       "        1200.61885333, 1487.81981592,  805.2671205 ,  159.07191167,\n",
       "         583.6638629 ,  313.25870938,  501.2639431 , 1091.37923203,\n",
       "         674.28511648, 2077.72149839, 1015.52022471,  508.94996047,\n",
       "        1295.14027109, 1847.91992927,  860.25218339, 2259.8348465 ,\n",
       "        1005.09756618, 1665.78235807,  437.59970417,  651.33256674,\n",
       "        1168.5445004 ,  432.54542665, 1456.13283644,  152.38025398,\n",
       "        1552.33727832,  979.02380352,  241.91746163,  191.79509101,\n",
       "        1242.97512913, 1093.69606791,  321.75170531,  586.61110692,\n",
       "        1379.12852359,  344.92086477,  770.86732326,  714.78039637,\n",
       "         126.06265635,  622.63387079, 1724.157693  , 1447.49990525,\n",
       "        1030.61699357, 1000.34397879, 1172.75976815,  905.26522326,\n",
       "         990.23121924,  477.94784184,  506.3272295 ,  109.60397296,\n",
       "          21.2585155 ,  899.5777349 ,  677.70902081,  206.19318595,\n",
       "         425.62694988,  775.61330314,   82.93523626,  310.20716429,\n",
       "         500.98227134,  327.22778997,  249.79746633,  308.72833734,\n",
       "         716.41597481,  194.18059344,  407.7643116 ,  654.84195318,\n",
       "         660.25557752,  257.79798694,  427.54420033,  425.95686803,\n",
       "         406.08689275,  526.91569815,  125.05788698,  302.55277748,\n",
       "         223.67485671,   36.25438709,  162.2001307 ,  218.98192778,\n",
       "         636.99733205,  601.98113999,  579.02458587,  493.60094762,\n",
       "         132.53750491,  545.69922533,  205.58239655,  505.7869081 ,\n",
       "         168.56847653,  411.34256501,  141.77722163,  362.27561283]),\n",
       " 'std_fit_time': array([407.70194968, 502.51812156, 202.17937297, 556.27241137,\n",
       "        512.4063498 , 559.97890936, 321.92926602,  49.98146324,\n",
       "        426.30921343, 184.11661902,  75.85049389, 405.65416533,\n",
       "        260.70557955,  24.75912482,  60.35296214, 266.16731133,\n",
       "        366.21885757, 357.69602889, 397.00990158, 203.9648381 ,\n",
       "         98.38478963,  83.54818037, 119.58410098, 225.26792044,\n",
       "        103.81491434, 124.11673844, 408.83307829, 331.09533779,\n",
       "        414.74108541, 322.3078886 ,  71.63547896, 247.83985885,\n",
       "        354.66521911, 282.11819595,  78.61121306, 132.73899467,\n",
       "        195.55332952,  54.28130479,  87.15591983,  73.70775521,\n",
       "         99.0017426 , 394.032662  ,  99.06030299, 461.5182059 ,\n",
       "         66.30849998, 372.57652829, 272.21546033,  88.82713732,\n",
       "         63.7762221 , 135.93470188, 413.56030896,  61.31307658,\n",
       "        262.53377737, 105.10325171, 138.46799167, 106.63739349,\n",
       "        195.4504242 , 211.65663139, 128.01360998, 326.73852957,\n",
       "        419.04539616, 515.90430869, 111.01532643, 234.07447752,\n",
       "        271.00459819, 239.92567489, 240.93385914, 108.39592889,\n",
       "        157.4215327 , 244.20093435, 106.86870382, 262.8003795 ,\n",
       "        358.18962104, 196.30244288,  63.4101836 ,  64.22240071,\n",
       "         56.04021133, 141.67939832,  63.19414172, 268.10753337,\n",
       "        101.47434495, 268.95247389,  23.48305417,  37.21466353,\n",
       "        178.55118554,  47.71790176,  84.666675  ,  70.44078514,\n",
       "         62.44772522, 183.91087067, 314.37675257,  70.13686198,\n",
       "         94.5331426 , 282.09805871, 104.49874625,  51.16650379,\n",
       "         37.03213833, 227.17382523,  20.17509226, 272.20847816,\n",
       "         90.79897721, 139.83845493,  70.33486139, 111.19849279,\n",
       "        121.06858786,  54.40134516, 319.69611942, 134.96953498,\n",
       "        115.75567857,  89.55873207, 121.43343207,  32.37601193,\n",
       "         46.19491211, 120.17058022, 176.71123144,  68.05146711,\n",
       "        262.65703533,  34.73378783, 162.51456543, 127.36075613,\n",
       "         83.11161223, 369.51066572, 175.70674952, 232.06592802,\n",
       "         36.32868412, 158.24315248, 182.51393411,  86.14359612,\n",
       "         50.5651712 , 179.24316026, 111.96463157, 610.6805179 ,\n",
       "         97.98416452, 130.79259943,  97.37026178, 328.93059415,\n",
       "        526.77956921, 233.33600308, 240.82874649, 326.8884277 ,\n",
       "        636.16396472,  91.70708443, 688.38867352, 460.14882227,\n",
       "        597.89096258, 662.24318163, 838.54670884, 303.87748728,\n",
       "        297.33102141,  37.54004471, 349.4912532 , 397.98121719,\n",
       "        207.03971325, 489.43985728, 490.88354391, 241.27095733,\n",
       "        168.08998157, 698.13668816, 337.89192973,  95.87816875,\n",
       "        208.09688024, 104.72889507, 151.33985585,  26.39995048,\n",
       "        198.34225787,  61.94236986,  63.786613  , 365.89335658,\n",
       "        192.07845629, 240.83852795, 279.22719898, 303.14400311,\n",
       "        371.85789428,  58.47226035, 337.31724889, 526.58260877,\n",
       "        217.50410765, 440.92683421, 173.38277513,  19.71194169,\n",
       "         77.21970841, 511.70655064,  24.60176228, 515.28273233,\n",
       "        363.93164799,  87.37180055, 313.62398579, 484.56524016,\n",
       "        420.15190969, 127.98383915,  79.40185278, 113.84226247,\n",
       "        223.62095518, 137.31548459, 161.47105326, 371.99238196,\n",
       "        314.19349237,  64.53120884, 227.55591116, 355.59161674,\n",
       "        333.39449778, 250.47226289, 369.15296469,  66.23756801,\n",
       "        302.97412126, 269.85368067,  81.84470561, 277.90999586,\n",
       "        217.11675336, 323.7214365 ,  87.20625135, 649.4091888 ,\n",
       "        128.81326689, 366.68162875, 140.52307744,  65.60858312,\n",
       "        265.19083733, 208.39199226, 230.49320971, 191.65608179,\n",
       "        127.93115466, 269.52072894, 471.49683417, 139.10128518,\n",
       "        232.81566338, 310.35908294, 203.55486889, 399.56867224,\n",
       "        345.79078732, 566.76539035, 241.98482938, 233.32571406,\n",
       "        235.5608786 , 172.00537057, 462.75463552,  68.3099134 ,\n",
       "        433.87275379, 288.58051486, 115.90768331,  58.74665737,\n",
       "        359.64995295, 385.08326446, 113.6828561 , 209.62800068,\n",
       "        451.00780505, 109.8275385 , 102.42888206, 289.3300528 ,\n",
       "         69.93676116, 403.54859161, 157.10237016, 246.14139631,\n",
       "        306.89746507, 126.38182969, 118.08588178, 120.62285782,\n",
       "         84.9020468 ,  55.38503583, 224.98458938,  10.02291407,\n",
       "         17.53443104, 174.33359699, 210.02401683,  35.42433235,\n",
       "        109.17696034, 103.20458284,  37.87905722,  91.75226391,\n",
       "        118.45032146,  44.2600753 ,  67.96546621,  82.24865912,\n",
       "        114.41557809,  40.66307866,  75.51825927, 178.46378859,\n",
       "        153.7726773 ,  93.10048541, 108.1704638 ,  65.39620359,\n",
       "         83.98930606,  86.7409947 ,  72.59733417, 141.17757652,\n",
       "         70.24960226,  21.84111993,  57.63033313,  63.22645447,\n",
       "         40.02408092, 186.03867721, 130.34321033, 187.23610604,\n",
       "         67.48357519, 107.23095481, 137.35087952,  66.57346969,\n",
       "         90.21606553,  86.42112281,  64.66484143,  55.28589879]),\n",
       " 'mean_score_time': array([0.57375398, 0.6159945 , 0.74511914, 0.5671474 , 0.42641153,\n",
       "        0.74591994, 0.50869083, 0.77574887, 0.76073432, 0.70127692,\n",
       "        0.50608864, 0.7124876 , 0.99516063, 0.92068872, 1.04400749,\n",
       "        0.71829333, 0.50008278, 0.17977376, 0.82960062, 0.77875161,\n",
       "        0.69927492, 0.61179047, 0.69407001, 0.59737663, 0.59577522,\n",
       "        0.69547143, 0.54072199, 0.81578722, 0.32711573, 0.54032145,\n",
       "        1.13549581, 0.81979132, 0.74111524, 0.79937129, 1.08604817,\n",
       "        0.42240744, 0.78195462, 0.80817986, 0.77855134, 1.02298737,\n",
       "        0.75552907, 0.59337277, 0.69767323, 0.74231639, 0.7589324 ,\n",
       "        0.42861371, 0.66043749, 0.82699823, 0.84781814, 0.61299152,\n",
       "        0.52690873, 0.71428957, 0.50108361, 0.66344047, 0.52210402,\n",
       "        0.73350797, 0.68025651, 0.73190618, 0.50668879, 0.41960492,\n",
       "        0.58596549, 0.7072825 , 0.81358523, 0.37175918, 0.74091516,\n",
       "        0.61699533, 0.47565928, 0.56034098, 0.61999826, 0.68446069,\n",
       "        0.33872714, 0.44743209, 0.66624308, 0.41760292, 0.59297242,\n",
       "        0.76653976, 0.62099919, 0.5737536 , 0.67324986, 0.42020555,\n",
       "        0.20319614, 0.78575845, 0.63641415, 0.6424201 , 0.51409626,\n",
       "        0.7991714 , 0.71468959, 0.57315311, 0.49928198, 0.76533861,\n",
       "        0.75452857, 0.52270451, 0.82499614, 0.43461952, 0.43401899,\n",
       "        0.66844492, 0.83580651, 0.67264905, 0.678655  , 0.30429382,\n",
       "        0.32911773, 0.67264919, 0.83900976, 0.64262033, 0.48927193,\n",
       "        0.54792876, 0.85082107, 0.59297228, 0.40979557, 0.93690405,\n",
       "        0.25664778, 0.71509013, 0.52550735, 0.56574602, 0.76794124,\n",
       "        0.42360873, 0.67405043, 0.56894927, 0.60017934, 0.9184865 ,\n",
       "        0.54732828, 0.80838017, 0.65343099, 0.63120928, 0.80117321,\n",
       "        0.65443158, 0.74471889, 0.53211365, 0.6648417 , 0.7711442 ,\n",
       "        0.55153246, 0.7673408 , 0.53671799, 0.52150316, 0.79136381,\n",
       "        0.74011416, 0.68746362, 0.52770915, 0.64282026, 0.54892974,\n",
       "        0.91548347, 0.82239385, 0.85822821, 0.50548782, 0.88605533,\n",
       "        0.33772583, 0.73410859, 0.53771896, 0.60678558, 0.49768014,\n",
       "        0.53531685, 0.74772158, 0.56354389, 0.74451861, 0.57675653,\n",
       "        0.48126435, 0.86383367, 0.79296522, 0.66984639, 0.72890339,\n",
       "        0.73731112, 0.74992385, 0.61859703, 0.46965337, 0.73931351,\n",
       "        1.05321636, 0.9282958 , 0.70187721, 0.89526405, 0.54933019,\n",
       "        0.72530012, 0.73330789, 0.59717636, 0.86023002, 0.66143847,\n",
       "        0.81358523, 0.92449198, 0.95732412, 0.65463181, 0.76513848,\n",
       "        0.78936191, 0.77714992, 0.94671373, 0.57235255, 0.47746096,\n",
       "        0.7495234 , 0.53311453, 0.66764441, 0.6250031 , 0.64922681,\n",
       "        0.74692078, 1.06102395, 0.58816767, 0.41299877, 0.91948714,\n",
       "        0.65803547, 0.77915192, 0.91388183, 0.66924591, 0.8151866 ,\n",
       "        0.74772158, 0.66924572, 0.56854877, 0.73550997, 0.63541322,\n",
       "        0.69427013, 0.55053134, 0.69967551, 0.44663086, 0.57555556,\n",
       "        1.03459854, 0.69166756, 0.65323052, 0.7088841 , 0.69326901,\n",
       "        0.87844782, 0.57875862, 0.94170885, 0.69146733, 0.64442205,\n",
       "        0.55633717, 0.74972372, 0.85082097, 0.71569066, 0.6107892 ,\n",
       "        0.68105721, 0.56514564, 1.03920302, 0.47425771, 0.55753794,\n",
       "        0.46044455, 0.80257444, 0.84301348, 0.41800351, 0.56434484,\n",
       "        0.44462905, 0.55453515, 0.43321815, 0.55213275, 1.03539929,\n",
       "        0.81198382, 0.71388907, 0.76413774, 0.56234283, 0.87104077,\n",
       "        0.51769967, 0.76513824, 0.68225851, 0.7471211 , 0.44342799,\n",
       "        0.68646264, 0.71408911, 0.72329803, 0.71308813, 0.48987284,\n",
       "        0.43542032, 0.38036718, 0.24964118, 0.60798678, 0.5837636 ,\n",
       "        0.58016005, 0.30669589, 0.69767356, 0.99275832, 0.53231359,\n",
       "        0.65323057, 0.54973078, 0.51049275, 0.26865931, 0.62720547,\n",
       "        0.51749959, 0.38577237, 0.36194944, 0.5655457 , 0.37816515,\n",
       "        0.73831277, 0.34333158, 0.47025394, 0.34533339, 0.43261757,\n",
       "        0.38256927, 0.55533586, 0.57635608, 0.43622112, 0.86223249,\n",
       "        0.69326925, 0.74051476, 0.34873667, 0.3329217 , 0.41860414,\n",
       "        0.47125483, 0.28267303, 0.93189926, 0.38557215, 0.32711587,\n",
       "        0.43381853, 0.74992375, 0.50068302, 0.60618501, 0.26105204]),\n",
       " 'std_score_time': array([0.22930499, 0.19713767, 0.21526271, 0.27949319, 0.12593411,\n",
       "        0.1733913 , 0.28237697, 0.22119026, 0.30612353, 0.20610242,\n",
       "        0.04845575, 0.16581671, 0.10609554, 0.28694503, 0.23489373,\n",
       "        0.24768728, 0.14677409, 0.14520368, 0.31822261, 0.26132632,\n",
       "        0.29440766, 0.24424677, 0.22500585, 0.24091531, 0.29577669,\n",
       "        0.30736196, 0.23473155, 0.5069665 , 0.23549605, 0.31383016,\n",
       "        0.12942704, 0.25200019, 0.21909095, 0.24411815, 0.18853581,\n",
       "        0.25528353, 0.26072292, 0.28560955, 0.31146124, 0.21175775,\n",
       "        0.23005977, 0.2761936 , 0.22478498, 0.21236483, 0.28477955,\n",
       "        0.21054325, 0.26715521, 0.20378978, 0.22609165, 0.2266817 ,\n",
       "        0.32435475, 0.22509832, 0.28671505, 0.30027395, 0.24332062,\n",
       "        0.28546208, 0.3261661 , 0.31025725, 0.05235124, 0.30191592,\n",
       "        0.3399748 , 0.29700034, 0.19966137, 0.1947661 , 0.28250264,\n",
       "        0.26353717, 0.25851854, 0.16592443, 0.24952044, 0.24023383,\n",
       "        0.15428338, 0.16341573, 0.36709165, 0.25890041, 0.21749101,\n",
       "        0.29870887, 0.22828065, 0.25768791, 0.29268741, 0.24713716,\n",
       "        0.07430952, 0.25352162, 0.23596671, 0.28737357, 0.28092077,\n",
       "        0.29066696, 0.22750519, 0.2656319 , 0.29448103, 0.25834587,\n",
       "        0.24077882, 0.36298155, 0.27484442, 0.23796175, 0.21100934,\n",
       "        0.22043394, 0.24615711, 0.44579082, 0.22566312, 0.23281623,\n",
       "        0.12542615, 0.43638681, 0.32680292, 0.25276164, 0.08378827,\n",
       "        0.33400955, 0.34510727, 0.14306402, 0.1924194 , 0.19982753,\n",
       "        0.17811637, 0.22315096, 0.04209294, 0.13819622, 0.40604438,\n",
       "        0.132608  , 0.30670068, 0.01629545, 0.18860793, 0.22736312,\n",
       "        0.17608611, 0.25938182, 0.19844133, 0.29422983, 0.28336255,\n",
       "        0.25338965, 0.34836093, 0.28753516, 0.38471292, 0.34675667,\n",
       "        0.35137407, 0.19813603, 0.21931281, 0.02414817, 0.30460864,\n",
       "        0.34152895, 0.3083933 , 0.19801398, 0.28153343, 0.43117162,\n",
       "        0.17883494, 0.2622021 , 0.2606317 , 0.27625198, 0.4378572 ,\n",
       "        0.09627505, 0.27476966, 0.33416582, 0.23633621, 0.14783894,\n",
       "        0.33897826, 0.37673957, 0.18908653, 0.27648635, 0.19326818,\n",
       "        0.25427973, 0.42178402, 0.19473281, 0.30439271, 0.36246935,\n",
       "        0.17059101, 0.31602095, 0.18454738, 0.42419694, 0.34368408,\n",
       "        0.24381231, 0.28603853, 0.36638706, 0.23942858, 0.26623315,\n",
       "        0.43774035, 0.4777431 , 0.2494623 , 0.25141959, 0.18923021,\n",
       "        0.33125501, 0.2104527 , 0.17678835, 0.30737706, 0.23165953,\n",
       "        0.27839433, 0.19921243, 0.28404761, 0.3383913 , 0.22837306,\n",
       "        0.24334756, 0.23620469, 0.33189046, 0.19089519, 0.28542442,\n",
       "        0.27150768, 0.37486469, 0.25254092, 0.19083719, 0.36048505,\n",
       "        0.22641481, 0.37730599, 0.25727343, 0.27556177, 0.15181003,\n",
       "        0.32273797, 0.21003151, 0.14802658, 0.26436215, 0.36965735,\n",
       "        0.28483099, 0.23079046, 0.45930846, 0.29652823, 0.33416844,\n",
       "        0.23116253, 0.26050426, 0.24480084, 0.23985383, 0.26370812,\n",
       "        0.2431813 , 0.31972783, 0.24160688, 0.23745753, 0.22888891,\n",
       "        0.31710097, 0.26051815, 0.38499697, 0.32802069, 0.35762208,\n",
       "        0.25780625, 0.29111243, 0.07431288, 0.39546306, 0.24973334,\n",
       "        0.24534161, 0.24655202, 0.24492181, 0.10304118, 0.34349413,\n",
       "        0.09393181, 0.3422895 , 0.36644409, 0.24552336, 0.23515481,\n",
       "        0.13657571, 0.29459124, 0.21934863, 0.39434248, 0.5198189 ,\n",
       "        0.04889562, 0.2512204 , 0.26078454, 0.28270209, 0.33872664,\n",
       "        0.27857385, 0.24543271, 0.33203938, 0.20923835, 0.21389052,\n",
       "        0.17852362, 0.20320064, 0.15844192, 0.35529318, 0.25863019,\n",
       "        0.00826852, 0.18633494, 0.41357259, 0.37584727, 0.2741657 ,\n",
       "        0.26371903, 0.0199331 , 0.27208297, 0.14585621, 0.3389027 ,\n",
       "        0.31413186, 0.12045533, 0.27870929, 0.01769183, 0.09854991,\n",
       "        0.1829087 , 0.24899797, 0.06093067, 0.08795554, 0.27418818,\n",
       "        0.25037615, 0.32465297, 0.01252451, 0.18973552, 0.29734213,\n",
       "        0.22478186, 0.27930801, 0.16641189, 0.24556667, 0.24682908,\n",
       "        0.39575769, 0.23867226, 0.28962496, 0.23948461, 0.23917471,\n",
       "        0.23494769, 0.28724079, 0.16552656, 0.23139307, 0.17841647]),\n",
       " 'param_colsample_bytree': masked_array(data=[0.6000000000000001, 0.6000000000000001,\n",
       "                    0.7000000000000002, 0.4000000000000001,\n",
       "                    0.7000000000000002, 0.4000000000000001,\n",
       "                    0.5000000000000001, 0.2, 0.2, 0.4000000000000001,\n",
       "                    0.30000000000000004, 0.9000000000000001,\n",
       "                    0.5000000000000001, 0.2, 0.5000000000000001, 0.2,\n",
       "                    0.8000000000000003, 0.6000000000000001,\n",
       "                    0.4000000000000001, 0.8000000000000003,\n",
       "                    0.30000000000000004, 0.4000000000000001,\n",
       "                    0.8000000000000003, 0.2, 0.8000000000000003,\n",
       "                    0.9000000000000001, 0.8000000000000003,\n",
       "                    0.8000000000000003, 0.5000000000000001,\n",
       "                    0.6000000000000001, 0.8000000000000003,\n",
       "                    0.9000000000000001, 0.8000000000000003,\n",
       "                    0.7000000000000002, 0.9000000000000001,\n",
       "                    0.30000000000000004, 0.6000000000000001,\n",
       "                    0.8000000000000003, 0.4000000000000001,\n",
       "                    0.9000000000000001, 0.7000000000000002,\n",
       "                    0.4000000000000001, 0.4000000000000001,\n",
       "                    0.5000000000000001, 0.30000000000000004,\n",
       "                    0.30000000000000004, 0.8000000000000003,\n",
       "                    0.30000000000000004, 0.7000000000000002, 0.2,\n",
       "                    0.5000000000000001, 0.9000000000000001,\n",
       "                    0.5000000000000001, 0.6000000000000001,\n",
       "                    0.6000000000000001, 0.2, 0.4000000000000001,\n",
       "                    0.5000000000000001, 0.7000000000000002,\n",
       "                    0.4000000000000001, 0.30000000000000004,\n",
       "                    0.8000000000000003, 0.30000000000000004,\n",
       "                    0.9000000000000001, 0.4000000000000001,\n",
       "                    0.30000000000000004, 0.2, 0.9000000000000001,\n",
       "                    0.9000000000000001, 0.8000000000000003,\n",
       "                    0.30000000000000004, 0.7000000000000002,\n",
       "                    0.8000000000000003, 0.8000000000000003,\n",
       "                    0.4000000000000001, 0.9000000000000001,\n",
       "                    0.5000000000000001, 0.30000000000000004,\n",
       "                    0.9000000000000001, 0.9000000000000001,\n",
       "                    0.30000000000000004, 0.5000000000000001,\n",
       "                    0.30000000000000004, 0.30000000000000004,\n",
       "                    0.9000000000000001, 0.5000000000000001, 0.2,\n",
       "                    0.7000000000000002, 0.2, 0.5000000000000001,\n",
       "                    0.7000000000000002, 0.8000000000000003,\n",
       "                    0.4000000000000001, 0.30000000000000004,\n",
       "                    0.4000000000000001, 0.9000000000000001,\n",
       "                    0.7000000000000002, 0.7000000000000002,\n",
       "                    0.30000000000000004, 0.6000000000000001, 0.2,\n",
       "                    0.8000000000000003, 0.4000000000000001, 0.2,\n",
       "                    0.8000000000000003, 0.7000000000000002,\n",
       "                    0.4000000000000001, 0.5000000000000001,\n",
       "                    0.9000000000000001, 0.6000000000000001,\n",
       "                    0.7000000000000002, 0.30000000000000004,\n",
       "                    0.9000000000000001, 0.8000000000000003,\n",
       "                    0.30000000000000004, 0.8000000000000003,\n",
       "                    0.6000000000000001, 0.7000000000000002,\n",
       "                    0.4000000000000001, 0.30000000000000004,\n",
       "                    0.8000000000000003, 0.7000000000000002,\n",
       "                    0.8000000000000003, 0.4000000000000001,\n",
       "                    0.30000000000000004, 0.9000000000000001,\n",
       "                    0.8000000000000003, 0.2, 0.5000000000000001,\n",
       "                    0.7000000000000002, 0.8000000000000003,\n",
       "                    0.9000000000000001, 0.5000000000000001,\n",
       "                    0.30000000000000004, 0.4000000000000001,\n",
       "                    0.30000000000000004, 0.4000000000000001,\n",
       "                    0.4000000000000001, 0.4000000000000001,\n",
       "                    0.4000000000000001, 0.6000000000000001,\n",
       "                    0.5000000000000001, 0.7000000000000002,\n",
       "                    0.30000000000000004, 0.4000000000000001,\n",
       "                    0.4000000000000001, 0.2, 0.2, 0.6000000000000001, 0.2,\n",
       "                    0.9000000000000001, 0.9000000000000001,\n",
       "                    0.9000000000000001, 0.30000000000000004, 0.2,\n",
       "                    0.6000000000000001, 0.9000000000000001, 0.2,\n",
       "                    0.6000000000000001, 0.5000000000000001,\n",
       "                    0.9000000000000001, 0.7000000000000002,\n",
       "                    0.6000000000000001, 0.4000000000000001,\n",
       "                    0.9000000000000001, 0.6000000000000001,\n",
       "                    0.6000000000000001, 0.8000000000000003,\n",
       "                    0.4000000000000001, 0.30000000000000004,\n",
       "                    0.6000000000000001, 0.2, 0.30000000000000004,\n",
       "                    0.4000000000000001, 0.4000000000000001,\n",
       "                    0.9000000000000001, 0.7000000000000002,\n",
       "                    0.5000000000000001, 0.9000000000000001,\n",
       "                    0.5000000000000001, 0.4000000000000001, 0.2,\n",
       "                    0.9000000000000001, 0.5000000000000001,\n",
       "                    0.6000000000000001, 0.5000000000000001,\n",
       "                    0.7000000000000002, 0.4000000000000001,\n",
       "                    0.7000000000000002, 0.5000000000000001, 0.2,\n",
       "                    0.30000000000000004, 0.9000000000000001, 0.2,\n",
       "                    0.5000000000000001, 0.6000000000000001,\n",
       "                    0.5000000000000001, 0.2, 0.8000000000000003, 0.2,\n",
       "                    0.5000000000000001, 0.2, 0.4000000000000001, 0.2,\n",
       "                    0.9000000000000001, 0.30000000000000004, 0.2,\n",
       "                    0.9000000000000001, 0.2, 0.5000000000000001,\n",
       "                    0.9000000000000001, 0.8000000000000003,\n",
       "                    0.6000000000000001, 0.4000000000000001,\n",
       "                    0.8000000000000003, 0.2, 0.5000000000000001, 0.2, 0.2,\n",
       "                    0.6000000000000001, 0.5000000000000001,\n",
       "                    0.6000000000000001, 0.8000000000000003,\n",
       "                    0.5000000000000001, 0.4000000000000001,\n",
       "                    0.9000000000000001, 0.8000000000000003,\n",
       "                    0.9000000000000001, 0.4000000000000001,\n",
       "                    0.4000000000000001, 0.2, 0.2, 0.5000000000000001, 0.2,\n",
       "                    0.6000000000000001, 0.4000000000000001,\n",
       "                    0.4000000000000001, 0.4000000000000001,\n",
       "                    0.30000000000000004, 0.30000000000000004,\n",
       "                    0.9000000000000001, 0.9000000000000001, 0.2,\n",
       "                    0.30000000000000004, 0.9000000000000001,\n",
       "                    0.6000000000000001, 0.2, 0.4000000000000001, 0.2,\n",
       "                    0.30000000000000004, 0.7000000000000002,\n",
       "                    0.8000000000000003, 0.8000000000000003,\n",
       "                    0.9000000000000001, 0.8000000000000003,\n",
       "                    0.7000000000000002, 0.8000000000000003,\n",
       "                    0.5000000000000001, 0.4000000000000001,\n",
       "                    0.30000000000000004, 0.4000000000000001,\n",
       "                    0.6000000000000001, 0.7000000000000002,\n",
       "                    0.4000000000000001, 0.8000000000000003,\n",
       "                    0.5000000000000001, 0.30000000000000004,\n",
       "                    0.7000000000000002, 0.6000000000000001,\n",
       "                    0.6000000000000001, 0.8000000000000003,\n",
       "                    0.5000000000000001, 0.8000000000000003,\n",
       "                    0.9000000000000001, 0.2, 0.30000000000000004,\n",
       "                    0.4000000000000001, 0.5000000000000001,\n",
       "                    0.5000000000000001, 0.2, 0.30000000000000004,\n",
       "                    0.30000000000000004, 0.4000000000000001,\n",
       "                    0.4000000000000001, 0.5000000000000001,\n",
       "                    0.7000000000000002, 0.9000000000000001,\n",
       "                    0.30000000000000004, 0.9000000000000001,\n",
       "                    0.6000000000000001, 0.4000000000000001,\n",
       "                    0.30000000000000004, 0.8000000000000003,\n",
       "                    0.4000000000000001, 0.2, 0.8000000000000003,\n",
       "                    0.8000000000000003, 0.8000000000000003,\n",
       "                    0.4000000000000001, 0.5000000000000001],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_eta': masked_array(data=[0.7413342480535051, 0.3360520118925199,\n",
       "                    0.19422083027537873, 0.1791957414320903,\n",
       "                    0.8907252318500077, 0.6936699623591323,\n",
       "                    0.8466860580643427, 0.8804638850767094,\n",
       "                    0.16191675010712914, 0.5318965813815931,\n",
       "                    0.7629654466931809, 0.18694285802578703,\n",
       "                    0.7797734065751928, 0.552062173655583,\n",
       "                    0.28277532367104086, 0.30992523525847837,\n",
       "                    0.26117335213889226, 0.2904601838724157,\n",
       "                    0.5510263032819386, 0.3158621176294205,\n",
       "                    0.48137539793449846, 0.2863383862338372,\n",
       "                    0.3041671663197334, 0.5091240508680961,\n",
       "                    0.7247744414490415, 0.6848646547827363,\n",
       "                    0.16348528638871562, 0.4989917588348419,\n",
       "                    0.8601742731132722, 0.6475161908920557,\n",
       "                    0.5606306392496586, 0.3882693325329317,\n",
       "                    0.6938120626156581, 0.633794276696836,\n",
       "                    0.6304829406318666, 0.8969506682280936,\n",
       "                    0.2794042186256814, 0.2462882243688232,\n",
       "                    0.5442962505184047, 0.6694355903425444,\n",
       "                    0.5408636292306167, 0.6991583411849157,\n",
       "                    0.5122228897236095, 0.11106500128706465,\n",
       "                    0.15733920499664367, 0.12590351605243227,\n",
       "                    0.507177037615785, 0.3111616826359753,\n",
       "                    0.3369538486827165, 0.5115270204512633,\n",
       "                    0.15881269145652813, 0.6005728205583825,\n",
       "                    0.36783406693139187, 0.419805574648623,\n",
       "                    0.6089877578547015, 0.8526315879365232,\n",
       "                    0.22047814494360535, 0.43982253609857713,\n",
       "                    0.7324923126939126, 0.16758797892990857,\n",
       "                    0.22502094734357742, 0.2643634926386764,\n",
       "                    0.16469252995469974, 0.6544788619733125,\n",
       "                    0.569212588537276, 0.2695053981361067,\n",
       "                    0.1558340008525918, 0.8194231066658774,\n",
       "                    0.5729560839950798, 0.6951982769129552,\n",
       "                    0.8976088513310531, 0.29895486452450437,\n",
       "                    0.5408783083857145, 0.5495525047112562,\n",
       "                    0.6079244985721958, 0.4012561246342594,\n",
       "                    0.3974546047221099, 0.4255395477381453,\n",
       "                    0.5852973162487732, 0.1464038287559542,\n",
       "                    0.5614120109150286, 0.16154544492638367,\n",
       "                    0.8480020908179052, 0.8824466030742328,\n",
       "                    0.3479195397872141, 0.3498428299407661,\n",
       "                    0.29321660360181034, 0.851789016939606,\n",
       "                    0.12029394832523979, 0.4694820812251801,\n",
       "                    0.2891509950416402, 0.726002049712162,\n",
       "                    0.412036995936402, 0.7419904757381942,\n",
       "                    0.8002655332233117, 0.48952369549426744,\n",
       "                    0.5067429678766243, 0.2707327480311088,\n",
       "                    0.27112123822323675, 0.5877073725958549,\n",
       "                    0.3631501118692796, 0.6433788977940536,\n",
       "                    0.11508536866111135, 0.725445711833762,\n",
       "                    0.38141574715644755, 0.4134208330904403,\n",
       "                    0.18099321363875848, 0.680266948242825,\n",
       "                    0.5534889943341327, 0.8308481881219253,\n",
       "                    0.6438229670280304, 0.2425647740694693,\n",
       "                    0.17540424603703011, 0.2390514409528482,\n",
       "                    0.1801613306880813, 0.4552343626277454,\n",
       "                    0.15869012654046247, 0.8029754694052041,\n",
       "                    0.7988586769025948, 0.6296710285483559,\n",
       "                    0.3968073142989993, 0.7466709904612854,\n",
       "                    0.5749410910799908, 0.11642002062133337,\n",
       "                    0.8623787166702053, 0.5078878115256925,\n",
       "                    0.17565059485244375, 0.8520413034407038,\n",
       "                    0.3489280111294333, 0.6972525594152981,\n",
       "                    0.33633407322858266, 0.26614054126230247,\n",
       "                    0.11169157863613909, 0.8758454305885852,\n",
       "                    0.7889798210553651, 0.4414071028848786,\n",
       "                    0.17373182630788345, 0.814059958489463,\n",
       "                    0.3359783074363626, 0.6629642986709641,\n",
       "                    0.8066531830942437, 0.5160576215723577,\n",
       "                    0.17050674882493944, 0.39295779364181793,\n",
       "                    0.778210659836553, 0.7593918474512433,\n",
       "                    0.35922136598262244, 0.48436553689373074,\n",
       "                    0.8773714664715472, 0.6935800746877178,\n",
       "                    0.297206371752117, 0.49159638932206307,\n",
       "                    0.8347650409374189, 0.6596869763549239,\n",
       "                    0.6561835115020502, 0.5126658935926601,\n",
       "                    0.48397324602325886, 0.7171432637957667,\n",
       "                    0.7626237937110016, 0.18506136395232203,\n",
       "                    0.8875387926066307, 0.2460056030672722,\n",
       "                    0.35225932629794254, 0.5251494567669934,\n",
       "                    0.5825581225819246, 0.6195849086419075,\n",
       "                    0.1345618311449492, 0.5349810709944606,\n",
       "                    0.19954995573933731, 0.4497753867979798,\n",
       "                    0.8977636841483815, 0.6555368195322938,\n",
       "                    0.5695402475869887, 0.7465153152250606,\n",
       "                    0.5196464143089287, 0.24911321753068405,\n",
       "                    0.1829811537907224, 0.2347231831174459,\n",
       "                    0.6289479440984249, 0.12179847593127091,\n",
       "                    0.5776120947499208, 0.12466379891729212,\n",
       "                    0.8969931604665183, 0.6409300603771406,\n",
       "                    0.5087288121285809, 0.6173885228811786,\n",
       "                    0.7420427658162391, 0.42604358436733536,\n",
       "                    0.2313700235809062, 0.46372731361529296,\n",
       "                    0.3339003998832132, 0.8602070724416611,\n",
       "                    0.7805928744310909, 0.3558640181508287,\n",
       "                    0.6308505683548842, 0.23785776469851028,\n",
       "                    0.16236441635578638, 0.2641431341276761,\n",
       "                    0.10714123484079466, 0.14525677783220445,\n",
       "                    0.32163278326845, 0.539008223547531,\n",
       "                    0.7619688775811085, 0.8723117050927903,\n",
       "                    0.2943829543057964, 0.18124967647759985,\n",
       "                    0.17401637274515452, 0.5730448079720071,\n",
       "                    0.5791597828532704, 0.4013392938836621,\n",
       "                    0.30813876356479175, 0.3012823938546708,\n",
       "                    0.7922092018582602, 0.5005759121210643,\n",
       "                    0.2919513146437066, 0.11875846703030267,\n",
       "                    0.8058057473850194, 0.7191376137838347,\n",
       "                    0.6419850126794466, 0.3629866726562734,\n",
       "                    0.7861617684008553, 0.6401962486991339,\n",
       "                    0.12379322210359787, 0.893814371959418,\n",
       "                    0.8669729536801625, 0.42668191396485766,\n",
       "                    0.32871738897837566, 0.3306114254655399,\n",
       "                    0.7706786373098249, 0.5325953139006316,\n",
       "                    0.4355819080752723, 0.17599596440702606,\n",
       "                    0.686379043657186, 0.4695751857730789,\n",
       "                    0.5208098789503465, 0.20640574958071564,\n",
       "                    0.14438403503050756, 0.5629774694430487,\n",
       "                    0.5347024620809224, 0.5361642929480243,\n",
       "                    0.28029467729667235, 0.8843962011388784,\n",
       "                    0.6006465457235308, 0.39955504534515585,\n",
       "                    0.1328167170399338, 0.6834569809239898,\n",
       "                    0.5499518522639881, 0.509520605767073,\n",
       "                    0.1949076433588638, 0.28518890370005023,\n",
       "                    0.5366470545074065, 0.7836120856451155,\n",
       "                    0.6548588052969151, 0.42199835688680354,\n",
       "                    0.8614431266165407, 0.7285283839154004,\n",
       "                    0.8037860298093317, 0.8317363821703684,\n",
       "                    0.26323087777301507, 0.8010657082058616,\n",
       "                    0.5928993713413531, 0.20017928669314344,\n",
       "                    0.45583471896599315, 0.5247178707160589,\n",
       "                    0.4666718692981936, 0.3444983581844995,\n",
       "                    0.3658640494887091, 0.5595108042783247,\n",
       "                    0.6743552811134613, 0.3242471013175867,\n",
       "                    0.35334475744484695, 0.8138034559320617,\n",
       "                    0.7981534355678294, 0.1127238750309898,\n",
       "                    0.7356568477145906, 0.7995424896398632,\n",
       "                    0.612552875228495, 0.13541785200282996,\n",
       "                    0.10310612065324216, 0.7536630359414763,\n",
       "                    0.5742277568118551, 0.8501530056531486,\n",
       "                    0.1429229399429618, 0.5621644830248167,\n",
       "                    0.12204668216784925, 0.5236372085944567,\n",
       "                    0.5748183706776107, 0.4459536541553898,\n",
       "                    0.6120366920597556, 0.7197988124405972,\n",
       "                    0.350081356500215, 0.2783645512781543,\n",
       "                    0.6369151235928865, 0.862346105227504,\n",
       "                    0.5613432007350374, 0.8018352339293168,\n",
       "                    0.6625812451403134, 0.2722883132903846,\n",
       "                    0.11613198959988989, 0.24016792097525821],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_gamma': masked_array(data=[2.3599976400951923, 2.443172023089311,\n",
       "                    1.0018147117050014, 0.8761843344481772,\n",
       "                    1.638584016806924, 0.4298286030392752,\n",
       "                    1.215635606877189, 1.2941612895214216,\n",
       "                    1.1987357729086467, 1.2277763973418756,\n",
       "                    2.6487300969038357, 1.8794071228163596,\n",
       "                    0.32392957946984696, 1.105066197039471,\n",
       "                    0.5327341097138466, 1.4004043988980783,\n",
       "                    1.3789229590307295, 2.660721285582703,\n",
       "                    2.865792597718236, 0.9779328266052172,\n",
       "                    0.1018160041432203, 2.277024516567566,\n",
       "                    2.013919848111815, 2.5807792490440224,\n",
       "                    0.8117359050055188, 2.644277694102617,\n",
       "                    2.0243481316075878, 2.4213015732857555,\n",
       "                    0.3067612131412499, 3.044727898395045,\n",
       "                    2.6316023819904406, 0.7346672417886687,\n",
       "                    0.9981361992624451, 1.9317009409590409,\n",
       "                    1.7904537141220909, 2.0653936917914013,\n",
       "                    1.7800048228801815, 2.4387311527687143,\n",
       "                    0.7992625362214324, 0.07950582656443501,\n",
       "                    2.7488538691374043, 2.652717255270129,\n",
       "                    1.0122865956847116, 1.8614928620349134,\n",
       "                    2.8587052440767127, 1.833031110393536,\n",
       "                    1.1860456936050257, 0.23053120044812486,\n",
       "                    1.9098483995480555, 0.3858682820787755,\n",
       "                    0.2841010512040543, 2.772190921772001,\n",
       "                    2.1427178883334936, 1.6754923996311584,\n",
       "                    2.36399839929158, 2.145730640528532,\n",
       "                    1.1294041176488567, 1.0301407228393036,\n",
       "                    2.1150938087020714, 0.32172712100675144,\n",
       "                    1.722489240247073, 2.4822574263842334,\n",
       "                    2.7140670905562048, 0.7037371191875703,\n",
       "                    2.668766353346288, 1.3110530986709554,\n",
       "                    0.9917017703998476, 0.6768570224149731,\n",
       "                    2.2707617230766624, 2.137270016831671,\n",
       "                    2.7335598323436736, 1.6765644307923318,\n",
       "                    1.8647916434170158, 0.37247118841696675,\n",
       "                    0.8658090222662366, 2.934440820744792,\n",
       "                    0.9946493370864642, 1.1034723292113429,\n",
       "                    0.7690702965986492, 2.843294520128703,\n",
       "                    2.268581709323341, 2.4883804509134384,\n",
       "                    2.5189944308584384, 0.91886766344905,\n",
       "                    2.314442752886443, 1.982378285120155,\n",
       "                    1.6044660693997965, 2.3138279623801883,\n",
       "                    1.8287024357941066, 2.6985401446733492,\n",
       "                    0.2644039666719393, 2.546120493793873,\n",
       "                    1.4507512170927785, 0.3352512472062485,\n",
       "                    0.9355631324985194, 2.3050348390183197,\n",
       "                    2.7616196874150076, 1.68547533783959,\n",
       "                    1.4699338400242354, 0.24375096991436146,\n",
       "                    1.4677917998811285, 2.474645292194842,\n",
       "                    2.0553829468005267, 0.8249327880825373,\n",
       "                    0.27617636109050353, 0.8933612946730319,\n",
       "                    1.579776003462603, 0.3210517785865998,\n",
       "                    2.524486635979765, 2.2003685851218124,\n",
       "                    0.9234355451230514, 0.21725808754417059,\n",
       "                    2.5019135148235447, 0.8128036215411429,\n",
       "                    0.4664028445159492, 2.7248228077942285,\n",
       "                    0.6626819759290514, 1.7920509068185586,\n",
       "                    2.8605683142857505, 1.5463286203176376,\n",
       "                    2.5131408259351957, 0.5558107863388634,\n",
       "                    1.6065392411998694, 1.6889862207408133,\n",
       "                    2.039197713592501, 2.3063374752430157,\n",
       "                    2.711393992102685, 2.9993710202032715,\n",
       "                    1.9057549748102678, 1.396110860509627,\n",
       "                    1.592055598488596, 2.7625442606417376,\n",
       "                    1.5548033444765053, 0.8437929467480797,\n",
       "                    1.821808936364659, 1.79652476197463, 3.008605398528274,\n",
       "                    1.8879712439023486, 1.7032807490141126,\n",
       "                    2.7015689127739773, 2.7772843563412826,\n",
       "                    2.5312700866631745, 2.8910402099802273,\n",
       "                    1.0651670185231976, 2.9811406610217324,\n",
       "                    0.7289319572598256, 1.130727049475298,\n",
       "                    1.9718194223801544, 1.0681024777854062,\n",
       "                    2.092353619990147, 2.288132008533525,\n",
       "                    1.2927164784425018, 1.2213750258882996,\n",
       "                    0.36605151648469664, 2.991724181148207,\n",
       "                    1.784703587536029, 1.4929267760746534,\n",
       "                    2.805661270172382, 2.463229264160505,\n",
       "                    3.0082480231754216, 0.6146487597441355,\n",
       "                    0.12317490115501357, 2.0288099666198587,\n",
       "                    1.7783038364786443, 0.905874102773282,\n",
       "                    1.8082969932742488, 1.9917784365845834,\n",
       "                    1.277776278314865, 0.6684199004203148,\n",
       "                    1.035772122171717, 0.9666468197598561,\n",
       "                    2.474957676682568, 1.0704659739453966,\n",
       "                    2.8669364282020693, 0.8595759862429773,\n",
       "                    0.22966515849809793, 0.4806260160653942,\n",
       "                    1.4384471725693377, 1.8503416165173225,\n",
       "                    3.045180523047898, 2.000050633822617,\n",
       "                    2.176864565876066, 2.9158514438724725,\n",
       "                    1.2162280984985243, 1.043833846873524,\n",
       "                    0.3158292731138273, 1.0114110701156418,\n",
       "                    0.9720366727556207, 1.7665746009914305,\n",
       "                    1.8711189082159916, 2.2518184706899644,\n",
       "                    1.262245853947683, 0.5784424233951909,\n",
       "                    0.4104390599431535, 1.4611800383399174,\n",
       "                    2.2348603328328145, 1.7297116796570886,\n",
       "                    0.2409980233802957, 1.1606262727666532,\n",
       "                    2.1116632423960056, 1.3750503365931606,\n",
       "                    2.0165829575463063, 0.7755582264323241,\n",
       "                    2.8517209589748655, 0.06693886153853297,\n",
       "                    2.117276358788009, 2.128923759336442,\n",
       "                    1.2484597289563373, 0.5958859060914213,\n",
       "                    2.9712898115742687, 0.6549841435853611,\n",
       "                    1.5139681955794309, 2.771614591584603,\n",
       "                    0.9237152806387781, 1.4424146950469598,\n",
       "                    1.4336053260716228, 0.9795784866007762,\n",
       "                    1.7488678077420388, 1.4860327336994508,\n",
       "                    1.9339738967389397, 0.4878460371187479,\n",
       "                    1.627479653771257, 0.32638080732210556,\n",
       "                    0.6814991209440597, 2.9323151413397577,\n",
       "                    1.9834706390233567, 1.4369265248248417,\n",
       "                    1.5790422355500324, 2.1213969785334412,\n",
       "                    2.537341321523175, 0.9882153575401229,\n",
       "                    2.2643076726435654, 2.5113035633130343,\n",
       "                    1.7354942620085214, 2.1883484668673328,\n",
       "                    0.7743152843630987, 1.2970107747679573,\n",
       "                    0.6777202402126364, 0.9821573725778135,\n",
       "                    0.6664817538519097, 2.212010952882472,\n",
       "                    1.5339812523975869, 2.878446357829725,\n",
       "                    1.8556636028002402, 0.2473959276982574,\n",
       "                    2.19716426542252, 0.8737171108419276,\n",
       "                    2.1774419833800107, 1.9744452707014941,\n",
       "                    2.434369125981914, 1.7983582863893661,\n",
       "                    1.2753027328013515, 2.618679743106584,\n",
       "                    2.729239823528659, 0.6997424399500813,\n",
       "                    2.310818101584896, 1.6096793371571254,\n",
       "                    2.2707818067860983, 2.1550395985772433,\n",
       "                    0.45005650400342073, 1.8685451406680402,\n",
       "                    2.7739948451089416, 1.0849793257877045,\n",
       "                    1.508103733922198, 2.969327794368384,\n",
       "                    1.6543991718242426, 2.7785352532628127,\n",
       "                    2.1915817778523965, 0.7867728939904219,\n",
       "                    2.285702940311724, 2.1082012579503644,\n",
       "                    0.8645318312220551, 0.9298601641305769,\n",
       "                    0.6785468083649832, 2.319165361847813,\n",
       "                    0.9980426517880293, 0.9189697689062,\n",
       "                    2.0590631240355326, 1.4716898017949502,\n",
       "                    1.5762144679361965, 2.345537209956281,\n",
       "                    2.092563530898945, 0.5936577851355563,\n",
       "                    1.2339861918588257, 0.1775430604376615,\n",
       "                    2.7907064173354432, 1.8654779255392053,\n",
       "                    2.252623671765733, 1.435880590943367,\n",
       "                    0.8945763679785066, 0.2859723334372169,\n",
       "                    0.834545723127197, 2.9860483560576796,\n",
       "                    2.878728950115069, 3.018377882660834,\n",
       "                    1.9805341367089546, 1.7686829888494024,\n",
       "                    0.4570044669244591, 1.167597473001239,\n",
       "                    0.13960944516616763],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_lambda': masked_array(data=[2.391943568277041, 3.363851535665577,\n",
       "                    4.567864398314624, 3.9721365455226327,\n",
       "                    4.237936411714312, 4.214386395214215,\n",
       "                    4.479697199220523, 4.636230121010764,\n",
       "                    0.7321492676648478, 0.5841487590066491,\n",
       "                    4.50619999030982, 2.3952841965991887,\n",
       "                    0.6516539769573032, 3.156578608916684,\n",
       "                    0.6479505717896838, 3.151584105187577,\n",
       "                    1.7962022875985462, 2.2184875041388388,\n",
       "                    1.3626218416959874, 2.2864511646088297,\n",
       "                    2.344075525341323, 1.1056083136152788,\n",
       "                    0.11304505776051577, 4.186968225772612,\n",
       "                    3.257767365691379, 2.3732452781836844,\n",
       "                    1.1052420411787005, 2.355057301673926,\n",
       "                    0.6338461606810303, 4.3582852211583765,\n",
       "                    2.1802200221368944, 4.697869719680846,\n",
       "                    0.2022911386522641, 1.6956842076077006,\n",
       "                    4.395483425809361, 1.0245145013661905,\n",
       "                    3.9480130847088786, 4.255452466991097,\n",
       "                    3.756376217600545, 2.7082240848812402,\n",
       "                    3.889301557037539, 2.5421593727642557,\n",
       "                    1.512468179244371, 1.0109214457776816,\n",
       "                    4.877137969267348, 4.807753416976545,\n",
       "                    0.13580960885523408, 1.7873554621066816,\n",
       "                    3.241137404100154, 4.237454160048706,\n",
       "                    1.0979289670207792, 2.7610392838859257,\n",
       "                    0.615943430441564, 0.8509582013653527,\n",
       "                    2.179823929616484, 3.3156272050210167,\n",
       "                    3.312920675669136, 1.0391881640486866,\n",
       "                    2.2718428849833527, 4.001708905896095,\n",
       "                    3.2908163343850965, 0.7339178801561391,\n",
       "                    1.405874216774583, 1.2982916824447377,\n",
       "                    4.291962892280817, 1.2435844652099788,\n",
       "                    4.5816386887719505, 1.9078721894895305,\n",
       "                    2.2900069526908355, 2.1161827310219645,\n",
       "                    3.033666739409418, 3.9147868626410642,\n",
       "                    3.0073779324140224, 2.535194554183209,\n",
       "                    3.693130589147542, 1.5127921227666712,\n",
       "                    4.925123873473173, 4.187337812172258,\n",
       "                    1.9641199639781015, 0.27469807025470805,\n",
       "                    4.338263896429692, 3.7688201960147873,\n",
       "                    2.127603549239482, 1.881524913042203,\n",
       "                    0.6160094254524168, 4.598702586597832,\n",
       "                    0.6843005454946077, 2.989626601730135,\n",
       "                    2.645316876190111, 1.8192187738928622,\n",
       "                    0.6443816608455338, 2.7835665480944565,\n",
       "                    0.22461197673714905, 4.222241706296631,\n",
       "                    2.1372809784165008, 0.754130636383824,\n",
       "                    3.8432470927638427, 3.0910192442257394,\n",
       "                    3.9501696183122235, 4.429422447042147,\n",
       "                    2.521781230206434, 3.007905450663149,\n",
       "                    1.9441840912089892, 1.019763338985928,\n",
       "                    0.6678711381795549, 4.419984903518624,\n",
       "                    1.9121368996175347, 2.4737650523687376,\n",
       "                    1.9104869259229507, 1.9667978265279684,\n",
       "                    2.908291789058258, 0.7818491478499673,\n",
       "                    2.081043789275796, 0.4885984833786894,\n",
       "                    1.3609016427155818, 0.5760165532177297,\n",
       "                    1.5349496777177056, 2.791488238009347,\n",
       "                    0.440811610169784, 2.0679951287851024,\n",
       "                    3.9576968067368283, 2.042018047463169,\n",
       "                    0.5275627545890177, 1.1553557650982333,\n",
       "                    4.6898138074273374, 2.625445207491422,\n",
       "                    3.978778411842777, 4.222102347210564,\n",
       "                    0.7749003588505732, 1.4661252401204812,\n",
       "                    1.4826523180229345, 1.2739780110941301,\n",
       "                    1.3143545688016178, 4.58856135431915,\n",
       "                    1.9814093978412128, 2.215230287385924,\n",
       "                    1.99112509947518, 1.8945768210697294,\n",
       "                    3.723924572908629, 2.7316949606815717,\n",
       "                    3.1750576447010785, 0.632374661800903,\n",
       "                    1.8187934657481737, 3.524610275986752,\n",
       "                    0.3418756169982762, 3.5050186987255216,\n",
       "                    1.39370594917947, 2.7286813290723986,\n",
       "                    1.7511464737046722, 4.43089233504884,\n",
       "                    4.056276028374638, 0.6619111595655025,\n",
       "                    0.6007493532607233, 1.6167779007251248,\n",
       "                    3.1008789041123346, 0.6841778413750199,\n",
       "                    3.8352752789706743, 1.69062687391837,\n",
       "                    4.554378872368142, 2.7325622670810796,\n",
       "                    0.6606821007425079, 4.537027865459427,\n",
       "                    3.747180356695325, 4.160782809461244,\n",
       "                    0.7438985192613756, 0.055903174121065824,\n",
       "                    3.068625574061487, 0.6771912977071193,\n",
       "                    1.594048333376955, 4.768328142575007,\n",
       "                    3.3969081703049584, 4.404147720751101,\n",
       "                    1.455537513991404, 3.0830092246063696,\n",
       "                    1.265026686360677, 4.072942079203426,\n",
       "                    2.679466570884344, 2.3971701823965486,\n",
       "                    3.9617962599835836, 2.8419851471659445,\n",
       "                    4.711872575890077, 1.9763854844112179,\n",
       "                    1.755562180950891, 1.456402898887284,\n",
       "                    1.6399904131425425, 3.4691882487552057,\n",
       "                    3.0888532768881922, 1.5649997552701973,\n",
       "                    1.7231024683328506, 4.2554610948202445,\n",
       "                    3.936641060200441, 0.9002191259793435,\n",
       "                    0.38937803474464183, 1.6025169578919636,\n",
       "                    1.8961959851059977, 0.1374006940258382,\n",
       "                    2.4376487805595692, 3.6702389216808013,\n",
       "                    4.268439845884284, 3.1419477058582475,\n",
       "                    0.9578911163170839, 4.028998494504491,\n",
       "                    4.262466587195901, 2.759875750641168,\n",
       "                    1.2928519338882016, 2.0282101827674435,\n",
       "                    3.3664499130580054, 0.6655067084801741,\n",
       "                    3.6069866321960005, 2.9250017672187067,\n",
       "                    4.701951191022603, 0.38015971849683805,\n",
       "                    1.3174833674962083, 3.850556895995367,\n",
       "                    4.897893148174609, 4.3772465034815164,\n",
       "                    0.6825244891981541, 3.3630810457978018,\n",
       "                    3.8632084878445068, 0.239200510836185,\n",
       "                    4.207106185634178, 3.370440432137127,\n",
       "                    3.0967933303221136, 1.3996065941322777,\n",
       "                    0.012988940043445196, 3.145387002834777,\n",
       "                    4.892850372087451, 3.1363258909008,\n",
       "                    0.44558177684010547, 1.655742604151853,\n",
       "                    1.6919194963683877, 1.8337434784457591,\n",
       "                    2.40119464897034, 1.7573445991222596,\n",
       "                    1.8944412907317265, 1.0080494256477646,\n",
       "                    3.981089834501998, 1.33392088032657,\n",
       "                    3.0748823800020912, 1.748808121205665,\n",
       "                    3.083619070477842, 0.3226918907395532,\n",
       "                    3.7538500690526835, 2.254181107333644,\n",
       "                    4.897873042501125, 3.9261730155645482,\n",
       "                    0.7820724970353765, 0.8860861839971645,\n",
       "                    4.630472702658747, 4.94147197729797, 4.508022153679546,\n",
       "                    1.265109211264115, 2.932038291519822,\n",
       "                    0.9177905149506904, 4.918994229193132,\n",
       "                    1.5759611474339357, 1.8452040824446359,\n",
       "                    3.0026613061693586, 2.599997596900386,\n",
       "                    1.0510077092813552, 2.6679370730186296,\n",
       "                    2.930045986228112, 0.3659343161681372,\n",
       "                    4.745954849994289, 2.455046710827356,\n",
       "                    3.8715136941664112, 0.580015750198265,\n",
       "                    4.994965553879423, 3.028152824051346,\n",
       "                    3.112500394397114, 2.3134651379976825,\n",
       "                    4.085448794309105, 3.101237057769381,\n",
       "                    4.394178844502867, 2.189273362667112,\n",
       "                    2.1531735034840898, 0.1329067162079295,\n",
       "                    3.579025487546201, 0.20784224477608082,\n",
       "                    3.355864545489131, 2.7188733580338424,\n",
       "                    1.3248835701937822, 0.8370444121994136,\n",
       "                    2.3368295583739234, 0.11511617882112801,\n",
       "                    2.2071043065168485, 1.983805170924791,\n",
       "                    3.089233858249303, 3.221784174455826,\n",
       "                    1.4691867696555354, 2.6762396401869504,\n",
       "                    1.90536067288771, 0.10947961597440325,\n",
       "                    2.5876006187981386, 4.145566305769694,\n",
       "                    2.7678271811861848, 2.862134916527718,\n",
       "                    4.027662391364425, 0.3068094700104479,\n",
       "                    0.4265135792864627],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[45, 60, 30, 70, 60, 40, 60, 5, 75, 65, 20, 75, 15, 5,\n",
       "                    5, 60, 30, 45, 70, 60, 55, 50, 20, 25, 35, 10, 55, 25,\n",
       "                    75, 65, 5, 40, 40, 45, 5, 35, 10, 5, 25, 5, 30, 35, 40,\n",
       "                    60, 55, 60, 25, 25, 5, 10, 45, 5, 65, 75, 75, 20, 20,\n",
       "                    25, 20, 20, 25, 70, 30, 40, 50, 30, 45, 70, 20, 70, 45,\n",
       "                    20, 70, 70, 75, 35, 25, 60, 20, 50, 50, 50, 15, 75, 20,\n",
       "                    5, 5, 65, 30, 75, 45, 50, 40, 60, 40, 15, 5, 15, 55,\n",
       "                    35, 35, 75, 30, 30, 25, 30, 75, 40, 60, 10, 20, 40, 15,\n",
       "                    40, 20, 20, 45, 10, 55, 40, 10, 60, 45, 30, 5, 10, 65,\n",
       "                    65, 75, 65, 40, 30, 10, 30, 75, 70, 65, 65, 35, 30, 45,\n",
       "                    10, 75, 70, 60, 55, 60, 70, 55, 60, 75, 15, 40, 20, 75,\n",
       "                    60, 55, 60, 40, 5, 35, 25, 20, 5, 45, 5, 5, 20, 40, 15,\n",
       "                    75, 60, 30, 5, 55, 25, 25, 60, 45, 5, 30, 55, 5, 40,\n",
       "                    40, 5, 65, 75, 70, 50, 30, 5, 35, 40, 15, 55, 25, 5,\n",
       "                    40, 25, 35, 65, 60, 30, 25, 15, 55, 70, 55, 25, 5, 45,\n",
       "                    30, 60, 60, 10, 25, 10, 45, 65, 10, 75, 30, 70, 25, 65,\n",
       "                    55, 75, 40, 45, 65, 40, 55, 35, 45, 30, 65, 60, 60, 5,\n",
       "                    20, 25, 15, 25, 65, 10, 70, 55, 35, 60, 45, 75, 35, 20,\n",
       "                    50, 25, 25, 70, 50, 15, 5, 40, 45, 40, 40, 65, 10, 70,\n",
       "                    25, 40, 25, 60, 15, 10, 50, 45, 30, 10, 20, 45, 65, 55,\n",
       "                    5, 30, 10, 5, 25, 25, 25, 50, 75, 60, 5, 45, 25, 70,\n",
       "                    10, 70, 35, 75],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[60, 60, 45, 65, 65, 65, 50, 45, 50, 60, 15, 50, 35, 55,\n",
       "                    45, 45, 70, 30, 50, 10, 10, 10, 10, 70, 25, 55, 45, 50,\n",
       "                    60, 50, 55, 65, 75, 75, 50, 55, 60, 65, 20, 75, 15, 40,\n",
       "                    15, 60, 15, 30, 40, 20, 45, 60, 50, 60, 15, 15, 15, 15,\n",
       "                    35, 40, 15, 50, 65, 70, 15, 70, 70, 70, 55, 65, 35, 60,\n",
       "                    45, 30, 75, 70, 20, 10, 10, 40, 20, 35, 50, 50, 15, 25,\n",
       "                    65, 75, 70, 10, 55, 50, 45, 50, 10, 45, 40, 10, 25, 55,\n",
       "                    20, 65, 50, 25, 45, 40, 10, 40, 75, 50, 25, 35, 40, 10,\n",
       "                    15, 25, 65, 35, 55, 15, 60, 70, 70, 65, 25, 75, 35, 75,\n",
       "                    45, 30, 15, 50, 25, 65, 70, 20, 50, 45, 55, 25, 20, 45,\n",
       "                    50, 30, 35, 75, 60, 65, 70, 75, 25, 20, 20, 70, 55, 55,\n",
       "                    50, 25, 10, 75, 30, 70, 10, 25, 25, 25, 60, 60, 30, 70,\n",
       "                    15, 70, 55, 45, 45, 45, 45, 70, 40, 60, 10, 20, 30, 50,\n",
       "                    10, 50, 25, 50, 45, 75, 35, 20, 10, 75, 30, 50, 75, 35,\n",
       "                    55, 45, 35, 65, 25, 55, 40, 15, 60, 45, 45, 30, 70, 35,\n",
       "                    70, 65, 40, 65, 15, 15, 35, 65, 30, 25, 70, 70, 35, 30,\n",
       "                    70, 50, 45, 75, 60, 75, 70, 50, 50, 45, 65, 10, 75, 50,\n",
       "                    15, 70, 60, 65, 35, 45, 55, 50, 70, 45, 15, 65, 75, 55,\n",
       "                    50, 50, 55, 45, 50, 30, 55, 20, 10, 75, 55, 20, 25, 55,\n",
       "                    25, 20, 45, 20, 20, 25, 75, 30, 75, 55, 60, 55, 35, 45,\n",
       "                    35, 70, 75, 25, 65, 25, 10, 25, 60, 50, 40, 35, 60, 75,\n",
       "                    40, 50, 30, 25, 10, 70],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_subsample': masked_array(data=[0.5, 0.8999999999999999, 0.7, 0.7, 0.8999999999999999,\n",
       "                    0.8999999999999999, 0.6, 0.7, 0.5, 0.7999999999999999,\n",
       "                    0.7, 0.8999999999999999, 0.6, 0.5, 0.7, 0.6, 0.6,\n",
       "                    0.7999999999999999, 0.6, 0.6, 0.7, 0.7, 0.5,\n",
       "                    0.7999999999999999, 0.8999999999999999, 0.5, 0.5, 0.5,\n",
       "                    0.6, 0.5, 0.5, 0.7, 0.6, 0.6, 0.8999999999999999, 0.7,\n",
       "                    0.5, 0.8999999999999999, 0.7999999999999999,\n",
       "                    0.8999999999999999, 0.7, 0.5, 0.6, 0.7,\n",
       "                    0.8999999999999999, 0.8999999999999999, 0.5,\n",
       "                    0.7999999999999999, 0.8999999999999999, 0.6,\n",
       "                    0.7999999999999999, 0.5, 0.8999999999999999, 0.5, 0.6,\n",
       "                    0.7, 0.5, 0.5, 0.7, 0.7999999999999999, 0.7, 0.7,\n",
       "                    0.7999999999999999, 0.6, 0.7999999999999999, 0.7, 0.6,\n",
       "                    0.5, 0.6, 0.7999999999999999, 0.6, 0.7999999999999999,\n",
       "                    0.6, 0.6, 0.5, 0.6, 0.8999999999999999, 0.7, 0.6,\n",
       "                    0.7999999999999999, 0.5, 0.8999999999999999, 0.6, 0.6,\n",
       "                    0.7999999999999999, 0.8999999999999999,\n",
       "                    0.7999999999999999, 0.7, 0.8999999999999999,\n",
       "                    0.8999999999999999, 0.7, 0.6, 0.7999999999999999, 0.7,\n",
       "                    0.8999999999999999, 0.7999999999999999,\n",
       "                    0.7999999999999999, 0.5, 0.7999999999999999, 0.7, 0.5,\n",
       "                    0.6, 0.5, 0.5, 0.6, 0.6, 0.5, 0.7, 0.8999999999999999,\n",
       "                    0.6, 0.8999999999999999, 0.5, 0.6, 0.7,\n",
       "                    0.7999999999999999, 0.7, 0.6, 0.7999999999999999, 0.7,\n",
       "                    0.6, 0.6, 0.8999999999999999, 0.6, 0.5,\n",
       "                    0.8999999999999999, 0.7, 0.5, 0.7, 0.7, 0.5, 0.6,\n",
       "                    0.8999999999999999, 0.6, 0.5, 0.6, 0.7,\n",
       "                    0.7999999999999999, 0.8999999999999999,\n",
       "                    0.8999999999999999, 0.7999999999999999, 0.6, 0.6,\n",
       "                    0.8999999999999999, 0.5, 0.5, 0.7, 0.8999999999999999,\n",
       "                    0.8999999999999999, 0.7999999999999999, 0.7,\n",
       "                    0.8999999999999999, 0.6, 0.6, 0.8999999999999999,\n",
       "                    0.8999999999999999, 0.8999999999999999, 0.6, 0.7, 0.6,\n",
       "                    0.5, 0.7999999999999999, 0.7, 0.8999999999999999, 0.7,\n",
       "                    0.6, 0.8999999999999999, 0.6, 0.7, 0.5, 0.7,\n",
       "                    0.7999999999999999, 0.5, 0.7999999999999999,\n",
       "                    0.7999999999999999, 0.8999999999999999, 0.7,\n",
       "                    0.7999999999999999, 0.5, 0.6, 0.6, 0.6,\n",
       "                    0.8999999999999999, 0.5, 0.8999999999999999, 0.7, 0.6,\n",
       "                    0.8999999999999999, 0.7999999999999999,\n",
       "                    0.7999999999999999, 0.6, 0.6, 0.6, 0.7999999999999999,\n",
       "                    0.6, 0.5, 0.8999999999999999, 0.5, 0.6, 0.6, 0.5, 0.5,\n",
       "                    0.6, 0.6, 0.5, 0.7999999999999999, 0.8999999999999999,\n",
       "                    0.5, 0.8999999999999999, 0.5, 0.7999999999999999,\n",
       "                    0.8999999999999999, 0.7, 0.8999999999999999, 0.6, 0.7,\n",
       "                    0.7, 0.5, 0.7999999999999999, 0.7, 0.8999999999999999,\n",
       "                    0.7999999999999999, 0.8999999999999999, 0.5, 0.6,\n",
       "                    0.8999999999999999, 0.7999999999999999, 0.5, 0.6, 0.7,\n",
       "                    0.7999999999999999, 0.5, 0.7999999999999999,\n",
       "                    0.8999999999999999, 0.5, 0.6, 0.5, 0.8999999999999999,\n",
       "                    0.7, 0.7999999999999999, 0.8999999999999999, 0.5, 0.6,\n",
       "                    0.7, 0.7999999999999999, 0.7, 0.6, 0.7, 0.5, 0.6, 0.5,\n",
       "                    0.7, 0.8999999999999999, 0.6, 0.5, 0.8999999999999999,\n",
       "                    0.5, 0.8999999999999999, 0.8999999999999999, 0.5, 0.6,\n",
       "                    0.8999999999999999, 0.6, 0.5, 0.7, 0.7,\n",
       "                    0.7999999999999999, 0.7999999999999999, 0.7,\n",
       "                    0.8999999999999999, 0.7, 0.7, 0.7999999999999999, 0.6,\n",
       "                    0.6, 0.5, 0.8999999999999999, 0.6, 0.6, 0.5,\n",
       "                    0.8999999999999999, 0.8999999999999999, 0.6, 0.6, 0.7,\n",
       "                    0.8999999999999999, 0.7999999999999999, 0.6, 0.5, 0.5,\n",
       "                    0.7999999999999999, 0.7, 0.8999999999999999,\n",
       "                    0.8999999999999999, 0.5, 0.7, 0.5, 0.6,\n",
       "                    0.8999999999999999, 0.8999999999999999,\n",
       "                    0.7999999999999999],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'colsample_bytree': 0.6000000000000001,\n",
       "   'eta': 0.7413342480535051,\n",
       "   'gamma': 2.3599976400951923,\n",
       "   'lambda': 2.391943568277041,\n",
       "   'max_depth': 45,\n",
       "   'n_estimators': 60,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.6000000000000001,\n",
       "   'eta': 0.3360520118925199,\n",
       "   'gamma': 2.443172023089311,\n",
       "   'lambda': 3.363851535665577,\n",
       "   'max_depth': 60,\n",
       "   'n_estimators': 60,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.7000000000000002,\n",
       "   'eta': 0.19422083027537873,\n",
       "   'gamma': 1.0018147117050014,\n",
       "   'lambda': 4.567864398314624,\n",
       "   'max_depth': 30,\n",
       "   'n_estimators': 45,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.1791957414320903,\n",
       "   'gamma': 0.8761843344481772,\n",
       "   'lambda': 3.9721365455226327,\n",
       "   'max_depth': 70,\n",
       "   'n_estimators': 65,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.7000000000000002,\n",
       "   'eta': 0.8907252318500077,\n",
       "   'gamma': 1.638584016806924,\n",
       "   'lambda': 4.237936411714312,\n",
       "   'max_depth': 60,\n",
       "   'n_estimators': 65,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.6936699623591323,\n",
       "   'gamma': 0.4298286030392752,\n",
       "   'lambda': 4.214386395214215,\n",
       "   'max_depth': 40,\n",
       "   'n_estimators': 65,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.8466860580643427,\n",
       "   'gamma': 1.215635606877189,\n",
       "   'lambda': 4.479697199220523,\n",
       "   'max_depth': 60,\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.8804638850767094,\n",
       "   'gamma': 1.2941612895214216,\n",
       "   'lambda': 4.636230121010764,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 45,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.16191675010712914,\n",
       "   'gamma': 1.1987357729086467,\n",
       "   'lambda': 0.7321492676648478,\n",
       "   'max_depth': 75,\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.5318965813815931,\n",
       "   'gamma': 1.2277763973418756,\n",
       "   'lambda': 0.5841487590066491,\n",
       "   'max_depth': 65,\n",
       "   'n_estimators': 60,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.7629654466931809,\n",
       "   'gamma': 2.6487300969038357,\n",
       "   'lambda': 4.50619999030982,\n",
       "   'max_depth': 20,\n",
       "   'n_estimators': 15,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.18694285802578703,\n",
       "   'gamma': 1.8794071228163596,\n",
       "   'lambda': 2.3952841965991887,\n",
       "   'max_depth': 75,\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.7797734065751928,\n",
       "   'gamma': 0.32392957946984696,\n",
       "   'lambda': 0.6516539769573032,\n",
       "   'max_depth': 15,\n",
       "   'n_estimators': 35,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.552062173655583,\n",
       "   'gamma': 1.105066197039471,\n",
       "   'lambda': 3.156578608916684,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 55,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.28277532367104086,\n",
       "   'gamma': 0.5327341097138466,\n",
       "   'lambda': 0.6479505717896838,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 45,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.30992523525847837,\n",
       "   'gamma': 1.4004043988980783,\n",
       "   'lambda': 3.151584105187577,\n",
       "   'max_depth': 60,\n",
       "   'n_estimators': 45,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.26117335213889226,\n",
       "   'gamma': 1.3789229590307295,\n",
       "   'lambda': 1.7962022875985462,\n",
       "   'max_depth': 30,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.6000000000000001,\n",
       "   'eta': 0.2904601838724157,\n",
       "   'gamma': 2.660721285582703,\n",
       "   'lambda': 2.2184875041388388,\n",
       "   'max_depth': 45,\n",
       "   'n_estimators': 30,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.5510263032819386,\n",
       "   'gamma': 2.865792597718236,\n",
       "   'lambda': 1.3626218416959874,\n",
       "   'max_depth': 70,\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.3158621176294205,\n",
       "   'gamma': 0.9779328266052172,\n",
       "   'lambda': 2.2864511646088297,\n",
       "   'max_depth': 60,\n",
       "   'n_estimators': 10,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.48137539793449846,\n",
       "   'gamma': 0.1018160041432203,\n",
       "   'lambda': 2.344075525341323,\n",
       "   'max_depth': 55,\n",
       "   'n_estimators': 10,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.2863383862338372,\n",
       "   'gamma': 2.277024516567566,\n",
       "   'lambda': 1.1056083136152788,\n",
       "   'max_depth': 50,\n",
       "   'n_estimators': 10,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.3041671663197334,\n",
       "   'gamma': 2.013919848111815,\n",
       "   'lambda': 0.11304505776051577,\n",
       "   'max_depth': 20,\n",
       "   'n_estimators': 10,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.5091240508680961,\n",
       "   'gamma': 2.5807792490440224,\n",
       "   'lambda': 4.186968225772612,\n",
       "   'max_depth': 25,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.7247744414490415,\n",
       "   'gamma': 0.8117359050055188,\n",
       "   'lambda': 3.257767365691379,\n",
       "   'max_depth': 35,\n",
       "   'n_estimators': 25,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.6848646547827363,\n",
       "   'gamma': 2.644277694102617,\n",
       "   'lambda': 2.3732452781836844,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 55,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.16348528638871562,\n",
       "   'gamma': 2.0243481316075878,\n",
       "   'lambda': 1.1052420411787005,\n",
       "   'max_depth': 55,\n",
       "   'n_estimators': 45,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.4989917588348419,\n",
       "   'gamma': 2.4213015732857555,\n",
       "   'lambda': 2.355057301673926,\n",
       "   'max_depth': 25,\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.8601742731132722,\n",
       "   'gamma': 0.3067612131412499,\n",
       "   'lambda': 0.6338461606810303,\n",
       "   'max_depth': 75,\n",
       "   'n_estimators': 60,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.6000000000000001,\n",
       "   'eta': 0.6475161908920557,\n",
       "   'gamma': 3.044727898395045,\n",
       "   'lambda': 4.3582852211583765,\n",
       "   'max_depth': 65,\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.5606306392496586,\n",
       "   'gamma': 2.6316023819904406,\n",
       "   'lambda': 2.1802200221368944,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 55,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.3882693325329317,\n",
       "   'gamma': 0.7346672417886687,\n",
       "   'lambda': 4.697869719680846,\n",
       "   'max_depth': 40,\n",
       "   'n_estimators': 65,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.6938120626156581,\n",
       "   'gamma': 0.9981361992624451,\n",
       "   'lambda': 0.2022911386522641,\n",
       "   'max_depth': 40,\n",
       "   'n_estimators': 75,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.7000000000000002,\n",
       "   'eta': 0.633794276696836,\n",
       "   'gamma': 1.9317009409590409,\n",
       "   'lambda': 1.6956842076077006,\n",
       "   'max_depth': 45,\n",
       "   'n_estimators': 75,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.6304829406318666,\n",
       "   'gamma': 1.7904537141220909,\n",
       "   'lambda': 4.395483425809361,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.8969506682280936,\n",
       "   'gamma': 2.0653936917914013,\n",
       "   'lambda': 1.0245145013661905,\n",
       "   'max_depth': 35,\n",
       "   'n_estimators': 55,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.6000000000000001,\n",
       "   'eta': 0.2794042186256814,\n",
       "   'gamma': 1.7800048228801815,\n",
       "   'lambda': 3.9480130847088786,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 60,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.2462882243688232,\n",
       "   'gamma': 2.4387311527687143,\n",
       "   'lambda': 4.255452466991097,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 65,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.5442962505184047,\n",
       "   'gamma': 0.7992625362214324,\n",
       "   'lambda': 3.756376217600545,\n",
       "   'max_depth': 25,\n",
       "   'n_estimators': 20,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.6694355903425444,\n",
       "   'gamma': 0.07950582656443501,\n",
       "   'lambda': 2.7082240848812402,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 75,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.7000000000000002,\n",
       "   'eta': 0.5408636292306167,\n",
       "   'gamma': 2.7488538691374043,\n",
       "   'lambda': 3.889301557037539,\n",
       "   'max_depth': 30,\n",
       "   'n_estimators': 15,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.6991583411849157,\n",
       "   'gamma': 2.652717255270129,\n",
       "   'lambda': 2.5421593727642557,\n",
       "   'max_depth': 35,\n",
       "   'n_estimators': 40,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.5122228897236095,\n",
       "   'gamma': 1.0122865956847116,\n",
       "   'lambda': 1.512468179244371,\n",
       "   'max_depth': 40,\n",
       "   'n_estimators': 15,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.11106500128706465,\n",
       "   'gamma': 1.8614928620349134,\n",
       "   'lambda': 1.0109214457776816,\n",
       "   'max_depth': 60,\n",
       "   'n_estimators': 60,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.15733920499664367,\n",
       "   'gamma': 2.8587052440767127,\n",
       "   'lambda': 4.877137969267348,\n",
       "   'max_depth': 55,\n",
       "   'n_estimators': 15,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.12590351605243227,\n",
       "   'gamma': 1.833031110393536,\n",
       "   'lambda': 4.807753416976545,\n",
       "   'max_depth': 60,\n",
       "   'n_estimators': 30,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.507177037615785,\n",
       "   'gamma': 1.1860456936050257,\n",
       "   'lambda': 0.13580960885523408,\n",
       "   'max_depth': 25,\n",
       "   'n_estimators': 40,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.3111616826359753,\n",
       "   'gamma': 0.23053120044812486,\n",
       "   'lambda': 1.7873554621066816,\n",
       "   'max_depth': 25,\n",
       "   'n_estimators': 20,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.7000000000000002,\n",
       "   'eta': 0.3369538486827165,\n",
       "   'gamma': 1.9098483995480555,\n",
       "   'lambda': 3.241137404100154,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 45,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.5115270204512633,\n",
       "   'gamma': 0.3858682820787755,\n",
       "   'lambda': 4.237454160048706,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 60,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.15881269145652813,\n",
       "   'gamma': 0.2841010512040543,\n",
       "   'lambda': 1.0979289670207792,\n",
       "   'max_depth': 45,\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.6005728205583825,\n",
       "   'gamma': 2.772190921772001,\n",
       "   'lambda': 2.7610392838859257,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 60,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.36783406693139187,\n",
       "   'gamma': 2.1427178883334936,\n",
       "   'lambda': 0.615943430441564,\n",
       "   'max_depth': 65,\n",
       "   'n_estimators': 15,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.6000000000000001,\n",
       "   'eta': 0.419805574648623,\n",
       "   'gamma': 1.6754923996311584,\n",
       "   'lambda': 0.8509582013653527,\n",
       "   'max_depth': 75,\n",
       "   'n_estimators': 15,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.6000000000000001,\n",
       "   'eta': 0.6089877578547015,\n",
       "   'gamma': 2.36399839929158,\n",
       "   'lambda': 2.179823929616484,\n",
       "   'max_depth': 75,\n",
       "   'n_estimators': 15,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.8526315879365232,\n",
       "   'gamma': 2.145730640528532,\n",
       "   'lambda': 3.3156272050210167,\n",
       "   'max_depth': 20,\n",
       "   'n_estimators': 15,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.22047814494360535,\n",
       "   'gamma': 1.1294041176488567,\n",
       "   'lambda': 3.312920675669136,\n",
       "   'max_depth': 20,\n",
       "   'n_estimators': 35,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.43982253609857713,\n",
       "   'gamma': 1.0301407228393036,\n",
       "   'lambda': 1.0391881640486866,\n",
       "   'max_depth': 25,\n",
       "   'n_estimators': 40,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.7000000000000002,\n",
       "   'eta': 0.7324923126939126,\n",
       "   'gamma': 2.1150938087020714,\n",
       "   'lambda': 2.2718428849833527,\n",
       "   'max_depth': 20,\n",
       "   'n_estimators': 15,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.16758797892990857,\n",
       "   'gamma': 0.32172712100675144,\n",
       "   'lambda': 4.001708905896095,\n",
       "   'max_depth': 20,\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.22502094734357742,\n",
       "   'gamma': 1.722489240247073,\n",
       "   'lambda': 3.2908163343850965,\n",
       "   'max_depth': 25,\n",
       "   'n_estimators': 65,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.2643634926386764,\n",
       "   'gamma': 2.4822574263842334,\n",
       "   'lambda': 0.7339178801561391,\n",
       "   'max_depth': 70,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.16469252995469974,\n",
       "   'gamma': 2.7140670905562048,\n",
       "   'lambda': 1.405874216774583,\n",
       "   'max_depth': 30,\n",
       "   'n_estimators': 15,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.6544788619733125,\n",
       "   'gamma': 0.7037371191875703,\n",
       "   'lambda': 1.2982916824447377,\n",
       "   'max_depth': 40,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.569212588537276,\n",
       "   'gamma': 2.668766353346288,\n",
       "   'lambda': 4.291962892280817,\n",
       "   'max_depth': 50,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.2695053981361067,\n",
       "   'gamma': 1.3110530986709554,\n",
       "   'lambda': 1.2435844652099788,\n",
       "   'max_depth': 30,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.1558340008525918,\n",
       "   'gamma': 0.9917017703998476,\n",
       "   'lambda': 4.5816386887719505,\n",
       "   'max_depth': 45,\n",
       "   'n_estimators': 55,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.8194231066658774,\n",
       "   'gamma': 0.6768570224149731,\n",
       "   'lambda': 1.9078721894895305,\n",
       "   'max_depth': 70,\n",
       "   'n_estimators': 65,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.5729560839950798,\n",
       "   'gamma': 2.2707617230766624,\n",
       "   'lambda': 2.2900069526908355,\n",
       "   'max_depth': 20,\n",
       "   'n_estimators': 35,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.6951982769129552,\n",
       "   'gamma': 2.137270016831671,\n",
       "   'lambda': 2.1161827310219645,\n",
       "   'max_depth': 70,\n",
       "   'n_estimators': 60,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.8976088513310531,\n",
       "   'gamma': 2.7335598323436736,\n",
       "   'lambda': 3.033666739409418,\n",
       "   'max_depth': 45,\n",
       "   'n_estimators': 45,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.7000000000000002,\n",
       "   'eta': 0.29895486452450437,\n",
       "   'gamma': 1.6765644307923318,\n",
       "   'lambda': 3.9147868626410642,\n",
       "   'max_depth': 20,\n",
       "   'n_estimators': 30,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.5408783083857145,\n",
       "   'gamma': 1.8647916434170158,\n",
       "   'lambda': 3.0073779324140224,\n",
       "   'max_depth': 70,\n",
       "   'n_estimators': 75,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.5495525047112562,\n",
       "   'gamma': 0.37247118841696675,\n",
       "   'lambda': 2.535194554183209,\n",
       "   'max_depth': 70,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.6079244985721958,\n",
       "   'gamma': 0.8658090222662366,\n",
       "   'lambda': 3.693130589147542,\n",
       "   'max_depth': 75,\n",
       "   'n_estimators': 20,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.4012561246342594,\n",
       "   'gamma': 2.934440820744792,\n",
       "   'lambda': 1.5127921227666712,\n",
       "   'max_depth': 35,\n",
       "   'n_estimators': 10,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.3974546047221099,\n",
       "   'gamma': 0.9946493370864642,\n",
       "   'lambda': 4.925123873473173,\n",
       "   'max_depth': 25,\n",
       "   'n_estimators': 10,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.4255395477381453,\n",
       "   'gamma': 1.1034723292113429,\n",
       "   'lambda': 4.187337812172258,\n",
       "   'max_depth': 60,\n",
       "   'n_estimators': 40,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.5852973162487732,\n",
       "   'gamma': 0.7690702965986492,\n",
       "   'lambda': 1.9641199639781015,\n",
       "   'max_depth': 20,\n",
       "   'n_estimators': 20,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.1464038287559542,\n",
       "   'gamma': 2.843294520128703,\n",
       "   'lambda': 0.27469807025470805,\n",
       "   'max_depth': 50,\n",
       "   'n_estimators': 35,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.5614120109150286,\n",
       "   'gamma': 2.268581709323341,\n",
       "   'lambda': 4.338263896429692,\n",
       "   'max_depth': 50,\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.16154544492638367,\n",
       "   'gamma': 2.4883804509134384,\n",
       "   'lambda': 3.7688201960147873,\n",
       "   'max_depth': 50,\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.8480020908179052,\n",
       "   'gamma': 2.5189944308584384,\n",
       "   'lambda': 2.127603549239482,\n",
       "   'max_depth': 15,\n",
       "   'n_estimators': 15,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.8824466030742328,\n",
       "   'gamma': 0.91886766344905,\n",
       "   'lambda': 1.881524913042203,\n",
       "   'max_depth': 75,\n",
       "   'n_estimators': 25,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.3479195397872141,\n",
       "   'gamma': 2.314442752886443,\n",
       "   'lambda': 0.6160094254524168,\n",
       "   'max_depth': 20,\n",
       "   'n_estimators': 65,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.3498428299407661,\n",
       "   'gamma': 1.982378285120155,\n",
       "   'lambda': 4.598702586597832,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 75,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.29321660360181034,\n",
       "   'gamma': 1.6044660693997965,\n",
       "   'lambda': 0.6843005454946077,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.7000000000000002,\n",
       "   'eta': 0.851789016939606,\n",
       "   'gamma': 2.3138279623801883,\n",
       "   'lambda': 2.989626601730135,\n",
       "   'max_depth': 65,\n",
       "   'n_estimators': 10,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.12029394832523979,\n",
       "   'gamma': 1.8287024357941066,\n",
       "   'lambda': 2.645316876190111,\n",
       "   'max_depth': 30,\n",
       "   'n_estimators': 55,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.4694820812251801,\n",
       "   'gamma': 2.6985401446733492,\n",
       "   'lambda': 1.8192187738928622,\n",
       "   'max_depth': 75,\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.7000000000000002,\n",
       "   'eta': 0.2891509950416402,\n",
       "   'gamma': 0.2644039666719393,\n",
       "   'lambda': 0.6443816608455338,\n",
       "   'max_depth': 45,\n",
       "   'n_estimators': 45,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.726002049712162,\n",
       "   'gamma': 2.546120493793873,\n",
       "   'lambda': 2.7835665480944565,\n",
       "   'max_depth': 50,\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.412036995936402,\n",
       "   'gamma': 1.4507512170927785,\n",
       "   'lambda': 0.22461197673714905,\n",
       "   'max_depth': 40,\n",
       "   'n_estimators': 10,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.7419904757381942,\n",
       "   'gamma': 0.3352512472062485,\n",
       "   'lambda': 4.222241706296631,\n",
       "   'max_depth': 60,\n",
       "   'n_estimators': 45,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.8002655332233117,\n",
       "   'gamma': 0.9355631324985194,\n",
       "   'lambda': 2.1372809784165008,\n",
       "   'max_depth': 40,\n",
       "   'n_estimators': 40,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.48952369549426744,\n",
       "   'gamma': 2.3050348390183197,\n",
       "   'lambda': 0.754130636383824,\n",
       "   'max_depth': 15,\n",
       "   'n_estimators': 10,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.7000000000000002,\n",
       "   'eta': 0.5067429678766243,\n",
       "   'gamma': 2.7616196874150076,\n",
       "   'lambda': 3.8432470927638427,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.7000000000000002,\n",
       "   'eta': 0.2707327480311088,\n",
       "   'gamma': 1.68547533783959,\n",
       "   'lambda': 3.0910192442257394,\n",
       "   'max_depth': 15,\n",
       "   'n_estimators': 55,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.27112123822323675,\n",
       "   'gamma': 1.4699338400242354,\n",
       "   'lambda': 3.9501696183122235,\n",
       "   'max_depth': 55,\n",
       "   'n_estimators': 20,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.6000000000000001,\n",
       "   'eta': 0.5877073725958549,\n",
       "   'gamma': 0.24375096991436146,\n",
       "   'lambda': 4.429422447042147,\n",
       "   'max_depth': 35,\n",
       "   'n_estimators': 65,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.3631501118692796,\n",
       "   'gamma': 1.4677917998811285,\n",
       "   'lambda': 2.521781230206434,\n",
       "   'max_depth': 35,\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.6433788977940536,\n",
       "   'gamma': 2.474645292194842,\n",
       "   'lambda': 3.007905450663149,\n",
       "   'max_depth': 75,\n",
       "   'n_estimators': 25,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.11508536866111135,\n",
       "   'gamma': 2.0553829468005267,\n",
       "   'lambda': 1.9441840912089892,\n",
       "   'max_depth': 30,\n",
       "   'n_estimators': 45,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.725445711833762,\n",
       "   'gamma': 0.8249327880825373,\n",
       "   'lambda': 1.019763338985928,\n",
       "   'max_depth': 30,\n",
       "   'n_estimators': 40,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.38141574715644755,\n",
       "   'gamma': 0.27617636109050353,\n",
       "   'lambda': 0.6678711381795549,\n",
       "   'max_depth': 25,\n",
       "   'n_estimators': 10,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.7000000000000002,\n",
       "   'eta': 0.4134208330904403,\n",
       "   'gamma': 0.8933612946730319,\n",
       "   'lambda': 4.419984903518624,\n",
       "   'max_depth': 30,\n",
       "   'n_estimators': 40,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.18099321363875848,\n",
       "   'gamma': 1.579776003462603,\n",
       "   'lambda': 1.9121368996175347,\n",
       "   'max_depth': 75,\n",
       "   'n_estimators': 75,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.680266948242825,\n",
       "   'gamma': 0.3210517785865998,\n",
       "   'lambda': 2.4737650523687376,\n",
       "   'max_depth': 40,\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.5534889943341327,\n",
       "   'gamma': 2.524486635979765,\n",
       "   'lambda': 1.9104869259229507,\n",
       "   'max_depth': 60,\n",
       "   'n_estimators': 25,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.6000000000000001,\n",
       "   'eta': 0.8308481881219253,\n",
       "   'gamma': 2.2003685851218124,\n",
       "   'lambda': 1.9667978265279684,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 35,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.7000000000000002,\n",
       "   'eta': 0.6438229670280304,\n",
       "   'gamma': 0.9234355451230514,\n",
       "   'lambda': 2.908291789058258,\n",
       "   'max_depth': 20,\n",
       "   'n_estimators': 40,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.2425647740694693,\n",
       "   'gamma': 0.21725808754417059,\n",
       "   'lambda': 0.7818491478499673,\n",
       "   'max_depth': 40,\n",
       "   'n_estimators': 10,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.17540424603703011,\n",
       "   'gamma': 2.5019135148235447,\n",
       "   'lambda': 2.081043789275796,\n",
       "   'max_depth': 15,\n",
       "   'n_estimators': 15,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.2390514409528482,\n",
       "   'gamma': 0.8128036215411429,\n",
       "   'lambda': 0.4885984833786894,\n",
       "   'max_depth': 40,\n",
       "   'n_estimators': 25,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.1801613306880813,\n",
       "   'gamma': 0.4664028445159492,\n",
       "   'lambda': 1.3609016427155818,\n",
       "   'max_depth': 20,\n",
       "   'n_estimators': 65,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.4552343626277454,\n",
       "   'gamma': 2.7248228077942285,\n",
       "   'lambda': 0.5760165532177297,\n",
       "   'max_depth': 20,\n",
       "   'n_estimators': 35,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.6000000000000001,\n",
       "   'eta': 0.15869012654046247,\n",
       "   'gamma': 0.6626819759290514,\n",
       "   'lambda': 1.5349496777177056,\n",
       "   'max_depth': 45,\n",
       "   'n_estimators': 55,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.7000000000000002,\n",
       "   'eta': 0.8029754694052041,\n",
       "   'gamma': 1.7920509068185586,\n",
       "   'lambda': 2.791488238009347,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 15,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.7988586769025948,\n",
       "   'gamma': 2.8605683142857505,\n",
       "   'lambda': 0.440811610169784,\n",
       "   'max_depth': 55,\n",
       "   'n_estimators': 60,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.6296710285483559,\n",
       "   'gamma': 1.5463286203176376,\n",
       "   'lambda': 2.0679951287851024,\n",
       "   'max_depth': 40,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.3968073142989993,\n",
       "   'gamma': 2.5131408259351957,\n",
       "   'lambda': 3.9576968067368283,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.7000000000000002,\n",
       "   'eta': 0.7466709904612854,\n",
       "   'gamma': 0.5558107863388634,\n",
       "   'lambda': 2.042018047463169,\n",
       "   'max_depth': 60,\n",
       "   'n_estimators': 65,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.5749410910799908,\n",
       "   'gamma': 1.6065392411998694,\n",
       "   'lambda': 0.5275627545890177,\n",
       "   'max_depth': 45,\n",
       "   'n_estimators': 25,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.11642002062133337,\n",
       "   'gamma': 1.6889862207408133,\n",
       "   'lambda': 1.1553557650982333,\n",
       "   'max_depth': 30,\n",
       "   'n_estimators': 75,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.8623787166702053,\n",
       "   'gamma': 2.039197713592501,\n",
       "   'lambda': 4.6898138074273374,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 35,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.5078878115256925,\n",
       "   'gamma': 2.3063374752430157,\n",
       "   'lambda': 2.625445207491422,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 75,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.17565059485244375,\n",
       "   'gamma': 2.711393992102685,\n",
       "   'lambda': 3.978778411842777,\n",
       "   'max_depth': 65,\n",
       "   'n_estimators': 45,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.8520413034407038,\n",
       "   'gamma': 2.9993710202032715,\n",
       "   'lambda': 4.222102347210564,\n",
       "   'max_depth': 65,\n",
       "   'n_estimators': 30,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.3489280111294333,\n",
       "   'gamma': 1.9057549748102678,\n",
       "   'lambda': 0.7749003588505732,\n",
       "   'max_depth': 75,\n",
       "   'n_estimators': 15,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.7000000000000002,\n",
       "   'eta': 0.6972525594152981,\n",
       "   'gamma': 1.396110860509627,\n",
       "   'lambda': 1.4661252401204812,\n",
       "   'max_depth': 65,\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.33633407322858266,\n",
       "   'gamma': 1.592055598488596,\n",
       "   'lambda': 1.4826523180229345,\n",
       "   'max_depth': 40,\n",
       "   'n_estimators': 25,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.26614054126230247,\n",
       "   'gamma': 2.7625442606417376,\n",
       "   'lambda': 1.2739780110941301,\n",
       "   'max_depth': 30,\n",
       "   'n_estimators': 65,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.11169157863613909,\n",
       "   'gamma': 1.5548033444765053,\n",
       "   'lambda': 1.3143545688016178,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.8758454305885852,\n",
       "   'gamma': 0.8437929467480797,\n",
       "   'lambda': 4.58856135431915,\n",
       "   'max_depth': 30,\n",
       "   'n_estimators': 20,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.7889798210553651,\n",
       "   'gamma': 1.821808936364659,\n",
       "   'lambda': 1.9814093978412128,\n",
       "   'max_depth': 75,\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.4414071028848786,\n",
       "   'gamma': 1.79652476197463,\n",
       "   'lambda': 2.215230287385924,\n",
       "   'max_depth': 70,\n",
       "   'n_estimators': 45,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.17373182630788345,\n",
       "   'gamma': 3.008605398528274,\n",
       "   'lambda': 1.99112509947518,\n",
       "   'max_depth': 65,\n",
       "   'n_estimators': 55,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.814059958489463,\n",
       "   'gamma': 1.8879712439023486,\n",
       "   'lambda': 1.8945768210697294,\n",
       "   'max_depth': 65,\n",
       "   'n_estimators': 25,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.3359783074363626,\n",
       "   'gamma': 1.7032807490141126,\n",
       "   'lambda': 3.723924572908629,\n",
       "   'max_depth': 35,\n",
       "   'n_estimators': 20,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.6629642986709641,\n",
       "   'gamma': 2.7015689127739773,\n",
       "   'lambda': 2.7316949606815717,\n",
       "   'max_depth': 30,\n",
       "   'n_estimators': 45,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.6000000000000001,\n",
       "   'eta': 0.8066531830942437,\n",
       "   'gamma': 2.7772843563412826,\n",
       "   'lambda': 3.1750576447010785,\n",
       "   'max_depth': 45,\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.5160576215723577,\n",
       "   'gamma': 2.5312700866631745,\n",
       "   'lambda': 0.632374661800903,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 30,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.7000000000000002,\n",
       "   'eta': 0.17050674882493944,\n",
       "   'gamma': 2.8910402099802273,\n",
       "   'lambda': 1.8187934657481737,\n",
       "   'max_depth': 75,\n",
       "   'n_estimators': 35,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.39295779364181793,\n",
       "   'gamma': 1.0651670185231976,\n",
       "   'lambda': 3.524610275986752,\n",
       "   'max_depth': 70,\n",
       "   'n_estimators': 75,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.778210659836553,\n",
       "   'gamma': 2.9811406610217324,\n",
       "   'lambda': 0.3418756169982762,\n",
       "   'max_depth': 60,\n",
       "   'n_estimators': 60,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.7593918474512433,\n",
       "   'gamma': 0.7289319572598256,\n",
       "   'lambda': 3.5050186987255216,\n",
       "   'max_depth': 55,\n",
       "   'n_estimators': 65,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.35922136598262244,\n",
       "   'gamma': 1.130727049475298,\n",
       "   'lambda': 1.39370594917947,\n",
       "   'max_depth': 60,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.48436553689373074,\n",
       "   'gamma': 1.9718194223801544,\n",
       "   'lambda': 2.7286813290723986,\n",
       "   'max_depth': 70,\n",
       "   'n_estimators': 75,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.6000000000000001,\n",
       "   'eta': 0.8773714664715472,\n",
       "   'gamma': 1.0681024777854062,\n",
       "   'lambda': 1.7511464737046722,\n",
       "   'max_depth': 55,\n",
       "   'n_estimators': 25,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.6935800746877178,\n",
       "   'gamma': 2.092353619990147,\n",
       "   'lambda': 4.43089233504884,\n",
       "   'max_depth': 60,\n",
       "   'n_estimators': 20,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.297206371752117,\n",
       "   'gamma': 2.288132008533525,\n",
       "   'lambda': 4.056276028374638,\n",
       "   'max_depth': 75,\n",
       "   'n_estimators': 20,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.49159638932206307,\n",
       "   'gamma': 1.2927164784425018,\n",
       "   'lambda': 0.6619111595655025,\n",
       "   'max_depth': 15,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.8347650409374189,\n",
       "   'gamma': 1.2213750258882996,\n",
       "   'lambda': 0.6007493532607233,\n",
       "   'max_depth': 40,\n",
       "   'n_estimators': 55,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.6596869763549239,\n",
       "   'gamma': 0.36605151648469664,\n",
       "   'lambda': 1.6167779007251248,\n",
       "   'max_depth': 20,\n",
       "   'n_estimators': 55,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.6561835115020502,\n",
       "   'gamma': 2.991724181148207,\n",
       "   'lambda': 3.1008789041123346,\n",
       "   'max_depth': 75,\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.6000000000000001,\n",
       "   'eta': 0.5126658935926601,\n",
       "   'gamma': 1.784703587536029,\n",
       "   'lambda': 0.6841778413750199,\n",
       "   'max_depth': 60,\n",
       "   'n_estimators': 25,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.48397324602325886,\n",
       "   'gamma': 1.4929267760746534,\n",
       "   'lambda': 3.8352752789706743,\n",
       "   'max_depth': 55,\n",
       "   'n_estimators': 10,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.7171432637957667,\n",
       "   'gamma': 2.805661270172382,\n",
       "   'lambda': 1.69062687391837,\n",
       "   'max_depth': 60,\n",
       "   'n_estimators': 75,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.6000000000000001,\n",
       "   'eta': 0.7626237937110016,\n",
       "   'gamma': 2.463229264160505,\n",
       "   'lambda': 4.554378872368142,\n",
       "   'max_depth': 40,\n",
       "   'n_estimators': 30,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.18506136395232203,\n",
       "   'gamma': 3.0082480231754216,\n",
       "   'lambda': 2.7325622670810796,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.8875387926066307,\n",
       "   'gamma': 0.6146487597441355,\n",
       "   'lambda': 0.6606821007425079,\n",
       "   'max_depth': 35,\n",
       "   'n_estimators': 10,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.7000000000000002,\n",
       "   'eta': 0.2460056030672722,\n",
       "   'gamma': 0.12317490115501357,\n",
       "   'lambda': 4.537027865459427,\n",
       "   'max_depth': 25,\n",
       "   'n_estimators': 25,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.6000000000000001,\n",
       "   'eta': 0.35225932629794254,\n",
       "   'gamma': 2.0288099666198587,\n",
       "   'lambda': 3.747180356695325,\n",
       "   'max_depth': 20,\n",
       "   'n_estimators': 25,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.5251494567669934,\n",
       "   'gamma': 1.7783038364786443,\n",
       "   'lambda': 4.160782809461244,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.5825581225819246,\n",
       "   'gamma': 0.905874102773282,\n",
       "   'lambda': 0.7438985192613756,\n",
       "   'max_depth': 45,\n",
       "   'n_estimators': 60,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.6000000000000001,\n",
       "   'eta': 0.6195849086419075,\n",
       "   'gamma': 1.8082969932742488,\n",
       "   'lambda': 0.055903174121065824,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 60,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.6000000000000001,\n",
       "   'eta': 0.1345618311449492,\n",
       "   'gamma': 1.9917784365845834,\n",
       "   'lambda': 3.068625574061487,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 30,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.5349810709944606,\n",
       "   'gamma': 1.277776278314865,\n",
       "   'lambda': 0.6771912977071193,\n",
       "   'max_depth': 20,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.19954995573933731,\n",
       "   'gamma': 0.6684199004203148,\n",
       "   'lambda': 1.594048333376955,\n",
       "   'max_depth': 40,\n",
       "   'n_estimators': 15,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.4497753867979798,\n",
       "   'gamma': 1.035772122171717,\n",
       "   'lambda': 4.768328142575007,\n",
       "   'max_depth': 15,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.6000000000000001,\n",
       "   'eta': 0.8977636841483815,\n",
       "   'gamma': 0.9666468197598561,\n",
       "   'lambda': 3.3969081703049584,\n",
       "   'max_depth': 75,\n",
       "   'n_estimators': 55,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.6555368195322938,\n",
       "   'gamma': 2.474957676682568,\n",
       "   'lambda': 4.404147720751101,\n",
       "   'max_depth': 60,\n",
       "   'n_estimators': 45,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.5695402475869887,\n",
       "   'gamma': 1.0704659739453966,\n",
       "   'lambda': 1.455537513991404,\n",
       "   'max_depth': 30,\n",
       "   'n_estimators': 45,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.7465153152250606,\n",
       "   'gamma': 2.8669364282020693,\n",
       "   'lambda': 3.0830092246063696,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 45,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.5196464143089287,\n",
       "   'gamma': 0.8595759862429773,\n",
       "   'lambda': 1.265026686360677,\n",
       "   'max_depth': 55,\n",
       "   'n_estimators': 45,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.24911321753068405,\n",
       "   'gamma': 0.22966515849809793,\n",
       "   'lambda': 4.072942079203426,\n",
       "   'max_depth': 25,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.7000000000000002,\n",
       "   'eta': 0.1829811537907224,\n",
       "   'gamma': 0.4806260160653942,\n",
       "   'lambda': 2.679466570884344,\n",
       "   'max_depth': 25,\n",
       "   'n_estimators': 40,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.2347231831174459,\n",
       "   'gamma': 1.4384471725693377,\n",
       "   'lambda': 2.3971701823965486,\n",
       "   'max_depth': 60,\n",
       "   'n_estimators': 60,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.6289479440984249,\n",
       "   'gamma': 1.8503416165173225,\n",
       "   'lambda': 3.9617962599835836,\n",
       "   'max_depth': 45,\n",
       "   'n_estimators': 10,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.12179847593127091,\n",
       "   'gamma': 3.045180523047898,\n",
       "   'lambda': 2.8419851471659445,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 20,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.5776120947499208,\n",
       "   'gamma': 2.000050633822617,\n",
       "   'lambda': 4.711872575890077,\n",
       "   'max_depth': 30,\n",
       "   'n_estimators': 30,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.12466379891729212,\n",
       "   'gamma': 2.176864565876066,\n",
       "   'lambda': 1.9763854844112179,\n",
       "   'max_depth': 55,\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.8969931604665183,\n",
       "   'gamma': 2.9158514438724725,\n",
       "   'lambda': 1.755562180950891,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.6409300603771406,\n",
       "   'gamma': 1.2162280984985243,\n",
       "   'lambda': 1.456402898887284,\n",
       "   'max_depth': 40,\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.6000000000000001,\n",
       "   'eta': 0.5087288121285809,\n",
       "   'gamma': 1.043833846873524,\n",
       "   'lambda': 1.6399904131425425,\n",
       "   'max_depth': 40,\n",
       "   'n_estimators': 25,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.6173885228811786,\n",
       "   'gamma': 0.3158292731138273,\n",
       "   'lambda': 3.4691882487552057,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.7000000000000002,\n",
       "   'eta': 0.7420427658162391,\n",
       "   'gamma': 1.0114110701156418,\n",
       "   'lambda': 3.0888532768881922,\n",
       "   'max_depth': 65,\n",
       "   'n_estimators': 45,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.42604358436733536,\n",
       "   'gamma': 0.9720366727556207,\n",
       "   'lambda': 1.5649997552701973,\n",
       "   'max_depth': 75,\n",
       "   'n_estimators': 75,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.7000000000000002,\n",
       "   'eta': 0.2313700235809062,\n",
       "   'gamma': 1.7665746009914305,\n",
       "   'lambda': 1.7231024683328506,\n",
       "   'max_depth': 70,\n",
       "   'n_estimators': 35,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.46372731361529296,\n",
       "   'gamma': 1.8711189082159916,\n",
       "   'lambda': 4.2554610948202445,\n",
       "   'max_depth': 50,\n",
       "   'n_estimators': 20,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.3339003998832132,\n",
       "   'gamma': 2.2518184706899644,\n",
       "   'lambda': 3.936641060200441,\n",
       "   'max_depth': 30,\n",
       "   'n_estimators': 10,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.8602070724416611,\n",
       "   'gamma': 1.262245853947683,\n",
       "   'lambda': 0.9002191259793435,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 75,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.7805928744310909,\n",
       "   'gamma': 0.5784424233951909,\n",
       "   'lambda': 0.38937803474464183,\n",
       "   'max_depth': 35,\n",
       "   'n_estimators': 30,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.3558640181508287,\n",
       "   'gamma': 0.4104390599431535,\n",
       "   'lambda': 1.6025169578919636,\n",
       "   'max_depth': 40,\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.6308505683548842,\n",
       "   'gamma': 1.4611800383399174,\n",
       "   'lambda': 1.8961959851059977,\n",
       "   'max_depth': 15,\n",
       "   'n_estimators': 75,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.6000000000000001,\n",
       "   'eta': 0.23785776469851028,\n",
       "   'gamma': 2.2348603328328145,\n",
       "   'lambda': 0.1374006940258382,\n",
       "   'max_depth': 55,\n",
       "   'n_estimators': 35,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.16236441635578638,\n",
       "   'gamma': 1.7297116796570886,\n",
       "   'lambda': 2.4376487805595692,\n",
       "   'max_depth': 25,\n",
       "   'n_estimators': 55,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.2641431341276761,\n",
       "   'gamma': 0.2409980233802957,\n",
       "   'lambda': 3.6702389216808013,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 45,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.10714123484079466,\n",
       "   'gamma': 1.1606262727666532,\n",
       "   'lambda': 4.268439845884284,\n",
       "   'max_depth': 40,\n",
       "   'n_estimators': 35,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.14525677783220445,\n",
       "   'gamma': 2.1116632423960056,\n",
       "   'lambda': 3.1419477058582475,\n",
       "   'max_depth': 25,\n",
       "   'n_estimators': 65,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.32163278326845,\n",
       "   'gamma': 1.3750503365931606,\n",
       "   'lambda': 0.9578911163170839,\n",
       "   'max_depth': 35,\n",
       "   'n_estimators': 25,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.539008223547531,\n",
       "   'gamma': 2.0165829575463063,\n",
       "   'lambda': 4.028998494504491,\n",
       "   'max_depth': 65,\n",
       "   'n_estimators': 55,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.7619688775811085,\n",
       "   'gamma': 0.7755582264323241,\n",
       "   'lambda': 4.262466587195901,\n",
       "   'max_depth': 60,\n",
       "   'n_estimators': 40,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.8723117050927903,\n",
       "   'gamma': 2.8517209589748655,\n",
       "   'lambda': 2.759875750641168,\n",
       "   'max_depth': 30,\n",
       "   'n_estimators': 15,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.2943829543057964,\n",
       "   'gamma': 0.06693886153853297,\n",
       "   'lambda': 1.2928519338882016,\n",
       "   'max_depth': 25,\n",
       "   'n_estimators': 60,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.18124967647759985,\n",
       "   'gamma': 2.117276358788009,\n",
       "   'lambda': 2.0282101827674435,\n",
       "   'max_depth': 15,\n",
       "   'n_estimators': 45,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.17401637274515452,\n",
       "   'gamma': 2.128923759336442,\n",
       "   'lambda': 3.3664499130580054,\n",
       "   'max_depth': 55,\n",
       "   'n_estimators': 45,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.5730448079720071,\n",
       "   'gamma': 1.2484597289563373,\n",
       "   'lambda': 0.6655067084801741,\n",
       "   'max_depth': 70,\n",
       "   'n_estimators': 30,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.5791597828532704,\n",
       "   'gamma': 0.5958859060914213,\n",
       "   'lambda': 3.6069866321960005,\n",
       "   'max_depth': 55,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.4013392938836621,\n",
       "   'gamma': 2.9712898115742687,\n",
       "   'lambda': 2.9250017672187067,\n",
       "   'max_depth': 25,\n",
       "   'n_estimators': 35,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.30813876356479175,\n",
       "   'gamma': 0.6549841435853611,\n",
       "   'lambda': 4.701951191022603,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.3012823938546708,\n",
       "   'gamma': 1.5139681955794309,\n",
       "   'lambda': 0.38015971849683805,\n",
       "   'max_depth': 45,\n",
       "   'n_estimators': 65,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.6000000000000001,\n",
       "   'eta': 0.7922092018582602,\n",
       "   'gamma': 2.771614591584603,\n",
       "   'lambda': 1.3174833674962083,\n",
       "   'max_depth': 30,\n",
       "   'n_estimators': 40,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.5005759121210643,\n",
       "   'gamma': 0.9237152806387781,\n",
       "   'lambda': 3.850556895995367,\n",
       "   'max_depth': 60,\n",
       "   'n_estimators': 65,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.2919513146437066,\n",
       "   'gamma': 1.4424146950469598,\n",
       "   'lambda': 4.897893148174609,\n",
       "   'max_depth': 60,\n",
       "   'n_estimators': 15,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.11875846703030267,\n",
       "   'gamma': 1.4336053260716228,\n",
       "   'lambda': 4.3772465034815164,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 15,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.8058057473850194,\n",
       "   'gamma': 0.9795784866007762,\n",
       "   'lambda': 0.6825244891981541,\n",
       "   'max_depth': 25,\n",
       "   'n_estimators': 35,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.7191376137838347,\n",
       "   'gamma': 1.7488678077420388,\n",
       "   'lambda': 3.3630810457978018,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 65,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.6419850126794466,\n",
       "   'gamma': 1.4860327336994508,\n",
       "   'lambda': 3.8632084878445068,\n",
       "   'max_depth': 45,\n",
       "   'n_estimators': 30,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.6000000000000001,\n",
       "   'eta': 0.3629866726562734,\n",
       "   'gamma': 1.9339738967389397,\n",
       "   'lambda': 0.239200510836185,\n",
       "   'max_depth': 65,\n",
       "   'n_estimators': 25,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.7861617684008553,\n",
       "   'gamma': 0.4878460371187479,\n",
       "   'lambda': 4.207106185634178,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.6000000000000001,\n",
       "   'eta': 0.6401962486991339,\n",
       "   'gamma': 1.627479653771257,\n",
       "   'lambda': 3.370440432137127,\n",
       "   'max_depth': 75,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.12379322210359787,\n",
       "   'gamma': 0.32638080732210556,\n",
       "   'lambda': 3.0967933303221136,\n",
       "   'max_depth': 30,\n",
       "   'n_estimators': 35,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.893814371959418,\n",
       "   'gamma': 0.6814991209440597,\n",
       "   'lambda': 1.3996065941322777,\n",
       "   'max_depth': 70,\n",
       "   'n_estimators': 30,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.8669729536801625,\n",
       "   'gamma': 2.9323151413397577,\n",
       "   'lambda': 0.012988940043445196,\n",
       "   'max_depth': 25,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.42668191396485766,\n",
       "   'gamma': 1.9834706390233567,\n",
       "   'lambda': 3.145387002834777,\n",
       "   'max_depth': 65,\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.32871738897837566,\n",
       "   'gamma': 1.4369265248248417,\n",
       "   'lambda': 4.892850372087451,\n",
       "   'max_depth': 55,\n",
       "   'n_estimators': 45,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.3306114254655399,\n",
       "   'gamma': 1.5790422355500324,\n",
       "   'lambda': 3.1363258909008,\n",
       "   'max_depth': 75,\n",
       "   'n_estimators': 75,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.7706786373098249,\n",
       "   'gamma': 2.1213969785334412,\n",
       "   'lambda': 0.44558177684010547,\n",
       "   'max_depth': 40,\n",
       "   'n_estimators': 60,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.5325953139006316,\n",
       "   'gamma': 2.537341321523175,\n",
       "   'lambda': 1.655742604151853,\n",
       "   'max_depth': 45,\n",
       "   'n_estimators': 75,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.4355819080752723,\n",
       "   'gamma': 0.9882153575401229,\n",
       "   'lambda': 1.6919194963683877,\n",
       "   'max_depth': 65,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.17599596440702606,\n",
       "   'gamma': 2.2643076726435654,\n",
       "   'lambda': 1.8337434784457591,\n",
       "   'max_depth': 40,\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.686379043657186,\n",
       "   'gamma': 2.5113035633130343,\n",
       "   'lambda': 2.40119464897034,\n",
       "   'max_depth': 55,\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.4695751857730789,\n",
       "   'gamma': 1.7354942620085214,\n",
       "   'lambda': 1.7573445991222596,\n",
       "   'max_depth': 35,\n",
       "   'n_estimators': 45,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.6000000000000001,\n",
       "   'eta': 0.5208098789503465,\n",
       "   'gamma': 2.1883484668673328,\n",
       "   'lambda': 1.8944412907317265,\n",
       "   'max_depth': 45,\n",
       "   'n_estimators': 65,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.20640574958071564,\n",
       "   'gamma': 0.7743152843630987,\n",
       "   'lambda': 1.0080494256477646,\n",
       "   'max_depth': 30,\n",
       "   'n_estimators': 10,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.14438403503050756,\n",
       "   'gamma': 1.2970107747679573,\n",
       "   'lambda': 3.981089834501998,\n",
       "   'max_depth': 65,\n",
       "   'n_estimators': 75,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.5629774694430487,\n",
       "   'gamma': 0.6777202402126364,\n",
       "   'lambda': 1.33392088032657,\n",
       "   'max_depth': 60,\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.5347024620809224,\n",
       "   'gamma': 0.9821573725778135,\n",
       "   'lambda': 3.0748823800020912,\n",
       "   'max_depth': 60,\n",
       "   'n_estimators': 15,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.5361642929480243,\n",
       "   'gamma': 0.6664817538519097,\n",
       "   'lambda': 1.748808121205665,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.28029467729667235,\n",
       "   'gamma': 2.212010952882472,\n",
       "   'lambda': 3.083619070477842,\n",
       "   'max_depth': 20,\n",
       "   'n_estimators': 60,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.8843962011388784,\n",
       "   'gamma': 1.5339812523975869,\n",
       "   'lambda': 0.3226918907395532,\n",
       "   'max_depth': 25,\n",
       "   'n_estimators': 65,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.6006465457235308,\n",
       "   'gamma': 2.878446357829725,\n",
       "   'lambda': 3.7538500690526835,\n",
       "   'max_depth': 15,\n",
       "   'n_estimators': 35,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.39955504534515585,\n",
       "   'gamma': 1.8556636028002402,\n",
       "   'lambda': 2.254181107333644,\n",
       "   'max_depth': 25,\n",
       "   'n_estimators': 45,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.1328167170399338,\n",
       "   'gamma': 0.2473959276982574,\n",
       "   'lambda': 4.897873042501125,\n",
       "   'max_depth': 65,\n",
       "   'n_estimators': 55,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.6000000000000001,\n",
       "   'eta': 0.6834569809239898,\n",
       "   'gamma': 2.19716426542252,\n",
       "   'lambda': 3.9261730155645482,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.5499518522639881,\n",
       "   'gamma': 0.8737171108419276,\n",
       "   'lambda': 0.7820724970353765,\n",
       "   'max_depth': 70,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.509520605767073,\n",
       "   'gamma': 2.1774419833800107,\n",
       "   'lambda': 0.8860861839971645,\n",
       "   'max_depth': 55,\n",
       "   'n_estimators': 45,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.1949076433588638,\n",
       "   'gamma': 1.9744452707014941,\n",
       "   'lambda': 4.630472702658747,\n",
       "   'max_depth': 35,\n",
       "   'n_estimators': 15,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.28518890370005023,\n",
       "   'gamma': 2.434369125981914,\n",
       "   'lambda': 4.94147197729797,\n",
       "   'max_depth': 60,\n",
       "   'n_estimators': 65,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.7000000000000002,\n",
       "   'eta': 0.5366470545074065,\n",
       "   'gamma': 1.7983582863893661,\n",
       "   'lambda': 4.508022153679546,\n",
       "   'max_depth': 45,\n",
       "   'n_estimators': 75,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.7836120856451155,\n",
       "   'gamma': 1.2753027328013515,\n",
       "   'lambda': 1.265109211264115,\n",
       "   'max_depth': 75,\n",
       "   'n_estimators': 55,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.6548588052969151,\n",
       "   'gamma': 2.618679743106584,\n",
       "   'lambda': 2.932038291519822,\n",
       "   'max_depth': 35,\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.42199835688680354,\n",
       "   'gamma': 2.729239823528659,\n",
       "   'lambda': 0.9177905149506904,\n",
       "   'max_depth': 20,\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.8614431266165407,\n",
       "   'gamma': 0.6997424399500813,\n",
       "   'lambda': 4.918994229193132,\n",
       "   'max_depth': 50,\n",
       "   'n_estimators': 55,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.7000000000000002,\n",
       "   'eta': 0.7285283839154004,\n",
       "   'gamma': 2.310818101584896,\n",
       "   'lambda': 1.5759611474339357,\n",
       "   'max_depth': 25,\n",
       "   'n_estimators': 45,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.8037860298093317,\n",
       "   'gamma': 1.6096793371571254,\n",
       "   'lambda': 1.8452040824446359,\n",
       "   'max_depth': 25,\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.8317363821703684,\n",
       "   'gamma': 2.2707818067860983,\n",
       "   'lambda': 3.0026613061693586,\n",
       "   'max_depth': 70,\n",
       "   'n_estimators': 30,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.26323087777301507,\n",
       "   'gamma': 2.1550395985772433,\n",
       "   'lambda': 2.599997596900386,\n",
       "   'max_depth': 50,\n",
       "   'n_estimators': 55,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.8010657082058616,\n",
       "   'gamma': 0.45005650400342073,\n",
       "   'lambda': 1.0510077092813552,\n",
       "   'max_depth': 15,\n",
       "   'n_estimators': 20,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.5928993713413531,\n",
       "   'gamma': 1.8685451406680402,\n",
       "   'lambda': 2.6679370730186296,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 10,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.6000000000000001,\n",
       "   'eta': 0.20017928669314344,\n",
       "   'gamma': 2.7739948451089416,\n",
       "   'lambda': 2.930045986228112,\n",
       "   'max_depth': 40,\n",
       "   'n_estimators': 75,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.7000000000000002,\n",
       "   'eta': 0.45583471896599315,\n",
       "   'gamma': 1.0849793257877045,\n",
       "   'lambda': 0.3659343161681372,\n",
       "   'max_depth': 45,\n",
       "   'n_estimators': 55,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.5247178707160589,\n",
       "   'gamma': 1.508103733922198,\n",
       "   'lambda': 4.745954849994289,\n",
       "   'max_depth': 40,\n",
       "   'n_estimators': 20,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.4666718692981936,\n",
       "   'gamma': 2.969327794368384,\n",
       "   'lambda': 2.455046710827356,\n",
       "   'max_depth': 40,\n",
       "   'n_estimators': 25,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.3444983581844995,\n",
       "   'gamma': 1.6543991718242426,\n",
       "   'lambda': 3.8715136941664112,\n",
       "   'max_depth': 65,\n",
       "   'n_estimators': 55,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.3658640494887091,\n",
       "   'gamma': 2.7785352532628127,\n",
       "   'lambda': 0.580015750198265,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 25,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.7000000000000002,\n",
       "   'eta': 0.5595108042783247,\n",
       "   'gamma': 2.1915817778523965,\n",
       "   'lambda': 4.994965553879423,\n",
       "   'max_depth': 70,\n",
       "   'n_estimators': 20,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.6000000000000001,\n",
       "   'eta': 0.6743552811134613,\n",
       "   'gamma': 0.7867728939904219,\n",
       "   'lambda': 3.028152824051346,\n",
       "   'max_depth': 25,\n",
       "   'n_estimators': 45,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.6000000000000001,\n",
       "   'eta': 0.3242471013175867,\n",
       "   'gamma': 2.285702940311724,\n",
       "   'lambda': 3.112500394397114,\n",
       "   'max_depth': 40,\n",
       "   'n_estimators': 20,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.35334475744484695,\n",
       "   'gamma': 2.1082012579503644,\n",
       "   'lambda': 2.3134651379976825,\n",
       "   'max_depth': 25,\n",
       "   'n_estimators': 20,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.8138034559320617,\n",
       "   'gamma': 0.8645318312220551,\n",
       "   'lambda': 4.085448794309105,\n",
       "   'max_depth': 60,\n",
       "   'n_estimators': 25,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.7981534355678294,\n",
       "   'gamma': 0.9298601641305769,\n",
       "   'lambda': 3.101237057769381,\n",
       "   'max_depth': 15,\n",
       "   'n_estimators': 75,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.1127238750309898,\n",
       "   'gamma': 0.6785468083649832,\n",
       "   'lambda': 4.394178844502867,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 30,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.7356568477145906,\n",
       "   'gamma': 2.319165361847813,\n",
       "   'lambda': 2.189273362667112,\n",
       "   'max_depth': 50,\n",
       "   'n_estimators': 75,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.7995424896398632,\n",
       "   'gamma': 0.9980426517880293,\n",
       "   'lambda': 2.1531735034840898,\n",
       "   'max_depth': 45,\n",
       "   'n_estimators': 55,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.612552875228495,\n",
       "   'gamma': 0.9189697689062,\n",
       "   'lambda': 0.1329067162079295,\n",
       "   'max_depth': 30,\n",
       "   'n_estimators': 60,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.13541785200282996,\n",
       "   'gamma': 2.0590631240355326,\n",
       "   'lambda': 3.579025487546201,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 55,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.10310612065324216,\n",
       "   'gamma': 1.4716898017949502,\n",
       "   'lambda': 0.20784224477608082,\n",
       "   'max_depth': 20,\n",
       "   'n_estimators': 35,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.7536630359414763,\n",
       "   'gamma': 1.5762144679361965,\n",
       "   'lambda': 3.355864545489131,\n",
       "   'max_depth': 45,\n",
       "   'n_estimators': 45,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.5742277568118551,\n",
       "   'gamma': 2.345537209956281,\n",
       "   'lambda': 2.7188733580338424,\n",
       "   'max_depth': 65,\n",
       "   'n_estimators': 35,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.8501530056531486,\n",
       "   'gamma': 2.092563530898945,\n",
       "   'lambda': 1.3248835701937822,\n",
       "   'max_depth': 55,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.1429229399429618,\n",
       "   'gamma': 0.5936577851355563,\n",
       "   'lambda': 0.8370444121994136,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 75,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.5621644830248167,\n",
       "   'gamma': 1.2339861918588257,\n",
       "   'lambda': 2.3368295583739234,\n",
       "   'max_depth': 30,\n",
       "   'n_estimators': 25,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.12204668216784925,\n",
       "   'gamma': 0.1775430604376615,\n",
       "   'lambda': 0.11511617882112801,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 65,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.7000000000000002,\n",
       "   'eta': 0.5236372085944567,\n",
       "   'gamma': 2.7907064173354432,\n",
       "   'lambda': 2.2071043065168485,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 25,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.5748183706776107,\n",
       "   'gamma': 1.8654779255392053,\n",
       "   'lambda': 1.983805170924791,\n",
       "   'max_depth': 25,\n",
       "   'n_estimators': 10,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.4459536541553898,\n",
       "   'gamma': 2.252623671765733,\n",
       "   'lambda': 3.089233858249303,\n",
       "   'max_depth': 25,\n",
       "   'n_estimators': 25,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.9000000000000001,\n",
       "   'eta': 0.6120366920597556,\n",
       "   'gamma': 1.435880590943367,\n",
       "   'lambda': 3.221784174455826,\n",
       "   'max_depth': 25,\n",
       "   'n_estimators': 60,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.6000000000000001,\n",
       "   'eta': 0.7197988124405972,\n",
       "   'gamma': 0.8945763679785066,\n",
       "   'lambda': 1.4691867696555354,\n",
       "   'max_depth': 50,\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.7999999999999999},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.350081356500215,\n",
       "   'gamma': 0.2859723334372169,\n",
       "   'lambda': 2.6762396401869504,\n",
       "   'max_depth': 75,\n",
       "   'n_estimators': 40,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.30000000000000004,\n",
       "   'eta': 0.2783645512781543,\n",
       "   'gamma': 0.834545723127197,\n",
       "   'lambda': 1.90536067288771,\n",
       "   'max_depth': 60,\n",
       "   'n_estimators': 35,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.6369151235928865,\n",
       "   'gamma': 2.9860483560576796,\n",
       "   'lambda': 0.10947961597440325,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 60,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.862346105227504,\n",
       "   'gamma': 2.878728950115069,\n",
       "   'lambda': 2.5876006187981386,\n",
       "   'max_depth': 45,\n",
       "   'n_estimators': 75,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'eta': 0.5613432007350374,\n",
       "   'gamma': 3.018377882660834,\n",
       "   'lambda': 4.145566305769694,\n",
       "   'max_depth': 25,\n",
       "   'n_estimators': 40,\n",
       "   'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.8018352339293168,\n",
       "   'gamma': 1.9805341367089546,\n",
       "   'lambda': 2.7678271811861848,\n",
       "   'max_depth': 70,\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.6625812451403134,\n",
       "   'gamma': 1.7686829888494024,\n",
       "   'lambda': 2.862134916527718,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 30,\n",
       "   'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.8000000000000003,\n",
       "   'eta': 0.2722883132903846,\n",
       "   'gamma': 0.4570044669244591,\n",
       "   'lambda': 4.027662391364425,\n",
       "   'max_depth': 70,\n",
       "   'n_estimators': 25,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.4000000000000001,\n",
       "   'eta': 0.11613198959988989,\n",
       "   'gamma': 1.167597473001239,\n",
       "   'lambda': 0.3068094700104479,\n",
       "   'max_depth': 35,\n",
       "   'n_estimators': 10,\n",
       "   'subsample': 0.8999999999999999},\n",
       "  {'colsample_bytree': 0.5000000000000001,\n",
       "   'eta': 0.24016792097525821,\n",
       "   'gamma': 0.13960944516616763,\n",
       "   'lambda': 0.4265135792864627,\n",
       "   'max_depth': 75,\n",
       "   'n_estimators': 70,\n",
       "   'subsample': 0.7999999999999999}],\n",
       " 'split0_test_score': array([-60691.812, -63911.512, -64268.457, -65072.582, -59824.997,\n",
       "        -60907.085, -62143.513, -66779.401, -63197.817, -62400.951,\n",
       "        -63433.635, -64135.427, -62160.63 , -71325.427, -75697.246,\n",
       "        -66849.862, -63863.172, -66998.69 , -62112.28 , -67643.842,\n",
       "        -63133.916, -66489.142, -65002.967, -64917.987, -62474.052,\n",
       "        -61097.625, -64497.847, -62442.531, -62382.842, -61681.277,\n",
       "        -65745.253, -60842.266, -62799.421, -61742.657, -68695.683,\n",
       "        -59347.677, -66652.525, -74379.231, -64558.331, -65180.396,\n",
       "        -64227.415, -63423.236, -63148.802, -65311.142, -72599.106,\n",
       "        -70094.876, -63609.356, -63275.477, -74747.467, -64544.45 ,\n",
       "        -64988.841, -65978.397, -64265.991, -62590.456, -62138.021,\n",
       "        -66175.82 , -64467.7  , -60262.641, -62873.726, -65986.056,\n",
       "        -64769.801, -63099.977, -67392.596, -61088.262, -62944.566,\n",
       "        -61782.841, -69891.062, -63484.261, -62085.507, -63730.208,\n",
       "        -62244.997, -65872.987, -62094.521, -62075.237, -64215.976,\n",
       "        -64827.5  , -67434.226, -64615.427, -60873.948, -64905.808,\n",
       "        -63703.3  , -65352.572, -60380.355, -61214.443, -62840.372,\n",
       "        -72813.083, -75965.605, -64658.862, -68900.032, -64326.863,\n",
       "        -63027.377, -62455.318, -61615.167, -62843.272, -61685.002,\n",
       "        -65771.036, -73791.825, -63987.877, -66347.366, -61419.251,\n",
       "        -65148.083, -61313.232, -68267.212, -62188.472, -65615.072,\n",
       "        -61979.911, -63387.471, -60272.282, -61899.392, -62883.941,\n",
       "        -61432.936, -66155.736, -70903.151, -64821.607, -63827.312,\n",
       "        -63787.063, -63995.561, -64109.702, -61446.851, -61787.105,\n",
       "        -63197.911, -61382.006, -62283.366, -65430.671, -70113.775,\n",
       "        -63927.622, -66362.262, -62544.06 , -62704.265, -61080.357,\n",
       "        -62875.933, -61783.557, -72695.577, -63613.395, -62243.637,\n",
       "        -62379.586, -65997.146, -60902.192, -65198.421, -63349.111,\n",
       "        -62201.455, -65418.101, -67188.522, -62177.795, -60126.446,\n",
       "        -60872.706, -61833.766, -63163.362, -61783.322, -64906.31 ,\n",
       "        -64568.   , -61036.226, -60852.257, -63796.866, -61463.771,\n",
       "        -60317.786, -66040.112, -61860.496, -62301.882, -76588.13 ,\n",
       "        -62449.391, -64402.992, -65172.261, -73744.405, -61809.161,\n",
       "        -68998.213, -82279.29 , -61098.717, -66417.875, -63738.58 ,\n",
       "        -62819.666, -63469.826, -63371.432, -70350.55 , -62311.216,\n",
       "        -62568.507, -65235.091, -62990.487, -65250.547, -84893.19 ,\n",
       "        -64979.437, -68634.542, -71915.295, -61606.712, -60781.926,\n",
       "        -69565.221, -63364.132, -64107.572, -63833.496, -63294.352,\n",
       "        -74995.561, -64715.623, -62386.657, -63262.015, -61674.495,\n",
       "        -65255.407, -64381.546, -76168.49 , -69898.817, -70076.061,\n",
       "        -63257.696, -63876.797, -61379.605, -66498.111, -63574.847,\n",
       "        -67399.131, -69589.931, -63146.067, -61368.521, -64366.932,\n",
       "        -72300.081, -62381.307, -61197.826, -64922.496, -66906.948,\n",
       "        -80296.875, -62780.816, -63439.937, -62025.006, -63168.992,\n",
       "        -60700.322, -63960.867, -66625.931, -62852.768, -62876.136,\n",
       "        -61367.896, -64066.651, -62657.141, -60337.047, -61557.131,\n",
       "        -61537.471, -65987.917, -63495.887, -63056.407, -61669.447,\n",
       "        -69500.355, -64870.262, -62651.176, -63535.585, -67848.972,\n",
       "        -62762.966, -60181.706, -67180.51 , -61097.116, -65154.286,\n",
       "        -63980.846, -61726.571, -60885.336, -74353.065, -64552.496,\n",
       "        -61402.147, -62547.372, -59953.615, -61288.076, -62272.497,\n",
       "        -59969.391, -61908.017, -62641.591, -64683.091, -62085.326,\n",
       "        -79542.978, -64749.67 , -61692.161, -65521.342, -63651.772,\n",
       "        -64632.382, -66294.891, -62685.866, -61320.742, -63869.435,\n",
       "        -64022.736, -62256.432, -63631.466, -74737.2  , -61533.141,\n",
       "        -61338.106, -60524.861, -72381.585, -65627.496, -61959.541,\n",
       "        -62418.088, -59688.071, -77000.461, -61579.486, -71248.097,\n",
       "        -72226.136, -64401.391, -62844.066, -62591.156, -61975.397,\n",
       "        -63382.337, -62997.752, -68902.587, -61063.286, -65756.597,\n",
       "        -62017.717, -63697.007, -65111.337, -67078.467, -63657.566]),\n",
       " 'split1_test_score': array([-64071.586, -64422.642, -66477.002, -68779.087, -63739.726,\n",
       "        -64888.142, -63426.886, -68531.802, -66246.607, -64645.506,\n",
       "        -65255.612, -68200.047, -62175.542, -72421.335, -76801.537,\n",
       "        -65666.5  , -65308.756, -67498.341, -63177.981, -68459.661,\n",
       "        -65638.892, -68446.566, -65920.441, -64262.771, -64328.346,\n",
       "        -64837.251, -67178.201, -64717.002, -62744.91 , -64182.546,\n",
       "        -66813.302, -64591.257, -63271.75 , -63729.352, -69154.7  ,\n",
       "        -61636.497, -66419.151, -75839.912, -65532.362, -67298.405,\n",
       "        -67048.661, -64192.181, -66815.326, -66594.452, -75678.231,\n",
       "        -71957.362, -62813.18 , -65385.345, -76848.63 , -64707.151,\n",
       "        -66076.526, -66585.472, -65632.996, -66040.162, -66469.74 ,\n",
       "        -64167.016, -67703.735, -64766.457, -66759.771, -69384.547,\n",
       "        -67189.482, -65689.882, -72028.651, -64533.301, -64997.427,\n",
       "        -63565.617, -68378.771, -65398.332, -64380.561, -66587.102,\n",
       "        -64828.761, -68120.907, -65141.781, -64688.492, -64548.787,\n",
       "        -67393.96 , -69837.536, -65085.577, -65225.087, -69065.832,\n",
       "        -63834.591, -68416.077, -64323.976, -64126.34 , -65066.382,\n",
       "        -75048.575, -77203.83 , -62156.511, -67217.077, -65225.597,\n",
       "        -64577.611, -63892.736, -65359.907, -63947.136, -64692.027,\n",
       "        -68557.811, -74586.371, -65391.387, -69797.321, -63363.796,\n",
       "        -64543.422, -63971.327, -70986.897, -62605.661, -68446.306,\n",
       "        -64146.472, -65918.091, -63512.621, -66222.016, -66087.345,\n",
       "        -65802.177, -68406.716, -72459.051, -66730.906, -65373.531,\n",
       "        -65829.907, -65953.122, -68122.152, -65054.711, -65820.761,\n",
       "        -66292.876, -64556.552, -65357.971, -67662.167, -72363.741,\n",
       "        -64382.666, -68772.757, -65153.516, -67110.706, -61639.887,\n",
       "        -67323.381, -66938.347, -73264.866, -61125.146, -61605.322,\n",
       "        -63776.121, -66910.276, -65422.486, -68547.786, -64056.482,\n",
       "        -63874.456, -66505.176, -69318.396, -63992.891, -63582.812,\n",
       "        -63443.991, -61946.127, -63364.807, -64790.226, -64686.356,\n",
       "        -67019.597, -65343.546, -64645.087, -64324.882, -63587.27 ,\n",
       "        -63121.962, -67367.281, -60884.516, -62689.821, -79724.146,\n",
       "        -65640.691, -67601.827, -68854.676, -74026.125, -64653.762,\n",
       "        -70952.482, -85193.54 , -63552.951, -70584.691, -63427.446,\n",
       "        -65385.497, -63883.946, -64374.491, -71122.056, -64813.401,\n",
       "        -66494.337, -68313.017, -67372.597, -67133.66 , -87883.19 ,\n",
       "        -66569.051, -68390.821, -74547.135, -63600.501, -66583.266,\n",
       "        -67997.982, -62191.492, -63512.082, -64972.666, -65473.106,\n",
       "        -74017.451, -66104.927, -63328.502, -62677.151, -61244.816,\n",
       "        -65897.501, -67851.747, -77675.251, -71674.372, -69883.486,\n",
       "        -66558.672, -63652.89 , -63361.912, -65530.766, -67360.122,\n",
       "        -68784.47 , -69357.087, -66185.516, -64199.496, -66238.252,\n",
       "        -73093.012, -65010.991, -64444.471, -64423.647, -70662.61 ,\n",
       "        -83237.265, -64569.086, -64355.601, -63148.126, -66748.607,\n",
       "        -64051.556, -63321.34 , -71218.297, -64808.3  , -61884.686,\n",
       "        -64537.407, -66018.556, -66081.842, -65301.857, -61197.187,\n",
       "        -63544.106, -66449.666, -64062.686, -62418.59 , -64328.436,\n",
       "        -71286.422, -65885.432, -62905.896, -64751.376, -71489.677,\n",
       "        -67237.642, -64202.29 , -64550.036, -64532.651, -69602.307,\n",
       "        -66353.302, -61140.551, -64872.817, -76782.726, -67064.281,\n",
       "        -61044.987, -65218.452, -63602.715, -63576.291, -64752.571,\n",
       "        -64095.202, -64059.826, -62353.481, -66258.851, -65161.337,\n",
       "        -81228.901, -68553.462, -62607.142, -67474.511, -66448.591,\n",
       "        -66495.287, -69042.2  , -65626.957, -62801.541, -67609.825,\n",
       "        -67477.602, -64315.757, -65875.266, -76469.247, -61579.426,\n",
       "        -61522.046, -64779.436, -75164.487, -68597.856, -63652.456,\n",
       "        -63264.242, -65513.101, -78945.781, -63534.995, -71905.356,\n",
       "        -76517.83 , -67168.28 , -65090.191, -66345.731, -64747.107,\n",
       "        -65469.166, -65309.236, -67251.382, -63863.301, -65338.267,\n",
       "        -63754.336, -63607.366, -67241.652, -68928.326, -65633.801]),\n",
       " 'split2_test_score': array([-62551.09 , -66189.01 , -68019.835, -67573.075, -63369.87 ,\n",
       "        -63277.185, -62909.36 , -68415.175, -64266.64 , -64600.475,\n",
       "        -65490.41 , -67943.835, -62053.585, -69750.39 , -75716.54 ,\n",
       "        -65830.615, -65933.835, -64779.1  , -65280.77 , -68079.34 ,\n",
       "        -66479.875, -67684.77 , -67289.7  , -65702.68 , -66072.145,\n",
       "        -59736.08 , -67024.945, -64906.56 , -63294.165, -62877.02 ,\n",
       "        -66465.295, -67020.505, -63201.18 , -63333.2  , -69746.98 ,\n",
       "        -63358.76 , -64892.22 , -77028.06 , -65451.86 , -65044.855,\n",
       "        -66170.61 , -62697.07 , -65239.245, -68777.175, -75499.81 ,\n",
       "        -73236.785, -62388.605, -66356.96 , -76709.25 , -66899.805,\n",
       "        -67740.475, -66883.02 , -65096.465, -65418.49 , -64540.945,\n",
       "        -64781.375, -67627.975, -63311.47 , -63578.115, -69839.09 ,\n",
       "        -67404.955, -65933.945, -70917.09 , -62646.13 , -63002.055,\n",
       "        -65061.885, -71284.885, -62847.2  , -64925.235, -63470.745,\n",
       "        -62569.85 , -68361.955, -64742.635, -64661.275, -65652.175,\n",
       "        -66440.755, -68548.2  , -64727.895, -65585.665, -69069.63 ,\n",
       "        -63841.535, -68893.23 , -63827.455, -63893.16 , -64550.175,\n",
       "        -74508.06 , -75857.155, -63599.645, -71043.525, -63560.54 ,\n",
       "        -66213.135, -63093.645, -61928.025, -65010.675, -63466.235,\n",
       "        -67683.815, -76028.475, -65981.37 , -71110.01 , -63368.695,\n",
       "        -64700.525, -63918.57 , -71604.33 , -62233.55 , -66114.615,\n",
       "        -67125.955, -65800.81 , -62576.525, -64556.97 , -62318.725,\n",
       "        -65053.355, -67992.46 , -72461.185, -67529.09 , -65118.355,\n",
       "        -64585.355, -66198.725, -67849.325, -65235.17 , -64042.22 ,\n",
       "        -62616.73 , -63588.83 , -63327.75 , -66847.505, -70481.74 ,\n",
       "        -63717.175, -67686.915, -63612.83 , -66001.465, -60826.5  ,\n",
       "        -64984.86 , -65934.76 , -72817.665, -64626.865, -65012.23 ,\n",
       "        -64762.855, -69953.095, -64090.795, -66582.33 , -64741.63 ,\n",
       "        -62318.44 , -63191.88 , -67819.62 , -63853.475, -61993.605,\n",
       "        -64776.165, -65486.52 , -64635.355, -64477.41 , -65372.465,\n",
       "        -65787.585, -63508.055, -64023.955, -64911.585, -63987.59 ,\n",
       "        -65808.795, -67168.76 , -64035.955, -63326.39 , -77470.805,\n",
       "        -66782.285, -69997.775, -67059.92 , -76889.355, -63364.975,\n",
       "        -69848.525, -84952.895, -62687.645, -69471.255, -64224.36 ,\n",
       "        -63241.015, -63288.725, -63746.015, -70613.04 , -63981.21 ,\n",
       "        -66441.88 , -68254.145, -67871.92 , -67423.735, -85669.47 ,\n",
       "        -65794.005, -70664.665, -74345.855, -64088.42 , -65796.64 ,\n",
       "        -69719.8  , -64217.565, -64617.625, -66256.975, -66159.79 ,\n",
       "        -70784.565, -65956.7  , -65724.855, -65172.165, -65331.205,\n",
       "        -65229.255, -66971.76 , -77791.66 , -72160.7  , -72128.   ,\n",
       "        -64848.865, -64087.015, -64090.83 , -66615.17 , -67552.07 ,\n",
       "        -71936.15 , -71186.595, -65733.38 , -65161.865, -66672.32 ,\n",
       "        -74277.885, -65713.95 , -64907.835, -63462.235, -71065.085,\n",
       "        -83260.46 , -63357.465, -64726.295, -64152.65 , -64855.055,\n",
       "        -61005.33 , -63444.615, -69217.21 , -66307.43 , -64699.335,\n",
       "        -65952.415, -64943.135, -63473.93 , -65034.93 , -63255.21 ,\n",
       "        -65467.835, -67999.665, -64630.715, -63302.305, -64629.275,\n",
       "        -68737.825, -66963.795, -63232.535, -64733.34 , -70803.065,\n",
       "        -65551.035, -61250.28 , -66705.82 , -65588.77 , -67772.81 ,\n",
       "        -63077.315, -64216.005, -63851.405, -75462.345, -67719.715,\n",
       "        -64159.06 , -65148.535, -62800.32 , -63898.32 , -63899.25 ,\n",
       "        -64962.73 , -63808.815, -64935.965, -65735.5  , -62502.675,\n",
       "        -78993.695, -65370.345, -66627.315, -65034.295, -65767.59 ,\n",
       "        -66251.345, -71730.66 , -64857.445, -65630.345, -67737.44 ,\n",
       "        -67005.25 , -65502.75 , -63072.48 , -77335.285, -63143.47 ,\n",
       "        -63908.04 , -63699.525, -73279.18 , -69016.565, -63921.115,\n",
       "        -66622.07 , -63009.305, -77804.615, -65256.915, -74499.86 ,\n",
       "        -76757.625, -66762.82 , -67893.15 , -64250.99 , -64624.805,\n",
       "        -66936.865, -65906.695, -68271.745, -63075.305, -65481.09 ,\n",
       "        -63617.02 , -64461.895, -66533.53 , -66165.265, -66566.505]),\n",
       " 'split3_test_score': array([-61924.72 , -64954.315, -66696.56 , -66608.095, -62812.71 ,\n",
       "        -63362.875, -62172.39 , -69072.18 , -62240.61 , -63293.675,\n",
       "        -64819.685, -67302.535, -61963.125, -70969.09 , -77505.775,\n",
       "        -62956.77 , -64491.35 , -68170.645, -62721.435, -67435.   ,\n",
       "        -64449.285, -65186.58 , -66889.275, -65021.245, -64149.41 ,\n",
       "        -64262.21 , -65397.76 , -63161.695, -62514.845, -62463.955,\n",
       "        -67928.315, -65605.025, -63038.985, -63249.285, -69055.08 ,\n",
       "        -61564.625, -69625.775, -78131.105, -63491.635, -66111.475,\n",
       "        -64226.77 , -63749.745, -64768.37 , -67789.78 , -73984.145,\n",
       "        -71394.43 , -64152.305, -65719.44 , -77262.465, -63408.085,\n",
       "        -63796.16 , -65574.87 , -65044.45 , -64513.17 , -63814.625,\n",
       "        -64984.25 , -68295.63 , -63757.625, -62813.735, -68500.945,\n",
       "        -65575.215, -66315.88 , -71076.16 , -63215.33 , -62782.38 ,\n",
       "        -64307.545, -67119.16 , -59877.54 , -64570.715, -65318.795,\n",
       "        -62151.64 , -65510.105, -64042.23 , -64271.18 , -65422.165,\n",
       "        -67649.555, -66581.64 , -64483.18 , -65604.76 , -65414.17 ,\n",
       "        -63662.915, -66318.725, -63731.635, -60643.875, -63803.875,\n",
       "        -74348.525, -76289.415, -63898.885, -68348.465, -63199.99 ,\n",
       "        -64814.05 , -63443.225, -63806.755, -62799.935, -63419.64 ,\n",
       "        -65119.1  , -75748.01 , -64030.35 , -67541.15 , -63599.405,\n",
       "        -64123.98 , -64146.5  , -69323.91 , -63692.305, -66338.645,\n",
       "        -63790.83 , -65493.625, -61540.795, -66102.65 , -65990.16 ,\n",
       "        -62019.355, -67151.56 , -71265.08 , -65145.325, -64914.445,\n",
       "        -66465.375, -66004.475, -67436.755, -63026.97 , -63287.26 ,\n",
       "        -67140.41 , -63512.07 , -63554.765, -66108.075, -70542.585,\n",
       "        -63446.415, -66390.12 , -63315.66 , -65533.78 , -62040.93 ,\n",
       "        -66591.88 , -65922.05 , -73433.04 , -63467.555, -62690.705,\n",
       "        -63133.89 , -68438.43 , -64757.445, -66845.745, -63052.415,\n",
       "        -62016.565, -65836.47 , -65686.025, -64430.78 , -62880.705,\n",
       "        -63405.11 , -60677.445, -64767.   , -64159.63 , -65354.59 ,\n",
       "        -67395.025, -62518.825, -64437.605, -63622.14 , -61878.76 ,\n",
       "        -63664.155, -65580.445, -61386.6  , -64104.105, -76202.505,\n",
       "        -64480.02 , -66960.695, -67144.355, -76037.815, -62563.855,\n",
       "        -69359.905, -84845.22 , -62058.265, -66015.495, -64605.64 ,\n",
       "        -62951.945, -63678.05 , -61665.855, -69754.875, -63033.595,\n",
       "        -64483.84 , -66231.945, -65199.305, -66264.21 , -86805.755,\n",
       "        -64558.63 , -68013.775, -70630.12 , -62585.995, -64533.13 ,\n",
       "        -66843.885, -63113.445, -62250.21 , -64785.125, -65303.71 ,\n",
       "        -70385.915, -65322.13 , -62104.59 , -61536.21 , -63963.91 ,\n",
       "        -64053.24 , -68021.18 , -78688.955, -70173.325, -68982.925,\n",
       "        -64482.5  , -63093.135, -63564.93 , -65063.63 , -64278.03 ,\n",
       "        -69749.815, -67991.365, -66528.565, -61897.93 , -64746.29 ,\n",
       "        -74892.32 , -63673.52 , -62749.625, -62148.425, -69530.135,\n",
       "        -81379.815, -62338.985, -64498.525, -63074.51 , -63218.2  ,\n",
       "        -60510.04 , -63428.55 , -68273.45 , -62949.72 , -61244.165,\n",
       "        -64239.72 , -64464.085, -63734.235, -60582.01 , -63296.835,\n",
       "        -60444.965, -64989.35 , -62848.57 , -62187.56 , -64136.565,\n",
       "        -66052.065, -64246.69 , -62479.58 , -64761.73 , -70184.44 ,\n",
       "        -65203.185, -63065.455, -65469.305, -64440.795, -66698.96 ,\n",
       "        -61975.655, -60243.795, -62472.82 , -72843.68 , -65593.265,\n",
       "        -63354.48 , -65814.995, -63452.395, -63388.035, -62863.885,\n",
       "        -63865.56 , -67159.945, -61732.38 , -65574.635, -64722.855,\n",
       "        -77383.27 , -67163.795, -63990.905, -64549.755, -63546.845,\n",
       "        -64748.42 , -72204.185, -66748.11 , -64362.735, -65740.64 ,\n",
       "        -64413.17 , -63830.005, -62502.8  , -77454.225, -60893.905,\n",
       "        -61520.31 , -61550.625, -74421.945, -68869.63 , -61255.81 ,\n",
       "        -63799.785, -59982.39 , -79372.275, -63743.515, -72028.235,\n",
       "        -75281.13 , -65153.59 , -65416.205, -62176.485, -62347.035,\n",
       "        -63973.505, -64663.62 , -70428.115, -63493.91 , -65714.58 ,\n",
       "        -61144.72 , -64557.17 , -65913.845, -66089.82 , -64391.06 ]),\n",
       " 'split4_test_score': array([-63038.73 , -64228.785, -67413.98 , -65956.905, -66827.04 ,\n",
       "        -63399.545, -61589.98 , -68865.611, -66145.116, -62527.106,\n",
       "        -66861.99 , -68502.711, -63332.53 , -71532.885, -77014.86 ,\n",
       "        -67251.681, -66147.43 , -67832.14 , -64386.921, -69241.15 ,\n",
       "        -65862.945, -67784.94 , -67221.716, -66349.51 , -64659.395,\n",
       "        -65084.185, -67279.995, -63430.791, -64841.431, -64066.02 ,\n",
       "        -68911.75 , -65273.145, -65053.625, -61997.125, -68581.86 ,\n",
       "        -63761.936, -68361.815, -77528.26 , -65331.846, -67450.39 ,\n",
       "        -65194.69 , -63052.445, -65921.991, -69013.47 , -74910.41 ,\n",
       "        -73357.69 , -66131.745, -66727.88 , -79082.345, -66164.226,\n",
       "        -66372.38 , -69476.775, -68317.89 , -65084.77 , -62970.205,\n",
       "        -65253.26 , -68855.41 , -63067.981, -64377.861, -67736.46 ,\n",
       "        -67917.225, -64880.865, -71831.351, -63706.08 , -64909.655,\n",
       "        -64310.26 , -69504.18 , -63561.925, -64887.57 , -65524.471,\n",
       "        -63411.09 , -67280.805, -64636.085, -64109.925, -65872.811,\n",
       "        -66999.19 , -70200.25 , -65340.576, -64736.171, -67306.12 ,\n",
       "        -64375.48 , -69423.701, -65193.18 , -62758.151, -65266.055,\n",
       "        -75265.385, -77172.075, -64856.22 , -71355.826, -65276.075,\n",
       "        -65111.45 , -64465.05 , -65326.681, -62028.806, -64403.675,\n",
       "        -66377.045, -75654.94 , -68039.115, -71254.001, -62822.585,\n",
       "        -66716.016, -63338.28 , -70551.951, -63303.646, -65731.35 ,\n",
       "        -63016.47 , -65441.306, -63666.115, -66432.585, -64891.295,\n",
       "        -64296.44 , -66543.14 , -73734.785, -63486.44 , -64673.661,\n",
       "        -62752.415, -66818.51 , -65555.965, -61894.24 , -62435.335,\n",
       "        -64917.141, -64791.23 , -65009.725, -67277.95 , -71419.525,\n",
       "        -64790.385, -70238.1  , -64722.15 , -64479.505, -63218.886,\n",
       "        -65131.84 , -66443.28 , -73274.5  , -65000.48 , -61342.686,\n",
       "        -63893.345, -67133.715, -64778.86 , -68509.53 , -63622.686,\n",
       "        -64045.01 , -64641.88 , -67439.2  , -63951.446, -60689.195,\n",
       "        -61479.735, -64106.571, -66505.711, -62246.565, -66560.736,\n",
       "        -70186.966, -63326.895, -64385.371, -63041.161, -64964.956,\n",
       "        -65345.346, -68261.64 , -61732.75 , -65393.55 , -80269.16 ,\n",
       "        -65898.34 , -68273.465, -67959.21 , -76279.315, -63415.53 ,\n",
       "        -69897.05 , -84517.47 , -64781.565, -68839.111, -63220.965,\n",
       "        -64873.18 , -65458.341, -64158.176, -69089.636, -64313.63 ,\n",
       "        -65914.865, -68216.38 , -66196.096, -67251.886, -86335.37 ,\n",
       "        -65775.01 , -67743.691, -74843.245, -63627.45 , -63773.305,\n",
       "        -71205.961, -64692.765, -61726.166, -67691.92 , -66524.78 ,\n",
       "        -71074.565, -64757.35 , -63537.285, -64032.605, -62645.216,\n",
       "        -66560.295, -69376.135, -77615.43 , -72410.715, -69622.98 ,\n",
       "        -65511.236, -65958.64 , -63338.881, -65846.021, -66841.4  ,\n",
       "        -70560.29 , -70745.356, -64392.34 , -61958.82 , -66951.37 ,\n",
       "        -73737.33 , -64166.73 , -66232.08 , -63532.476, -72053.12 ,\n",
       "        -82443.895, -63843.675, -66317.041, -65948.915, -65883.015,\n",
       "        -62582.551, -63422.135, -70268.97 , -59924.175, -64888.775,\n",
       "        -65213.825, -67509.21 , -63895.041, -62115.955, -62726.875,\n",
       "        -63678.   , -66967.61 , -65596.881, -63916.84 , -66113.81 ,\n",
       "        -68864.441, -66444.07 , -62769.195, -64858.17 , -72103.665,\n",
       "        -66897.056, -63578.8  , -66891.735, -65288.965, -68557.76 ,\n",
       "        -65425.935, -63186.356, -62924.266, -76264.36 , -68940.455,\n",
       "        -63780.34 , -61478.885, -65652.3  , -65744.595, -63316.035,\n",
       "        -61138.53 , -64864.18 , -64413.101, -67304.07 , -63091.89 ,\n",
       "        -80329.5  , -66777.79 , -62925.461, -65487.19 , -65504.47 ,\n",
       "        -64134.205, -72010.036, -65948.095, -64329.325, -66222.725,\n",
       "        -67405.155, -63164.855, -65408.535, -77301.625, -63791.541,\n",
       "        -63209.531, -63342.096, -74683.23 , -68012.22 , -64420.55 ,\n",
       "        -64489.8  , -63755.506, -80378.02 , -65089.215, -72808.305,\n",
       "        -74682.45 , -67113.485, -67303.215, -63176.84 , -63246.955,\n",
       "        -64892.56 , -64271.05 , -68270.25 , -62965.655, -68296.74 ,\n",
       "        -65972.905, -65106.88 , -69139.42 , -67031.606, -66330.051]),\n",
       " 'mean_test_score': array([-62455.5876, -64741.2528, -66575.1668, -66797.9488, -63314.8686,\n",
       "        -63166.9664, -62448.4258, -68332.8338, -64419.358 , -63493.5426,\n",
       "        -65172.2664, -67216.911 , -62337.0824, -71199.8254, -76547.1916,\n",
       "        -65711.0856, -65148.9086, -67055.7832, -63535.8774, -68171.7986,\n",
       "        -65112.9826, -67118.3996, -66464.8198, -65250.8386, -64336.6696,\n",
       "        -63003.4702, -66275.7496, -63731.7158, -63155.6386, -63054.1636,\n",
       "        -67172.783 , -64666.4396, -63472.9922, -62810.3238, -69046.8606,\n",
       "        -61933.899 , -67190.2972, -76581.3136, -64873.2068, -66217.1042,\n",
       "        -65373.6292, -63422.9354, -65178.7468, -67497.2038, -74534.3404,\n",
       "        -72008.2286, -63819.0382, -65493.0204, -76930.0314, -65144.7434,\n",
       "        -65794.8764, -66899.7068, -65671.5584, -64729.4096, -63986.7072,\n",
       "        -65072.3442, -67390.09  , -63033.2348, -64080.6416, -68289.4196,\n",
       "        -66571.3356, -65184.1098, -70649.1696, -63037.8206, -63727.2166,\n",
       "        -63805.6296, -69235.6116, -63033.8516, -64169.9176, -64926.2642,\n",
       "        -63041.2676, -67029.3518, -64131.4504, -63961.2218, -65142.3828,\n",
       "        -66662.192 , -68520.3704, -64850.531 , -64405.1262, -67152.312 ,\n",
       "        -63883.5642, -67680.861 , -63491.3202, -62527.1938, -64305.3718,\n",
       "        -74396.7256, -76497.616 , -63834.0246, -69372.985 , -64317.813 ,\n",
       "        -64748.7246, -63469.9948, -63607.307 , -63325.9648, -63533.3158,\n",
       "        -66701.7614, -75161.9242, -65486.0198, -69209.9696, -62914.7464,\n",
       "        -65046.4052, -63337.5818, -70146.86  , -62804.7268, -66449.1976,\n",
       "        -64011.9276, -65208.2606, -62313.6676, -65042.7226, -64434.2932,\n",
       "        -63720.8526, -67249.9224, -72164.6504, -65542.6736, -64781.4608,\n",
       "        -64684.023 , -65794.0786, -66614.7798, -63331.5884, -63474.5362,\n",
       "        -64833.0136, -63566.1376, -63906.7154, -66665.2736, -70984.2732,\n",
       "        -64052.8526, -67890.0308, -63869.6432, -65165.9442, -61761.312 ,\n",
       "        -65381.5788, -65404.3988, -73097.1296, -63566.6882, -62578.916 ,\n",
       "        -63589.1594, -67686.5324, -63990.3556, -67136.7624, -63764.4648,\n",
       "        -62891.1852, -65118.7014, -67490.3526, -63681.2774, -61854.5526,\n",
       "        -62795.5414, -62810.0858, -64487.247 , -63491.4306, -65376.0914,\n",
       "        -66991.4346, -63146.7094, -63668.855 , -63939.3268, -63176.4694,\n",
       "        -63651.6088, -66883.6476, -61980.0634, -63563.1496, -78050.9492,\n",
       "        -65050.1454, -67447.3508, -67238.0844, -75395.403 , -63161.4566,\n",
       "        -69811.235 , -84357.683 , -62835.8286, -68265.6854, -63843.3982,\n",
       "        -63854.2606, -63955.7776, -63463.1938, -70186.0314, -63690.6104,\n",
       "        -65180.6858, -67250.1156, -65926.081 , -66664.8076, -86317.395 ,\n",
       "        -65535.2266, -68689.4988, -73256.33  , -63101.8156, -64293.6534,\n",
       "        -69066.5698, -63515.8798, -63242.731 , -65508.0364, -65351.1476,\n",
       "        -72251.6114, -65371.346 , -63416.3778, -63336.0292, -62971.9284,\n",
       "        -65399.1396, -67320.4736, -77587.9572, -71263.5858, -70138.6904,\n",
       "        -64931.7938, -64133.6954, -63147.2316, -65910.7396, -65921.2938,\n",
       "        -69685.9712, -69774.0668, -65197.1736, -62917.3264, -65795.0328,\n",
       "        -73660.1256, -64189.2996, -63906.3674, -63697.8558, -70043.5796,\n",
       "        -82123.662 , -63378.0054, -64667.4798, -63669.8414, -64774.7738,\n",
       "        -61769.9598, -63515.5014, -69120.7716, -63368.4786, -63118.6194,\n",
       "        -64262.2526, -65400.3274, -63968.4378, -62674.3598, -62406.6476,\n",
       "        -62934.4754, -66478.8416, -64126.9478, -62976.3404, -64175.5066,\n",
       "        -68888.2216, -65682.0498, -62807.6764, -64528.0402, -70485.9638,\n",
       "        -65530.3768, -62455.7062, -66159.4812, -64189.6594, -67557.2246,\n",
       "        -64162.6106, -62102.6556, -63001.3288, -75141.2352, -66774.0424,\n",
       "        -62748.2028, -64041.6478, -63092.269 , -63579.0634, -63420.8476,\n",
       "        -62806.2826, -64360.1566, -63215.3036, -65911.2294, -63512.8166,\n",
       "        -79495.6688, -66523.0124, -63568.5968, -65613.4186, -64983.8536,\n",
       "        -65252.3278, -70256.3944, -65173.2946, -63688.9376, -66236.013 ,\n",
       "        -66064.7826, -63813.9598, -64098.1094, -76659.5164, -62188.2966,\n",
       "        -62299.6066, -62779.3086, -73986.0854, -68024.7534, -63041.8944,\n",
       "        -64118.797 , -62389.6746, -78700.2304, -63840.8252, -72497.9706,\n",
       "        -75093.0342, -66119.9132, -65709.3654, -63708.2404, -63388.2598,\n",
       "        -64930.8866, -64629.6706, -68624.8158, -62892.2914, -66117.4548,\n",
       "        -63301.3396, -64286.0636, -66787.9568, -67058.6968, -65315.7966]),\n",
       " 'std_test_score': array([1127.24011389,  799.24993517, 1275.61408984, 1284.21329434,\n",
       "        2233.8417111 , 1278.60318415,  644.35106421,  811.17275968,\n",
       "        1586.15936351,  971.59890663, 1105.48455189, 1590.66851656,\n",
       "         503.65892973,  868.66471847,  723.14273185, 1501.51759585,\n",
       "         863.04200917, 1202.33524002, 1147.77708707,  641.33238171,\n",
       "        1188.66464935, 1153.86044347,  879.41381675,  714.28572531,\n",
       "        1150.69599041, 2171.84882673, 1123.9238847 ,  941.11898147,\n",
       "         898.62278905,  955.15903156, 1119.14633454, 2069.8122644 ,\n",
       "         806.82597314,  788.91114483,  410.22319138, 1567.50142664,\n",
       "        1641.28183369, 1333.94057766,  772.98931782, 1014.9182126 ,\n",
       "        1104.71916619,  522.05116867, 1226.66608822, 1386.85379912,\n",
       "        1134.00994338, 1214.13765302, 1308.55156459, 1204.29468338,\n",
       "        1382.89001051, 1239.95223008, 1329.8684996 , 1367.06303882,\n",
       "        1393.13854845, 1178.22465882, 1480.07845042,  657.63700945,\n",
       "        1527.25185646, 1502.23916112, 1454.66864241, 1360.64496168,\n",
       "        1193.83648107, 1143.19981349, 1682.84851072, 1154.88026224,\n",
       "        1004.26217081, 1116.60452335, 1408.17675519, 1792.74311832,\n",
       "        1061.63139452, 1167.8527799 ,  997.95991415, 1155.49497144,\n",
       "        1077.59104269,  968.92949381,  645.33356566, 1003.89908172,\n",
       "        1378.12048795,  316.43355089, 1793.54057471, 1756.65829566,\n",
       "         255.86216349, 1569.98194731, 1639.41969871, 1396.18607801,\n",
       "         889.91019731,  860.4788579 ,  581.41324538,  959.00289038,\n",
       "        1590.19667953,  844.44187599, 1026.90334184,  684.21776882,\n",
       "        1603.51133652, 1040.80956565, 1052.02877399, 1255.85484899,\n",
       "         842.12881875, 1492.38257183, 1955.12954972,  790.1230902 ,\n",
       "         896.82016707, 1048.00014519, 1200.35852869,  597.05201404,\n",
       "        1031.86046037, 1725.3509955 ,  928.00773218, 1272.81455535,\n",
       "        1706.59774738, 1564.69633801, 1706.95232566,  848.05588507,\n",
       "        1004.0452963 , 1432.73625765,  529.85853784, 1343.99755735,\n",
       "         950.51832261, 1540.67644847, 1568.67920634, 1399.24774596,\n",
       "        1734.8099205 , 1204.62784035, 1132.88183737,  804.62301297,\n",
       "         812.12939116,  479.34531121, 1477.68064578,  949.22267528,\n",
       "        1493.08806845,  843.21117277, 1531.89039609, 1848.89222703,\n",
       "         286.97613445, 1352.98713992, 1305.5325121 ,  796.91692379,\n",
       "        1375.85344462, 1600.50025382, 1266.86481359,  589.87469825,\n",
       "         879.415538  , 1137.29002942, 1166.14545239,  777.4264719 ,\n",
       "        1296.42706147, 1424.32953101, 1737.35176879, 1198.91097975,\n",
       "        1230.67791545,  648.11847298, 1881.34996107, 1402.67655157,\n",
       "        1422.44303662,  635.85268257, 1314.74283229, 1945.10953963,\n",
       "         961.65926666, 1082.08811699, 1100.23200096, 1650.04242066,\n",
       "        1493.81740517, 1828.43898113, 1219.96267205, 1267.0124559 ,\n",
       "         950.66077027,  659.68219844, 1061.68506852, 1260.49791091,\n",
       "        1768.48873689,  509.42529009, 1062.40399572,  777.31554703,\n",
       "         962.5276792 ,  703.35246931,  901.95991347, 1494.28120079,\n",
       "        1278.16249532, 1737.67209447,  812.53503322, 1013.77504999,\n",
       "         700.84209746, 1033.89218433, 1677.31304927,  894.40176195,\n",
       "        2008.06550735, 1505.62711363,  873.65955632, 1095.07084359,\n",
       "        1337.42771869, 1120.61124917, 1879.67738321,  581.40628553,\n",
       "        1275.34464741, 1227.64492082, 1504.87243526,  831.57820064,\n",
       "        1658.78758669,  809.56199027, 1033.51120757, 1060.95761354,\n",
       "        1094.8818648 ,  970.82834934,  924.3560848 ,  584.3612681 ,\n",
       "        1660.26392628, 1541.04876891, 1124.67261725, 1256.32292595,\n",
       "        1485.84077992, 1043.314446  ,  903.02041886, 1143.25070071,\n",
       "        1753.60665096,  949.74943318, 1764.48336286, 1142.40357243,\n",
       "         783.854789  ,  933.50809493, 1323.58381268, 1423.5250833 ,\n",
       "        1355.15071744,  226.87083516, 1591.82010589, 2145.46332863,\n",
       "        1464.72398689, 1563.0519352 , 1240.47248132, 1139.36408723,\n",
       "        2127.38374237,  871.72366472, 1760.20865169, 1001.22342839,\n",
       "         943.51493004,  621.36351786, 1433.05020147, 1684.81698543,\n",
       "         998.41544959,  254.46900041,  498.11460909, 1467.62344534,\n",
       "        1584.21023037, 1503.84955939,  994.3679964 , 1606.89267105,\n",
       "        1532.36041538, 1574.09002635, 1425.69981843, 1340.72990457,\n",
       "        1411.31038597, 1549.28256561, 1275.63376874, 1706.1911397 ,\n",
       "        1837.41055434, 1419.6178845 ,  853.49378321, 1911.17764239,\n",
       "        1702.93977108, 1238.25207527,  861.78260083, 1217.95788978,\n",
       "        1297.36755592, 1346.96881528, 1696.65149892,  995.47386496,\n",
       "        1172.19238499,  941.42842537, 2289.5581832 , 1383.70739588,\n",
       "        1485.19256507, 1413.3580479 , 1521.50676996, 1090.99038558,\n",
       "        1318.34628567, 1022.93999825, 1091.61813031, 1053.69896054,\n",
       "        1533.37763559, 1013.78018261, 1246.83226115, 1217.21747939,\n",
       "        1423.26827727, 2240.48703986, 1184.75819823, 1325.17499057,\n",
       "        1116.95086944, 1626.28228224, 1130.19344374, 1788.21320519,\n",
       "        1491.8091259 , 1138.05159521, 1235.29335574,  988.48571711,\n",
       "        1045.53729188,  968.39470589, 1100.35487236, 1657.75852392,\n",
       "         563.1648132 , 1369.31201007, 1023.09146733, 1122.40605757]),\n",
       " 'rank_test_score': array([ 14, 145, 214, 222,  55,  50,  13, 251, 136,  72, 166, 234,  10,\n",
       "        273, 290, 194, 164, 227,  77, 248, 160, 229, 210, 173, 133,  36,\n",
       "        208,  95,  48,  42, 232, 141,  68,  26, 256,   4, 233, 291, 151,\n",
       "        206, 178,  65, 168, 242, 284, 275,  99, 185, 293, 163, 196, 224,\n",
       "        191, 144, 112, 159, 239,  37, 117, 250, 213, 170, 271,  39,  94,\n",
       "         97, 260,  38, 124, 152,  40, 226, 121, 110, 162, 216, 252, 150,\n",
       "        135, 231, 105, 244,  70,  16, 131, 283, 289, 100, 261, 132, 146,\n",
       "         67,  84,  56,  76, 219, 287, 184, 259,  30, 157,  59, 267,  22,\n",
       "        209, 114, 172,   9, 156, 137,  93, 236, 276, 189, 148, 143, 195,\n",
       "        215,  57,  69, 149,  79, 107, 218, 272, 116, 246, 104, 165,   1,\n",
       "        180, 183, 279,  80,  17,  83, 245, 113, 230,  96,  28, 161, 241,\n",
       "         88,   3,  21,  25, 138,  71, 179, 225,  46,  86, 108,  51,  85,\n",
       "        223,   5,  78, 295, 158, 240, 235, 288,  49, 264, 299,  27, 249,\n",
       "        102, 103, 109,  66, 268,  90, 169, 237, 201, 217, 300, 188, 254,\n",
       "        280,  44, 130, 257,  75,  53, 186, 176, 277, 177,  63,  58,  33,\n",
       "        181, 238, 294, 274, 266, 154, 122,  47, 198, 200, 262, 263, 171,\n",
       "         31, 197, 281, 126, 106,  91, 265, 298,  61, 142,  87, 147,   2,\n",
       "         74, 258,  60,  45, 128, 182, 111,  18,  12,  32, 211, 120,  34,\n",
       "        125, 255, 192,  24, 139, 270, 187,  15, 205, 127, 243, 123,   6,\n",
       "         35, 286, 220,  19, 115,  43,  82,  64,  23, 134,  52, 199,  73,\n",
       "        297, 212,  81, 190, 155, 174, 269, 167,  89, 207, 202,  98, 118,\n",
       "        292,   7,   8,  20, 282, 247,  41, 119,  11, 296, 101, 278, 285,\n",
       "        204, 193,  92,  62, 153, 140, 253,  29, 203,  54, 129, 221, 228,\n",
       "        175])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_c3.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create custom model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMod:\n",
    "    def __init__(self, model_1, model_2, model_3, inter):\n",
    "        self.mod1 = model_1\n",
    "        self.mod2 = model_2\n",
    "        self.mod3 = model_3\n",
    "        self.thresholds = [inter[0], inter[1]]\n",
    "        \n",
    "    def predict_custom(self, X, val = \"item_price\"):\n",
    "        '''Custom Predict function\n",
    "        \n",
    "        '''\n",
    "        # Split into subsets and store index\n",
    "        C1 = X.loc[X[val] < self.thresholds[0], :].index\n",
    "        C2 = X.loc[(X[val] >= self.thresholds[0]) &\n",
    "                   (X[val] <= self.thresholds[1]), :].index\n",
    "        C3 = X.loc[X[val] > self.thresholds[1], :].index\n",
    "        index_cust = C1.append([C2, C3]) # Index for consistent prediction order\n",
    "        \n",
    "        # Predict using the three models\n",
    "        Y1 = self.mod1.predict(X.loc[C1, :]) # Model 1\n",
    "        Y2 = self.mod2.predict(X.loc[C2, :]) # Model 2\n",
    "        Y3 = self.mod3.predict(X.loc[C3, :]) # Model 3\n",
    "        \n",
    "        # Combine prediction to be consistent with the order of X\n",
    "        preds = pd.Series(np.concatenate([Y1, Y2, Y3]), name = \"predictions\", index = index_cust) # Create series of predictions\n",
    "        preds = preds[X.index] # Reorder based in the index of X\n",
    "        \n",
    "        return(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "order_item_id\n",
       "65902    False\n",
       "90887    False\n",
       "65284     True\n",
       "31170     True\n",
       "74301     True\n",
       "         ...  \n",
       "11239    False\n",
       "43483     True\n",
       "45333    False\n",
       "76278    False\n",
       "15059    False\n",
       "Name: predictions, Length: 79142, dtype: bool"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = CustomMod(mod_c1.best_estimator_, mod_c2.best_estimator_, mod_c3.best_estimator_, inter = [K1,K2])\n",
    "test.predict_custom(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(               item_id  brand_id  item_price  user_id  time_to_delivery  \\\n",
       " order_item_id                                                             \n",
       " 3548                84         5       24.90    20671               NaN   \n",
       " 14074               70        34       19.90    33217               4.0   \n",
       " 38997             1517         6       69.90    15564              21.0   \n",
       " 98241             1692        50       89.90    40112               2.0   \n",
       " 46914             1625        20       99.90    16535              22.0   \n",
       " ...                ...       ...         ...      ...               ...   \n",
       " 15325              159        54       39.90    31764               NaN   \n",
       " 21001              699        39       44.95    34279               3.0   \n",
       " 71194             1609         1      179.90    16864               4.0   \n",
       " 83590               16        12       69.90    26046               2.0   \n",
       " 50164             1593        17      219.90     5006               NaN   \n",
       " \n",
       "                user_age  customer_age  item_size_1  item_size_10  \\\n",
       " order_item_id                                                      \n",
       " 3548            19544.0          2208            0             0   \n",
       " 14074           17835.0          2208            0             0   \n",
       " 38997           22398.0          1768            0             0   \n",
       " 98241           23890.0          2208            0             0   \n",
       " 46914           22012.0          1708            0             0   \n",
       " ...                 ...           ...          ...           ...   \n",
       " 15325           21778.0          1712            0             0   \n",
       " 21001               NaN          1707            0             0   \n",
       " 71194           18884.0          2010            0             0   \n",
       " 83590           16820.0          2073            0             0   \n",
       " 50164           14694.0          2208            0             0   \n",
       " \n",
       "                item_size_104  ...  user_state_Hesse  user_state_Lower Saxony  \\\n",
       " order_item_id                 ...                                              \n",
       " 3548                       0  ...                 0                        0   \n",
       " 14074                      0  ...                 0                        0   \n",
       " 38997                      0  ...                 1                        0   \n",
       " 98241                      0  ...                 0                        0   \n",
       " 46914                      0  ...                 0                        1   \n",
       " ...                      ...  ...               ...                      ...   \n",
       " 15325                      0  ...                 0                        0   \n",
       " 21001                      0  ...                 0                        0   \n",
       " 71194                      0  ...                 1                        0   \n",
       " 83590                      0  ...                 1                        0   \n",
       " 50164                      0  ...                 0                        0   \n",
       " \n",
       "                user_state_Mecklenburg-Western Pomerania  \\\n",
       " order_item_id                                             \n",
       " 3548                                                  0   \n",
       " 14074                                                 0   \n",
       " 38997                                                 0   \n",
       " 98241                                                 0   \n",
       " 46914                                                 0   \n",
       " ...                                                 ...   \n",
       " 15325                                                 0   \n",
       " 21001                                                 0   \n",
       " 71194                                                 0   \n",
       " 83590                                                 0   \n",
       " 50164                                                 0   \n",
       " \n",
       "                user_state_North Rhine-Westphalia  \\\n",
       " order_item_id                                      \n",
       " 3548                                           0   \n",
       " 14074                                          0   \n",
       " 38997                                          0   \n",
       " 98241                                          0   \n",
       " 46914                                          0   \n",
       " ...                                          ...   \n",
       " 15325                                          0   \n",
       " 21001                                          0   \n",
       " 71194                                          0   \n",
       " 83590                                          0   \n",
       " 50164                                          0   \n",
       " \n",
       "                user_state_Rhineland-Palatinate  user_state_Saarland  \\\n",
       " order_item_id                                                         \n",
       " 3548                                         0                    0   \n",
       " 14074                                        0                    0   \n",
       " 38997                                        0                    0   \n",
       " 98241                                        0                    0   \n",
       " 46914                                        0                    0   \n",
       " ...                                        ...                  ...   \n",
       " 15325                                        0                    0   \n",
       " 21001                                        0                    0   \n",
       " 71194                                        0                    0   \n",
       " 83590                                        0                    0   \n",
       " 50164                                        0                    0   \n",
       " \n",
       "                user_state_Saxony  user_state_Saxony-Anhalt  \\\n",
       " order_item_id                                                \n",
       " 3548                           0                         0   \n",
       " 14074                          0                         0   \n",
       " 38997                          0                         0   \n",
       " 98241                          0                         0   \n",
       " 46914                          0                         0   \n",
       " ...                          ...                       ...   \n",
       " 15325                          0                         0   \n",
       " 21001                          1                         0   \n",
       " 71194                          0                         0   \n",
       " 83590                          0                         0   \n",
       " 50164                          0                         0   \n",
       " \n",
       "                user_state_Schleswig-Holstein  user_state_Thuringia  \n",
       " order_item_id                                                       \n",
       " 3548                                       0                     0  \n",
       " 14074                                      0                     0  \n",
       " 38997                                      0                     0  \n",
       " 98241                                      0                     0  \n",
       " 46914                                      0                     0  \n",
       " ...                                      ...                   ...  \n",
       " 15325                                      0                     0  \n",
       " 21001                                      0                     0  \n",
       " 71194                                      0                     0  \n",
       " 83590                                      0                     0  \n",
       " 50164                                      0                     0  \n",
       " \n",
       " [19786 rows x 127 columns],\n",
       " order_item_id\n",
       " 3548     False\n",
       " 14074    False\n",
       " 38997    False\n",
       " 98241    False\n",
       " 46914    False\n",
       "          ...  \n",
       " 15325    False\n",
       " 21001     True\n",
       " 71194     True\n",
       " 83590     True\n",
       " 50164    False\n",
       " Name: predictions, Length: 19786, dtype: bool)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Evaluation Score: 160620.02300000002\n",
      "Base Custom Evaluation Score: 175109.465\n"
     ]
    }
   ],
   "source": [
    "preds = test.predict_custom(X_test)\n",
    "print(\"Custom Evaluation Score:\", cost_custom_eval(y_test, 1 * preds, X_test))\n",
    "print(\"Base Custom Evaluation Score:\", base_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"max_depth\" : np.arange(1,20),\n",
    "             \"eta\" : stats.uniform(0.1, 0.8),\n",
    "             \"gamma\" : stats.uniform(0.05, 3),\n",
    "             \"lambda\" : stats.uniform(0, 5),\n",
    "             \"alpha\" : stats.uniform(0, 5),\n",
    "             \"colsample_bytree\" : np.arange(0.2, 1, step = 0.1),\n",
    "             \"subsample\" : np.arange(0.5, 1, step = 0.1),\n",
    "             \"n_estimators\" : np.arange(10, 50, step = 5)}\n",
    "    \n",
    "gbm = xgb.XGBClassifier(objective = \"binary:logistic\", \n",
    "             num_parallel_tree = 1, num_boost_round = 80, early_stopping_rounds = 10)\n",
    "#metric = \"roc_auc\"\n",
    "n = 200\n",
    "fold = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomized_auc = RandomizedSearchCV(estimator = gbm, n_iter = n, cv = fold, scoring = cost_custom_score,\n",
    "                                    param_distributions = param_grid, verbose = 1, random_state = 321,\n",
    "                                   n_jobs = -1)\n",
    "randomized_auc.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 with Bayesion optimization CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.05, random_state = 123)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label = y_train)\n",
    "dtest = xgb.DMatrix(X_test, label = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to use Bayesian optimization to optimize a given function on a given dataset based on given parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_evaluate(max_depth, gamma, eta, colsample_bytree, lam, alph, est):\n",
    "    params = {'eval_metric': 'auc',\n",
    "              'max_depth': int(max_depth),\n",
    "              'eta': eta,\n",
    "              'gamma': gamma,\n",
    "              'colsample_bytree': colsample_bytree,\n",
    "              'objective' : 'binary:logistic',\n",
    "              'n_estimators' :  int(est),\n",
    "              'lambda' : lam,\n",
    "              'alph' : alph}\n",
    "    # Used around 1000 boosting rounds in the full model\n",
    "    cv_result = xgb.cv(params, dtrain, num_boost_round = 150, nfold = 10, early_stopping_rounds = 30,\n",
    "                      nthread = 5)\n",
    "    return cv_result['test-auc-mean'].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   alph    | colsam... |    est    |    eta    |   gamma   |    lam    | max_depth |\n",
      "-------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cv() got an unexpected keyword argument 'nthread'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: (2.772031951138902, 0.31595307159873065, 73.69931345197509, 0.049828210582467926, 3.132826075386422, 6.898211422364913, 120.39318457604423)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-155d501b962e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m                                              'est' : (10, 80)})\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Optimally needs quite a few more initiation points and number of iterations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mxgb_bo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_points\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[1;34m(self, init_points, n_iter, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_bounds_transformer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params, lazy)\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPTIMIZATION_STEP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-d771c549bbc4>\u001b[0m in \u001b[0;36mxgb_evaluate\u001b[1;34m(max_depth, gamma, eta, colsample_bytree, lam, alph, est)\u001b[0m\n\u001b[0;32m     10\u001b[0m               'alph' : alph}\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# Used around 1000 boosting rounds in the full model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     cv_result = xgb.cv(params, dtrain, num_boost_round = 150, nfold = 10, early_stopping_rounds = 30,\n\u001b[0m\u001b[0;32m     13\u001b[0m                       nthread = 5)\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test-auc-mean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cv() got an unexpected keyword argument 'nthread'"
     ]
    }
   ],
   "source": [
    "xgb_bo = BayesianOptimization(xgb_evaluate, {'max_depth': (3, 200), \n",
    "                                             'gamma': (0, 8),\n",
    "                                             'colsample_bytree': (0.3, 0.9),\n",
    "                                             'eta' : (0.001, 0.3),\n",
    "                                             'lam' : (0.1, 8),\n",
    "                                             'alph' : (0.1, 3),\n",
    "                                             'est' : (10, 80)})\n",
    "# Optimally needs quite a few more initiation points and number of iterations\n",
    "xgb_bo.maximize(init_points = 5, n_iter = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgb_bo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-5354f7bea34d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb_bo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#params['max_depth'] = int(params['max_depth'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"params\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"max_depth\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"params\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"max_depth\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#params[\"params\"][\"n_estimators\"] = int(params[\"params\"][\"est\"])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"params\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'objective'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'binary:logistic'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xgb_bo' is not defined"
     ]
    }
   ],
   "source": [
    "params = xgb_bo.max\n",
    "#params['max_depth'] = int(params['max_depth'])\n",
    "params[\"params\"][\"max_depth\"] = int(params[\"params\"][\"max_depth\"])\n",
    "#params[\"params\"][\"n_estimators\"] = int(params[\"params\"][\"est\"])\n",
    "params[\"params\"]['objective'] = 'binary:logistic'\n",
    "params[\"params\"].pop(\"est\")\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "params =  {'alpha': 0.5793037571735223,\n",
    "\n",
    "  'colsample_bytree': 0.3,\n",
    "\n",
    "  'eta': 0.11,\n",
    "\n",
    "  'gamma': 0.0,\n",
    "\n",
    "  'lambda': 6.468522967337128,\n",
    "\n",
    "  'max_depth': 77,\n",
    "\n",
    "  'objective': 'binary:logistic'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Train model with optimal parameters and calculate auc for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:27:36] WARNING: D:\\Build\\xgboost\\xgboost-1.3.1.git\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7745377371183007"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_opt = xgb.train(params, dtrain, num_boost_round = 400)\n",
    "\n",
    "preds = model_opt.predict(dtest)\n",
    "roc_auc_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create .csv of prediction for unknown dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_u = pd.read_pickle('./data/unknown_cleaned_w_dummies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model_opt.predict(xgb.DMatrix(data_u))\n",
    "predict_unknown = pd.Series(preds, index=data_u[\"item_id\"].index, name='return')\n",
    "predict_unknown.to_csv(\"sixth_pred.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
